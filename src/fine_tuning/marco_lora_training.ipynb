{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXsya849ozQj"
      },
      "source": [
        "# Marco LoRA Fine-Tuning Training Notebook\n",
        "\n",
        "Step-by-step LoRA fine-tuning of Qwen2.5-7B-Instruct for Italian teaching.\n",
        "\n",
        "## Training Pipeline Overview\n",
        "1. **Environment Setup** - Check GPU, install dependencies\n",
        "2. **Data Preprocessing** - Load and validate training data\n",
        "3. **Model Initialization** - Configure LoRA and load base model\n",
        "4. **Training Setup** - Verify configuration and memory usage\n",
        "5. **Fine-Tuning** - Execute training with validation monitoring\n",
        "6. **Testing** - Quick inference tests with trained model\n",
        "7. **Evaluation** - Generate plots, examples, and quality metrics\n",
        "\n",
        "**Estimated Training Time (3 epochs, 10K samples):**\n",
        "- **T4**: ~6-8 hours (memory-optimized)\n",
        "- **L4**: ~2-3 hours (high-performance) ⭐ **RECOMMENDED**  \n",
        "- **A100**: ~1.5-2.5 hours (maximum performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eik2uX_RozQl"
      },
      "source": [
        "## 1. Environment Setup & GPU Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKTeQI7rozQl",
        "outputId": "3030f73b-740f-435b-87d4-75b35acaa4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Working directory: /content/drive/MyDrive/Colab Notebooks/italian_teacher\n",
            "📦 Environment setup complete\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path('/content/drive/MyDrive/Colab Notebooks/italian_teacher')\n",
        "if project_root.exists():\n",
        "    sys.path.append(str(project_root))\n",
        "    os.chdir(project_root)\n",
        "    print(f\"✅ Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"❌ Project directory not found. Update path for your setup.\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "print(\"📦 Environment setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awzu2YGLozQm",
        "outputId": "54f5e050-f840-4a0a-d423-7a8ae667ee83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 GPU Detection:\n",
            "CUDA Available: True\n",
            "GPU: NVIDIA L4\n",
            "GPU Memory: 22.2 GB\n",
            "Current GPU usage: 0.00 GB allocated, 0.00 GB cached\n",
            "🚀 Detected L4 - Using high-performance settings\n",
            "\n",
            "📊 Optimized Settings:\n",
            "   Train batch size: 3\n",
            "   Eval batch size: 4\n",
            "   Gradient accumulation: 3\n",
            "   Effective batch size: 9\n",
            "   Pin memory: True\n",
            "   Estimated training time: ~2-3 hours for 3 epochs\n"
          ]
        }
      ],
      "source": [
        "# GPU Detection and Memory Info\n",
        "print(\"🔍 GPU Detection:\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Memory usage\n",
        "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "    cached = torch.cuda.memory_reserved(0) / (1024**3)\n",
        "    print(f\"Current GPU usage: {allocated:.2f} GB allocated, {cached:.2f} GB cached\")\n",
        "\n",
        "    # Determine optimal configuration based on GPU\n",
        "    if \"T4\" in gpu_name:\n",
        "        print(\"🔧 Detected T4 - Using memory-optimized settings\")\n",
        "        recommended_batch_size = 1\n",
        "        recommended_eval_batch_size = 1\n",
        "        gradient_accumulation = 8\n",
        "        pin_memory = False\n",
        "        training_speed = \"~6-8 hours for 3 epochs\"\n",
        "    elif \"L4\" in gpu_name:\n",
        "        print(\"🚀 Detected L4 - Using high-performance settings\")\n",
        "        recommended_batch_size = 3\n",
        "        recommended_eval_batch_size = 4\n",
        "        gradient_accumulation = 3\n",
        "        pin_memory = True\n",
        "        training_speed = \"~2-3 hours for 3 epochs\"\n",
        "    elif \"A100\" in gpu_name:\n",
        "        print(\"🏎️  Detected A100 - Using maximum performance settings\")\n",
        "        recommended_batch_size = 2\n",
        "        recommended_eval_batch_size = 3\n",
        "        gradient_accumulation = 4\n",
        "        pin_memory = True\n",
        "        training_speed = \"~1.5-2.5 hours for 3 epochs\"\n",
        "    else:\n",
        "        print(f\"❓ Unknown GPU ({gpu_name}) - Using conservative settings\")\n",
        "        recommended_batch_size = 1\n",
        "        recommended_eval_batch_size = 1\n",
        "        gradient_accumulation = 8\n",
        "        pin_memory = False\n",
        "        training_speed = \"~6-10 hours for 3 epochs (estimated)\"\n",
        "\n",
        "    effective_batch_size = recommended_batch_size * gradient_accumulation\n",
        "    print(f\"\\n📊 Optimized Settings:\")\n",
        "    print(f\"   Train batch size: {recommended_batch_size}\")\n",
        "    print(f\"   Eval batch size: {recommended_eval_batch_size}\")\n",
        "    print(f\"   Gradient accumulation: {gradient_accumulation}\")\n",
        "    print(f\"   Effective batch size: {effective_batch_size}\")\n",
        "    print(f\"   Pin memory: {pin_memory}\")\n",
        "    print(f\"   Estimated training time: {training_speed}\")\n",
        "else:\n",
        "    print(\"❌ No GPU detected. Training will be extremely slow.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbntrENWozQn",
        "outputId": "d380919a-cb9c-453a-e8a3-29c754f5b7f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/italian_teacher\n",
            "📦 Installing required packages...\n",
            "✅ Added to Python path: /content/drive/MyDrive/Colab Notebooks/italian_teacher/src/fine_tuning\n",
            "✅ All training modules imported successfully\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())\n",
        "\n",
        "# Install required packages first\n",
        "print(\"📦 Installing required packages...\")\n",
        "!pip install -q accelerate>=0.24.0 peft>=0.7.0 bitsandbytes>=0.41.0 transformers>=4.36.0 datasets>=2.14.0 wandb>=0.16.0\n",
        "\n",
        "# Standalone import approach - avoids src package issues\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add fine_tuning directory directly to path\n",
        "fine_tuning_path = Path.cwd() / \"src\" / \"fine_tuning\"\n",
        "if str(fine_tuning_path) not in sys.path:\n",
        "    sys.path.insert(0, str(fine_tuning_path))\n",
        "\n",
        "print(f\"✅ Added to Python path: {fine_tuning_path}\")\n",
        "\n",
        "# Direct imports from fine_tuning directory\n",
        "try:\n",
        "    from lora_trainer import MarcoLoRATrainer\n",
        "    from config import get_default_config\n",
        "    from inference import MarcoInference\n",
        "    print(\"✅ All training modules imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Import error: {e}\")\n",
        "    print(\"Installing additional packages from requirements...\")\n",
        "    !pip install -r src/fine_tuning/requirements.txt\n",
        "    print(\"🔄 Please restart runtime (Runtime → Restart Session) and run this cell again\")\n",
        "\n",
        "    # Try import again after installation\n",
        "    try:\n",
        "        from lora_trainer import MarcoLoRATrainer\n",
        "        from config import get_default_config\n",
        "        from inference import MarcoInference\n",
        "        print(\"✅ Packages installed and imported\")\n",
        "    except ImportError as e2:\n",
        "        print(f\"❌ Still failing: {e2}\")\n",
        "        print(\"🔄 Please restart runtime (Runtime → Restart Session) and run this cell again\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHR0bxyaozQo"
      },
      "source": [
        "# Data validation passed - ready for training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3bliHVcozQo",
        "outputId": "841300c0-4899-4241-bed9-2f08367a85b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Training Data Status:\n",
            "Data directory exists: True\n",
            "Train file exists: True\n",
            "Validation file exists: True\n",
            "Test file exists: True\n",
            "Training samples: 8,104\n",
            "Validation samples: 1,519\n",
            "Test samples: 507\n",
            "\n",
            "📈 Total samples: 10,130\n",
            "Train/Val/Test split: 8104/1519/507\n"
          ]
        }
      ],
      "source": [
        "# Check training data availability\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"data/processed_llm_improved\")\n",
        "train_file = data_dir / \"train.jsonl\"\n",
        "val_file = data_dir / \"validation.jsonl\"\n",
        "test_file = data_dir / \"test.jsonl\"\n",
        "\n",
        "print(\"📊 Training Data Status:\")\n",
        "print(f\"Data directory exists: {data_dir.exists()}\")\n",
        "print(f\"Train file exists: {train_file.exists()}\")\n",
        "print(f\"Validation file exists: {val_file.exists()}\")\n",
        "print(f\"Test file exists: {test_file.exists()}\")\n",
        "\n",
        "# Initialize variables\n",
        "train_samples = 0\n",
        "val_samples = 0\n",
        "test_samples = 0\n",
        "\n",
        "if train_file.exists():\n",
        "    # Count samples\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        train_samples = sum(1 for line in f)\n",
        "    print(f\"Training samples: {train_samples:,}\")\n",
        "else:\n",
        "    print(\"❌ Training file not found\")\n",
        "\n",
        "if val_file.exists():\n",
        "    with open(val_file, 'r', encoding='utf-8') as f:\n",
        "        val_samples = sum(1 for line in f)\n",
        "    print(f\"Validation samples: {val_samples:,}\")\n",
        "else:\n",
        "    print(\"❌ Validation file not found\")\n",
        "\n",
        "if test_file.exists():\n",
        "    with open(test_file, 'r', encoding='utf-8') as f:\n",
        "        test_samples = sum(1 for line in f)\n",
        "    print(f\"Test samples: {test_samples:,}\")\n",
        "else:\n",
        "    print(\"❌ Test file not found\")\n",
        "\n",
        "total_samples = train_samples + val_samples + test_samples\n",
        "print(f\"\\n📈 Total samples: {total_samples:,}\")\n",
        "if total_samples > 0:\n",
        "    print(f\"Train/Val/Test split: {train_samples}/{val_samples}/{test_samples}\")\n",
        "else:\n",
        "    print(\"⚠️  No training data found. Check data path or run data preparation first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA66wIgLozQo",
        "outputId": "55ce984a-37d4-4cea-b314-afb198aac435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Sample Training Data:\n",
            "Sample structure:\n",
            "  - messages: <class 'list'>\n",
            "  - metadata: <class 'dict'>\n",
            "\n",
            "💬 Sample conversation:\n",
            "  1. user: What's 'We try.' in Italian?\n",
            "  2. assistant: Well done! The translation is 'Ci proviamo.'.\n",
            "\n",
            "📋 Metadata: {'conversation_id': 'translation_5261', 'source': 'tatoeba', 'level': 'A1', 'topic': 'general'}\n",
            "\n",
            "✅ Data structure looks good for training\n"
          ]
        }
      ],
      "source": [
        "# Sample data inspection\n",
        "print(\"🔍 Sample Training Data:\")\n",
        "\n",
        "if train_file.exists() and train_samples > 0:\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        # Read first sample\n",
        "        sample = json.loads(f.readline())\n",
        "\n",
        "    print(\"Sample structure:\")\n",
        "    for key in sample.keys():\n",
        "        print(f\"  - {key}: {type(sample[key])}\")\n",
        "\n",
        "    print(\"\\n💬 Sample conversation:\")\n",
        "    # Handle both 'messages' and 'conversation' formats\n",
        "    if 'messages' in sample:\n",
        "        conversation = sample['messages']\n",
        "    elif 'conversation' in sample:\n",
        "        conversation = sample['conversation']\n",
        "    else:\n",
        "        print(\"❌ Unknown conversation format in sample\")\n",
        "        conversation = []\n",
        "\n",
        "    for i, msg in enumerate(conversation[:4]):  # Show first 4 messages\n",
        "        role = msg.get('role', 'unknown')\n",
        "        content = msg.get('content', '')\n",
        "        content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
        "        print(f\"  {i+1}. {role}: {content_preview}\")\n",
        "\n",
        "    if 'metadata' in sample:\n",
        "        print(f\"\\n📋 Metadata: {sample['metadata']}\")\n",
        "\n",
        "    print(\"\\n✅ Data structure looks good for training\")\n",
        "else:\n",
        "    print(\"❌ No training data available for inspection\")\n",
        "    print(\"Please ensure data files are in the correct location:\")\n",
        "    print(f\"  Expected: {train_file}\")\n",
        "    print(\"  Or run data preparation pipeline first\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get default configuration and customize for detected GPU\n",
        "config = get_default_config()\n",
        "\n",
        "# Override with detected optimal settings\n",
        "if torch.cuda.is_available():\n",
        "    # Use the recommended settings from GPU detection\n",
        "    if 'recommended_batch_size' in locals():\n",
        "        config.training.per_device_train_batch_size = recommended_batch_size\n",
        "    if 'recommended_eval_batch_size' in locals():\n",
        "        config.training.per_device_eval_batch_size = recommended_eval_batch_size\n",
        "    if 'gradient_accumulation' in locals():\n",
        "        config.training.gradient_accumulation_steps = gradient_accumulation\n",
        "    if 'pin_memory' in locals():\n",
        "        config.training.dataloader_pin_memory = pin_memory\n",
        "\n",
        "# Customize training settings\n",
        "config.training.num_train_epochs = 3  # Start with 3 epochs\n",
        "config.training.output_dir = \"./marco_lora_checkpoints\"\n",
        "\n",
        "# Set run name based on GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    config.training.run_name = f\"marco-lora-{gpu_name.lower().replace(' ', '-')}\"\n",
        "else:\n",
        "    config.training.run_name = \"marco-lora-cpu\"\n",
        "\n",
        "# Enable experiment tracking (optional)\n",
        "config.experiment.use_wandb = False  # Set to True if you want wandb\n",
        "config.experiment.experiment_name = f\"marco-italian-teacher-{pd.Timestamp.now().strftime('%Y%m%d-%H%M')}\"\n",
        "\n",
        "print(\"⚙️  Training Configuration:\")\n",
        "print(f\"Model: {config.training.model_name}\")\n",
        "print(f\"Train batch size: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"Eval batch size: {config.training.per_device_eval_batch_size}\")\n",
        "print(f\"Gradient accumulation: {config.training.gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {config.training.per_device_train_batch_size * config.training.gradient_accumulation_steps}\")\n",
        "print(f\"Pin memory: {config.training.dataloader_pin_memory}\")\n",
        "print(f\"Learning rate: {config.training.learning_rate}\")\n",
        "print(f\"Epochs: {config.training.num_train_epochs}\")\n",
        "print(f\"LoRA rank: {config.lora.r}\")\n",
        "print(f\"LoRA alpha: {config.lora.lora_alpha}\")\n",
        "print(f\"Max sequence length: {config.data.max_length}\")\n",
        "print(f\"Output directory: {config.training.output_dir}\")\n",
        "print(f\"Experiment tracking: {'Enabled' if config.experiment.use_wandb else 'Disabled'}\")\n",
        "if 'training_speed' in locals():\n",
        "    print(f\"Estimated training time: {training_speed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgIkHx5Y7DcZ",
        "outputId": "35e91f8f-aae1-41f2-e9b9-98e7cbf4fd54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️  Training Configuration:\n",
            "Model: Qwen/Qwen2.5-7B-Instruct\n",
            "Train batch size: 3\n",
            "Eval batch size: 4\n",
            "Gradient accumulation: 3\n",
            "Effective batch size: 9\n",
            "Pin memory: True\n",
            "Learning rate: 0.0002\n",
            "Epochs: 3\n",
            "LoRA rank: 16\n",
            "LoRA alpha: 32\n",
            "Max sequence length: 1024\n",
            "Output directory: ./marco_lora_checkpoints\n",
            "Experiment tracking: Disabled\n",
            "Estimated training time: ~2-3 hours for 3 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Configuration & Initialization"
      ],
      "metadata": {
        "id": "03cKH5WU7Dca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize trainer (this will load the model)\n",
        "print(\"🚀 Initializing Marco LoRA Trainer...\")\n",
        "print(\"This will download and load Qwen2.5-7B-Instruct (may take a few minutes)\")\n",
        "\n",
        "# Check if we have training data before proceeding\n",
        "if total_samples == 0:\n",
        "    print(\"❌ No training data found. Cannot proceed with training.\")\n",
        "    print(\"Please ensure your data files are available at:\")\n",
        "    print(f\"  Train: {train_file}\")\n",
        "    print(f\"  Validation: {val_file}\")\n",
        "    print(f\"  Test: {test_file}\")\n",
        "    print(\"\\nTo fix this:\")\n",
        "    print(\"1. Check if the data path is correct\")\n",
        "    print(\"2. Run data preparation pipeline if needed\")\n",
        "    print(\"3. Or update the data paths in the config\")\n",
        "else:\n",
        "    print(f\"✅ Found {total_samples:,} training samples\")\n",
        "\n",
        "    try:\n",
        "        trainer = MarcoLoRATrainer(config=config)\n",
        "        print(\"✅ Trainer initialized successfully\")\n",
        "        print(f\"GPU memory after model loading: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to initialize trainer: {e}\")\n",
        "        print(\"This might be due to:\")\n",
        "        print(\"1. Missing packages (restart runtime after installing)\")\n",
        "        print(\"2. Insufficient GPU memory\")\n",
        "        print(\"3. Internet connection issues for model download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8_40R-W7Dca",
        "outputId": "5d90e2dc-9e45-403b-ceda-160a31adcc2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing Marco LoRA Trainer...\n",
            "This will download and load Qwen2.5-7B-Instruct (may take a few minutes)\n",
            "✅ Found 10,130 training samples\n",
            "🚀 L4 GPU detected: Using high-performance settings\n",
            "   Effective batch size: 9\n",
            "   Memory optimization: Enabled\n",
            "✅ Trainer initialized successfully\n",
            "GPU memory after model loading: 0.00 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup model components (tokenizer, LoRA, data)\n",
        "print(\"🔧 Setting up model components...\")\n",
        "\n",
        "# Check if trainer was successfully initialized\n",
        "if 'trainer' not in locals():\n",
        "    print(\"❌ Trainer not initialized. Please run the previous cell successfully first.\")\n",
        "    print(\"Cannot proceed with model setup without trainer.\")\n",
        "else:\n",
        "    try:\n",
        "        print(\"Loading tokenizer and model...\")\n",
        "        trainer.setup_model_and_tokenizer()\n",
        "        print(f\"GPU memory after base model: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
        "\n",
        "        print(\"Applying LoRA configuration...\")\n",
        "        trainer.setup_lora()\n",
        "        print(f\"GPU memory after LoRA: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
        "\n",
        "        print(\"Preparing training datasets...\")\n",
        "        trainer.setup_data()\n",
        "\n",
        "        print(\"\\n✅ All components ready for training\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Setup failed: {e}\")\n",
        "        print(\"This might be due to:\")\n",
        "        print(\"1. GPU memory issues (try smaller batch size)\")\n",
        "        print(\"2. Data loading problems (check file paths)\")\n",
        "        print(\"3. Network issues (model download interrupted)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "dce87a0f8aca4a9ea0484b230090be06",
            "50c6935df9f941948063dfd51fe2c835",
            "59cdf7dce098429990e04dfa9f649e62",
            "cfba2de461584b65aaf11513f78c7a09",
            "f06518b21ed046a0b7a7e37292691c1a",
            "49754e059667485291195d226f6b45e9",
            "93852ecd329a46cb9088c7ac4c5890de",
            "aebb73f234ff44ddbad91c8ecd0b83d3",
            "967f0462120744c294308bf5a989b44b",
            "7a460b935a9a49ec9ed1824fa3d1a99c",
            "796e9fc4a32e45a8b86e942fc093d9db"
          ]
        },
        "id": "EejdtLV27hZQ",
        "outputId": "4eb879f1-5d09-4dca-fd09-a04e4a4f0bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Setting up model components...\n",
            "Loading tokenizer and model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dce87a0f8aca4a9ea0484b230090be06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory after base model: 7.21 GB\n",
            "Applying LoRA configuration...\n",
            "GPU memory after LoRA: 7.36 GB\n",
            "Preparing training datasets...\n",
            "\n",
            "✅ All components ready for training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZOlQcQNozQp",
        "outputId": "d7f7b815-65bc-425a-a184-f4661a934d7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Training Setup Verification:\n",
            "Training samples: 8,104\n",
            "Validation samples: 1,519\n",
            "\n",
            "💾 Memory Usage:\n",
            "Used: 7.36 GB / 22.2 GB (33.2%)\n",
            "✅ Good memory usage. Could potentially increase batch size.\n",
            "\n",
            "⏱️  Training Estimates:\n",
            "Steps per epoch: 900\n",
            "Total training steps: 2700\n",
            "Estimated training time: 18.0 hours\n",
            "Performance profile: High-performance on L4 🚀\n",
            "\n",
            "🚦 Ready to start training!\n"
          ]
        }
      ],
      "source": [
        "# Verify training setup\n",
        "print(\"🔍 Training Setup Verification:\")\n",
        "\n",
        "# Check if trainer exists and has datasets\n",
        "if 'trainer' not in locals():\n",
        "    print(\"❌ Trainer not initialized. Please run the previous cells successfully.\")\n",
        "elif not hasattr(trainer, 'datasets') or trainer.datasets is None:\n",
        "    print(\"❌ Datasets not loaded. Please run the setup cell above successfully first.\")\n",
        "    print(\"   The setup cell loads the model, applies LoRA, and prepares datasets.\")\n",
        "else:\n",
        "    # Check datasets\n",
        "    print(f\"Training samples: {len(trainer.datasets['train']):,}\")\n",
        "    if 'validation' in trainer.datasets:\n",
        "        print(f\"Validation samples: {len(trainer.datasets['validation']):,}\")\n",
        "\n",
        "    # Memory check\n",
        "    if torch.cuda.is_available():\n",
        "        memory_used = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "        memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        memory_percent = (memory_used / memory_total) * 100\n",
        "\n",
        "        print(f\"\\n💾 Memory Usage:\")\n",
        "        print(f\"Used: {memory_used:.2f} GB / {memory_total:.1f} GB ({memory_percent:.1f}%)\")\n",
        "\n",
        "        if memory_percent > 85:\n",
        "            print(\"⚠️  High memory usage. Consider reducing batch size.\")\n",
        "        elif memory_percent < 50:\n",
        "            print(\"✅ Good memory usage. Could potentially increase batch size.\")\n",
        "        else:\n",
        "            print(\"✅ Optimal memory usage for training.\")\n",
        "\n",
        "        # Estimate training time\n",
        "        total_samples = len(trainer.datasets['train'])\n",
        "        effective_batch_size = config.training.per_device_train_batch_size * config.training.gradient_accumulation_steps\n",
        "        steps_per_epoch = total_samples // effective_batch_size\n",
        "        total_steps = steps_per_epoch * config.training.num_train_epochs\n",
        "\n",
        "        print(f\"\\n⏱️  Training Estimates:\")\n",
        "        print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "        print(f\"Total training steps: {total_steps}\")\n",
        "\n",
        "        # GPU-specific time estimates\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        if \"T4\" in gpu_name:\n",
        "            estimated_hours = total_steps * 0.8 / 60  # ~0.8 min per step on T4\n",
        "            performance_note = \"Memory-optimized for T4\"\n",
        "        elif \"L4\" in gpu_name:\n",
        "            estimated_hours = total_steps * 0.4 / 60  # ~0.4 min per step on L4\n",
        "            performance_note = \"High-performance on L4 🚀\"\n",
        "        elif \"A100\" in gpu_name:\n",
        "            estimated_hours = total_steps * 0.3 / 60  # ~0.3 min per step on A100\n",
        "            performance_note = \"Maximum performance on A100\"\n",
        "        else:\n",
        "            estimated_hours = total_steps * 1.0 / 60  # Conservative estimate\n",
        "            performance_note = \"Conservative estimate for unknown GPU\"\n",
        "\n",
        "        print(f\"Estimated training time: {estimated_hours:.1f} hours\")\n",
        "        print(f\"Performance profile: {performance_note}\")\n",
        "\n",
        "        print(\"\\n🚦 Ready to start training!\")\n",
        "    else:\n",
        "        print(\"\\n❌ No GPU detected - training will be extremely slow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dToaciNbozQq",
        "outputId": "bd907bd9-3b0e-4bb2-a839-5ac3284ea6e0"
      },
      "source": [
        "## 5. Fine-Tuning Execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Setup Weights & Biases for tracking\n",
        "if config.experiment.use_wandb:\n",
        "    try:\n",
        "        import wandb\n",
        "\n",
        "        # You may need to login to wandb first\n",
        "        # wandb.login()  # Uncomment if needed\n",
        "\n",
        "        trainer.setup_wandb()\n",
        "        print(\"✅ Weights & Biases tracking enabled\")\n",
        "        print(f\"Experiment: {config.experiment.experiment_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  W&B setup failed: {e}\")\n",
        "        print(\"Training will continue without experiment tracking\")\n",
        "        config.experiment.use_wandb = False\n",
        "else:\n",
        "    print(\"📊 Training without experiment tracking\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjD7O5KY8jsx",
        "outputId": "6869105f-f1c8-493d-eb51-ffc02600c5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Training without experiment tracking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training!\n",
        "print(\"🚀 Starting Marco LoRA Fine-Tuning...\")\n",
        "print(\"This will take ~2-3 hours on L4 GPU. Monitor the progress below.\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Run training\n",
        "try:\n",
        "    # Note: This calls the complete training pipeline\n",
        "    # The trainer handles all setup internally\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🎉 Training completed successfully!\")\n",
        "    print(f\"📁 Model saved to: {config.training.output_dir}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⏹️  Training interrupted by user\")\n",
        "    print(\"Partial model may be saved in checkpoints\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Training failed with error: {e}\")\n",
        "    print(\"Check the error details above\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ef78c826cb084607bbfe6a4970225d15",
            "9f123c7f53a3421991433e3e35282721",
            "412cd12a54794b7fb18570bb8d9a0e15",
            "96249aeb437f42f589027ec219d4788b",
            "2737964cc578420a945cf3312b7394e9",
            "1d19cee99b364c2a9974beed2c7a3192",
            "5deec450d7d748f1ac472ea4de4da7b5",
            "062cbc6a23e2441188de31ad4e1b4262",
            "46614f8808874dd5b42489ac3fa3b90c",
            "e6880bbdd1b643da9d78b9b7ddb6689a",
            "c3e569f60fbc48e2b37c4a697947fe58"
          ]
        },
        "id": "Zg45Daw4yHGd",
        "outputId": "7874501f-e69b-47cc-863b-84e0429e0114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Marco LoRA Fine-Tuning...\n",
            "This will take ~2-3 hours on L4 GPU. Monitor the progress below.\n",
            "\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef78c826cb084607bbfe6a4970225d15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/italian_teacher/src/fine_tuning/lora_trainer.py:232: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  self.trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mari-katzir\u001b[0m (\u001b[33mariel-katzir\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (0.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/italian_teacher/wandb/run-20250919_185052-apdn6sq5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ariel-katzir/huggingface/runs/apdn6sq5' target=\"_blank\">marco-lora-nvidia-l4</a></strong> to <a href='https://wandb.ai/ariel-katzir/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ariel-katzir/huggingface' target=\"_blank\">https://wandb.ai/ariel-katzir/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ariel-katzir/huggingface/runs/apdn6sq5' target=\"_blank\">https://wandb.ai/ariel-katzir/huggingface/runs/apdn6sq5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2702' max='2703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2702/2703 3:38:41 < 00:05, 0.17 it/s, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.440300</td>\n",
              "      <td>0.588308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.526400</td>\n",
              "      <td>0.579639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.424400</td>\n",
              "      <td>0.575169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.416700</td>\n",
              "      <td>0.568732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.332800</td>\n",
              "      <td>0.573220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.365500</td>\n",
              "      <td>0.571508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.449600</td>\n",
              "      <td>0.566036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.388800</td>\n",
              "      <td>0.557911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.424300</td>\n",
              "      <td>0.554573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.432700</td>\n",
              "      <td>0.558335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.388600</td>\n",
              "      <td>0.550103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.409400</td>\n",
              "      <td>0.540427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.365900</td>\n",
              "      <td>0.539931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.299000</td>\n",
              "      <td>0.562959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.345200</td>\n",
              "      <td>0.568461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.253700</td>\n",
              "      <td>0.570468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.314600</td>\n",
              "      <td>0.567769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.296900</td>\n",
              "      <td>0.565992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.332600</td>\n",
              "      <td>0.566889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.210300</td>\n",
              "      <td>0.566253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.278300</td>\n",
              "      <td>0.566834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.272800</td>\n",
              "      <td>0.566782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "🎉 Training completed successfully!\n",
            "📁 Model saved to: ./marco_lora_checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqpumMxEozQq",
        "outputId": "fae00869-6b9e-4137-abe2-c7ab1497e20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Training Setup Verification:\n",
            "Training samples: 8,104\n",
            "Validation samples: 1,519\n",
            "\n",
            "💾 Memory Usage:\n",
            "Used: 15.35 GB / 22.2 GB (69.3%)\n",
            "✅ Optimal memory usage for training.\n",
            "\n",
            "⏱️  Training Estimates:\n",
            "Steps per epoch: 900\n",
            "Total training steps: 2700\n",
            "Estimated training time: 18.0 hours\n",
            "Performance profile: High-performance on L4 🚀\n",
            "\n",
            "🚦 Ready to start training!\n"
          ]
        }
      ],
      "source": [
        "# Verify training setup\n",
        "print(\"🔍 Training Setup Verification:\")\n",
        "\n",
        "# Check if trainer exists and has datasets\n",
        "if 'trainer' not in locals():\n",
        "    print(\"❌ Trainer not initialized. Please run previous cells successfully.\")\n",
        "elif not hasattr(trainer, 'datasets') or trainer.datasets is None:\n",
        "    print(\"❌ Datasets not loaded. Please run the setup cell successfully.\")\n",
        "else:\n",
        "    # Check datasets\n",
        "    print(f\"Training samples: {len(trainer.datasets['train']):,}\")\n",
        "    if 'validation' in trainer.datasets:\n",
        "        print(f\"Validation samples: {len(trainer.datasets['validation']):,}\")\n",
        "\n",
        "    # Memory check\n",
        "    if torch.cuda.is_available():\n",
        "        memory_used = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "        memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        memory_percent = (memory_used / memory_total) * 100\n",
        "\n",
        "        print(f\"\\n💾 Memory Usage:\")\n",
        "        print(f\"Used: {memory_used:.2f} GB / {memory_total:.1f} GB ({memory_percent:.1f}%)\")\n",
        "\n",
        "        if memory_percent > 85:\n",
        "            print(\"⚠️  High memory usage. Consider reducing batch size.\")\n",
        "        elif memory_percent < 50:\n",
        "            print(\"✅ Good memory usage. Could potentially increase batch size.\")\n",
        "        else:\n",
        "            print(\"✅ Optimal memory usage for training.\")\n",
        "\n",
        "        # Estimate training time\n",
        "        total_samples = len(trainer.datasets['train'])\n",
        "        effective_batch_size = config.training.per_device_train_batch_size * config.training.gradient_accumulation_steps\n",
        "        steps_per_epoch = total_samples // effective_batch_size\n",
        "        total_steps = steps_per_epoch * config.training.num_train_epochs\n",
        "\n",
        "        print(f\"\\n⏱️  Training Estimates:\")\n",
        "        print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "        print(f\"Total training steps: {total_steps}\")\n",
        "\n",
        "        # GPU-specific time estimates\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        if \"T4\" in gpu_name:\n",
        "            estimated_hours = total_steps * 0.8 / 60  # ~0.8 min per step on T4\n",
        "            performance_note = \"Memory-optimized for T4\"\n",
        "        elif \"L4\" in gpu_name:\n",
        "            estimated_hours = total_steps * 0.4 / 60  # ~0.4 min per step on L4\n",
        "            performance_note = \"High-performance on L4 🚀\"\n",
        "        elif \"A100\" in gpu_name:\n",
        "            estimated_hours = total_steps * 0.3 / 60  # ~0.3 min per step on A100\n",
        "            performance_note = \"Maximum performance on A100\"\n",
        "        else:\n",
        "            estimated_hours = total_steps * 1.0 / 60  # Conservative estimate\n",
        "            performance_note = \"Conservative estimate for unknown GPU\"\n",
        "\n",
        "        print(f\"Estimated training time: {estimated_hours:.1f} hours\")\n",
        "        print(f\"Performance profile: {performance_note}\")\n",
        "\n",
        "        print(\"\\n🚦 Ready to start training!\")\n",
        "    else:\n",
        "        print(\"\\n❌ No GPU detected - training will be extremely slow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Gp1SMIozQr"
      },
      "source": [
        "## 5. Fine-Tuning Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoYYQBiLozQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b221dd5-0538-475b-c1f6-57b147f02e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Training without experiment tracking\n"
          ]
        }
      ],
      "source": [
        "# Optional: Setup Weights & Biases for tracking\n",
        "if config.experiment.use_wandb:\n",
        "    try:\n",
        "        import wandb\n",
        "\n",
        "        # You may need to login to wandb first\n",
        "        # wandb.login()  # Uncomment if needed\n",
        "\n",
        "        trainer.setup_wandb()\n",
        "        print(\"✅ Weights & Biases tracking enabled\")\n",
        "        print(f\"Experiment: {config.experiment.experiment_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  W&B setup failed: {e}\")\n",
        "        print(\"Training will continue without experiment tracking\")\n",
        "        config.experiment.use_wandb = False\n",
        "else:\n",
        "    print(\"📊 Training without experiment tracking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYenHNNAozQr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "daacf7dc4afe4ed98a70a6173b6a697f",
            "75780c02748b4fc98ee24c491276202f",
            "a4100e9d646f4c209e26c79339563f1a",
            "f8c5e3db937b403cb5183f779c41185f",
            "1a6521fe15034623b88beaa025a273d5",
            "063e69493945473ca3f81b5e0911776e",
            "32276e2d9ece452d8152560bd54e6ddb",
            "f9c2ee44629d41ea8840fd43ffa1c5f2",
            "af42a22be407490586effb2caf439e18",
            "e9615672a0fb45258d3c76ef10827f10",
            "14e831a3057f4394a2874d3ee87c7254"
          ]
        },
        "outputId": "58f4b582-bdbf-4ba6-ecea-8ccec74f6686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Marco LoRA Fine-Tuning...\n",
            "This will take several hours. Monitor the progress below.\n",
            "\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daacf7dc4afe4ed98a70a6173b6a697f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏹️  Training interrupted by user\n",
            "Partial model may be saved in checkpoints\n"
          ]
        }
      ],
      "source": [
        "# Start training!\n",
        "print(\"🚀 Starting Marco LoRA Fine-Tuning...\")\n",
        "print(\"This will take several hours. Monitor the progress below.\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Run training\n",
        "try:\n",
        "    # Note: This calls the complete training pipeline\n",
        "    # The trainer handles all setup internally\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🎉 Training completed successfully!\")\n",
        "    print(f\"📁 Model saved to: {config.training.output_dir}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⏹️  Training interrupted by user\")\n",
        "    print(\"Partial model may be saved in checkpoints\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Training failed with error: {e}\")\n",
        "    print(\"Check the error details above\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2a8g91sozQr"
      },
      "source": [
        "## 6. Model Testing & Quick Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcSkkXbnozQr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "abd56e86f06c49188256e5e377416a7f",
            "de2c520fa93a487786067a9ddbdccfa2",
            "8f2095778d84460eb151fc24ef77ecff",
            "c99321a28a524aec83a5d43d07096cca",
            "ca88aaaeb98247e5bdca810fed2b930f",
            "a6f7de3d2b834ed48cb4a291a1b70a20",
            "824903a33ad04815aa904b644e81079b",
            "2e6a7783030943fe816221436b2f6432",
            "05bcf9b7214841b184f26562f971f033",
            "2f1513dcfae041fd9bccd2a1c8145a9a",
            "e48398d39ec747a6af0aaef0d81250cd"
          ]
        },
        "outputId": "4e1b07d5-d7d4-455e-d875-b33fd233a571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Testing trained Marco model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abd56e86f06c49188256e5e377416a7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Trained Marco model loaded for testing\n"
          ]
        }
      ],
      "source": [
        "# Test the trained model\n",
        "print(\"🧪 Testing trained Marco model...\")\n",
        "\n",
        "# Initialize inference with trained LoRA adapter\n",
        "marco = MarcoInference(\n",
        "    lora_adapter_path=config.training.output_dir\n",
        ")\n",
        "\n",
        "print(\"✅ Trained Marco model loaded for testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwUSUQYFozQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e639b3-5c7b-40b2-b0cf-c665913486be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💬 Quick Inference Tests:\n",
            "============================================================\n",
            "\n",
            "🙋 Test 1: What does 'Buongiorno' mean?\n",
            "🤖 Marco: It's 'Good morning'.\n",
            "----------------------------------------\n",
            "\n",
            "🙋 Test 2: Can you explain the grammar in 'Ho mangiato una pizza'?\n",
            "🤖 Marco: Great question! This translates to 'I ate a pizza.'. This sentence uses role-based morphology. The subject is \"Io\" (I), which is in the first person singular form and agrees with the verb \"ho mangiato\" (I have eaten). The object of the sentence is \"una pizza\" (a pizza), which is also in the singular form and agrees with the verb. The verb \"mangiare\" (to eat) is conjugated in the present tense, third person singular, to agree with the subject \"Io\". \n",
            "\n",
            "So in summary:\n",
            "- Subject: Io (first person singular)\n",
            "- Verb: ho mangiato (present tense, third person singular)\n",
            "- Object: una pizza (singular)\n",
            "\n",
            "The word order follows standard Italian syntax, with the subject followed by the verb and then the object.\n",
            "----------------------------------------\n",
            "\n",
            "🙋 Test 3: Help me practice Italian greetings at A1 level\n",
            "🤖 Marco: Great job! Here's the translation: 'Ciao! Io sono Stella.'.\n",
            "----------------------------------------\n",
            "\n",
            "🙋 Test 4: What's the difference between 'essere' and 'stare'?\n",
            "🤖 Marco: Great question! These verbs are often confused. \"Essere\" is used to express existence or state of being, while \"stare\" is used to express location or position. For example, \"Sono alzato\" means \"I am standing up\" (using \"essere\"), but \"Sto alzato\" means \"I am standing\" (using \"stare\"). The verb \"essere\" can also be used with some adjectives to describe permanent characteristics, such as \"Sono vecchio\" (I am old) or \"Siamo felici\" (We are happy). In contrast, \"stare\" is typically used with temporary states or situations, like \"Sto male\" (I feel bad) or \"Stiamo in piedi\" (We are standing).\n",
            "----------------------------------------\n",
            "\n",
            "🙋 Test 5: Translate and explain: 'Sto studiando l'italiano da due anni'\n",
            "🤖 Marco: Great question! This translates to 'I've been studying Italian for two years.'. This sentence uses role and argument structure as follows:\n",
            "\n",
            "- Subject: \"Sto\" (I am)\n",
            "- Verb: \"studiando\" (studying)\n",
            "- Object: \"l'\" (the) + \"italiano\" (Italian)\n",
            "\n",
            "The sentence is in the present tense, first person singular, indicating that the speaker is currently studying the language. The object \"l'\" + \"italiano\" is a direct object, referring to the language being studied.\n",
            "\n",
            "The word order follows standard Italian syntax, with the subject followed by the verb, then the object. \n",
            "\n",
            "In summary:\n",
            "- \"Sto\" - I am\n",
            "- \"studiando\" - studying\n",
            "- \"l'\" + \"italiano\" - the Italian\n",
            "\n",
            "This simple yet grammatically correct sentence conveys the speaker's ongoing effort to learn Italian over a period of two years.\n",
            "----------------------------------------\n",
            "\n",
            "✅ Quick testing complete!\n"
          ]
        }
      ],
      "source": [
        "# Quick conversation tests\n",
        "test_questions = [\n",
        "    \"What does 'Buongiorno' mean?\",\n",
        "    \"Can you explain the grammar in 'Ho mangiato una pizza'?\",\n",
        "    \"Help me practice Italian greetings at A1 level\",\n",
        "    \"What's the difference between 'essere' and 'stare'?\",\n",
        "    \"Translate and explain: 'Sto studiando l'italiano da due anni'\"\n",
        "]\n",
        "\n",
        "print(\"💬 Quick Inference Tests:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n🙋 Test {i}: {question}\")\n",
        "\n",
        "    try:\n",
        "        response = marco.chat(question)\n",
        "        print(f\"🤖 Marco: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n✅ Quick testing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogIgn0KVozQs"
      },
      "source": [
        "## 7. Evaluation & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5JXJ7rWozQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691d590f-2827-4f9d-e20d-b71da4140daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 No training logs found for analysis\n"
          ]
        }
      ],
      "source": [
        "# Training metrics analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Check for training logs\n",
        "log_file = Path(config.training.output_dir) / \"trainer_state.json\"\n",
        "\n",
        "if log_file.exists():\n",
        "    print(\"📊 Analyzing training metrics...\")\n",
        "\n",
        "    with open(log_file, 'r') as f:\n",
        "        trainer_state = json.load(f)\n",
        "\n",
        "    # Extract training history\n",
        "    log_history = trainer_state.get('log_history', [])\n",
        "\n",
        "    if log_history:\n",
        "        # Create DataFrames for analysis\n",
        "        train_logs = [log for log in log_history if 'train_loss' in log]\n",
        "        eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
        "\n",
        "        if train_logs:\n",
        "            train_df = pd.DataFrame(train_logs)\n",
        "\n",
        "            # Plot training loss\n",
        "            plt.figure(figsize=(12, 5))\n",
        "\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(train_df['step'], train_df['train_loss'], 'b-', linewidth=2)\n",
        "            plt.title('Training Loss')\n",
        "            plt.xlabel('Step')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot learning rate\n",
        "            plt.subplot(1, 2, 2)\n",
        "            if 'learning_rate' in train_df.columns:\n",
        "                plt.plot(train_df['step'], train_df['learning_rate'], 'g-', linewidth=2)\n",
        "                plt.title('Learning Rate Schedule')\n",
        "                plt.xlabel('Step')\n",
        "                plt.ylabel('Learning Rate')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Training summary\n",
        "            final_loss = train_df['train_loss'].iloc[-1]\n",
        "            initial_loss = train_df['train_loss'].iloc[0]\n",
        "            improvement = ((initial_loss - final_loss) / initial_loss) * 100\n",
        "\n",
        "            print(f\"\\n📈 Training Summary:\")\n",
        "            print(f\"Initial loss: {initial_loss:.4f}\")\n",
        "            print(f\"Final loss: {final_loss:.4f}\")\n",
        "            print(f\"Improvement: {improvement:.1f}%\")\n",
        "\n",
        "        if eval_logs:\n",
        "            eval_df = pd.DataFrame(eval_logs)\n",
        "            print(f\"\\n📊 Validation Results:\")\n",
        "            print(f\"Final validation loss: {eval_df['eval_loss'].iloc[-1]:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"📋 No training logs found for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NHlUUJAtozQs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850,
          "referenced_widgets": [
            "b0bf33b0435c429e958cefef7fe9071e",
            "e42d63d0f7af432f94b50bed1a5f2b62",
            "148e682d31144a0db13b17c99130e49e",
            "5a86317710524108b14521ee943fbea9",
            "3bef6b85d164472fac845afb6d575fce",
            "d97fb02dd89f414a80e5f730dde9ce78",
            "5acce7c6f6e4420f848692f19cd09f3f",
            "1f68506135b14ea79fc236fdb2186c84",
            "bcec9858c1b145bf82c7fba5a3ff5e37",
            "a34731bb48bc47b8a0863102ca86dee6",
            "b0b2d7b95d4247bdb388407c04379520"
          ]
        },
        "outputId": "83e65cba-12c5-46ec-9436-4a4778ffaa2a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Comparing Fine-tuned vs Base Model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0bf33b0435c429e958cefef7fe9071e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "\n",
            "🔍 Comparison Test 1: Explain the grammar in 'Sono andato al mare'\n",
            "------------------------------------------------------------\n",
            "🤖 Base Model:\n",
            "The phrase \"Sono andato al mare\" is a common Italian sentence that translates to English as \"I went to the sea\" or \"I went to the beach.\" Let's break down the grammar:\n",
            "\n",
            "1. **Sono**: This is the first-person singular form of the verb \"essere\" (to be) in the present tense. It means \"I am,\" but in this...\n",
            "\n",
            "🎓 Fine-tuned Marco:\n",
            "Great question! This translates to 'I went to the beach.'. This sentence uses role reversal, which is a grammatical structure where the subject and object are switched. In this case, \"Sono\" (I am) is the subject, and \"andato al mare\" (went to the sea) is the object.\n",
            "\n",
            "The verb \"andare\" (to go) is con...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "🔍 Comparison Test 2: What's the difference between 'molto' and 'troppo'?\n",
            "------------------------------------------------------------\n",
            "🤖 Base Model:\n",
            "In Italian, both \"molto\" and \"troppo\" can be used to express the degree of something, but they have different meanings and uses:\n",
            "\n",
            "1. **Molto**:\n",
            "   - **Meaning**: \"Molto\" means \"very\" or \"much.\"\n",
            "   - **Usage**: It is used to indicate that something is in a high degree or to a great extent.\n",
            "   - **Exa...\n",
            "\n",
            "🎓 Fine-tuned Marco:\n",
            "Bravissimo! This B2 level expression showcases using advanced subjunctive forms - typical of formal or literary Italian.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "🔍 Comparison Test 3: Help me understand when to use the subjunctive mood\n",
            "------------------------------------------------------------\n",
            "🤖 Base Model:\n",
            "The subjunctive mood is used in English to express wishes, suggestions, demands, or other hypothetical or non-factual situations. It's often used after certain verbs, adjectives, and phrases that indicate such meanings. Here are some key points to help you understand when to use the subjunctive mood...\n",
            "\n",
            "🎓 Fine-tuned Marco:\n",
            "Great question! This is common in formal or literary Italian.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "✅ Model comparison complete!\n"
          ]
        }
      ],
      "source": [
        "# Compare with base model (before fine-tuning)\n",
        "print(\"🔄 Comparing Fine-tuned vs Base Model...\")\n",
        "\n",
        "# Load base model for comparison\n",
        "base_marco = MarcoInference()  # No LoRA adapter = base model\n",
        "\n",
        "comparison_questions = [\n",
        "    \"Explain the grammar in 'Sono andato al mare'\",\n",
        "    \"What's the difference between 'molto' and 'troppo'?\",\n",
        "    \"Help me understand when to use the subjunctive mood\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "for i, question in enumerate(comparison_questions, 1):\n",
        "    print(f\"\\n🔍 Comparison Test {i}: {question}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Base model response\n",
        "    print(\"🤖 Base Model:\")\n",
        "    try:\n",
        "        base_response = base_marco.chat(question)\n",
        "        print(f\"{base_response[:300]}{'...' if len(base_response) > 300 else ''}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "\n",
        "    print(\"\\n🎓 Fine-tuned Marco:\")\n",
        "    try:\n",
        "        tuned_response = marco.chat(question)\n",
        "        print(f\"{tuned_response[:300]}{'...' if len(tuned_response) > 300 else ''}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "print(\"\\n✅ Model comparison complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XC-VNr-qozQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c97ff7-6ea0-4fe7-a4f1-1bf839f2a1eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Testing Marco across different CEFR levels...\n",
            "\n",
            "📚 A1 Level Test:\n",
            "Question: Help me learn basic Italian greetings\n",
            "--------------------------------------------------\n",
            "Marco: Great question! The translation is 'Salve! Io sono Stella.'.\n",
            "\n",
            "\n",
            "\n",
            "📚 A2 Level Test:\n",
            "Question: Explain how to talk about daily routines in Italian\n",
            "--------------------------------------------------\n",
            "Marco: Bravissimo! Here's how you would say it: 'Parliamo delle routine quotidiane.'.\n",
            "\n",
            "\n",
            "\n",
            "📚 B1 Level Test:\n",
            "Question: What's the difference between passato prossimo and imperfetto?\n",
            "--------------------------------------------------\n",
            "Marco: Great question! In Italian, \"passato prossimo\" is used to describe actions that happened in the past and have ended. It's formed by adding the auxiliary verb \"essere\" or \"essere\" (for verbs like \"andare\" and \"venire\") to the past participle of the main verb.\n",
            "\n",
            "On the other hand, \"imperfetto\" is used to describe ongoing actions in the past. It's formed by adding the endings \"-avo/-evi/-iva/-ivamo/-ivate/-ivano\" to the base form of the verb.\n",
            "\n",
            "Here's an example:\n",
            "\n",
            "- \"Ho mangiato il pane.\" (I ate the bread.) - Passato Prossimo\n",
            "- \"Mangiavo il pane.\" (I was eating the bread.) - Imperfetto\n",
            "\n",
            "The main difference is that \"passato prossimo\" describes completed actions, while \"imperfetto\" describes actions that were happening at a specific time in the past but haven't necessarily finished.\n",
            "\n",
            "\n",
            "\n",
            "📚 B2 Level Test:\n",
            "Question: Explain the use of the conditional mood in Italian\n",
            "--------------------------------------------------\n",
            "Marco: Great question! This translates to 'Use the conditional mood in Italian.'. In Italian, the conditional mood is used to express hypothetical situations or actions that would happen if certain conditions were met. It's formed by adding \"avrebbe\" (would have) to the infinitive form of the verb. \n",
            "\n",
            "For example:\n",
            "- \"Avrei studiato più duramente se avessi saputo quanto sarebbe stato importante.\" - \"I would have studied harder if I had known how important it would be.\"\n",
            "\n",
            "The conditional mood is often used with \"se\" (if) and \"quando\" (when) clauses. Here are some examples:\n",
            "\n",
            "1. Se avessi tempo libero, viaggerei in tutti i paesi europei.\n",
            "   If I had free time, I would travel to all European countries.\n",
            "\n",
            "2. Quando arriverai, vieni da me a mangiare qualcosa.\n",
            "\n",
            "\n",
            "✅ CEFR level testing complete!\n"
          ]
        }
      ],
      "source": [
        "# Generate example conversations for different CEFR levels\n",
        "print(\"🎯 Testing Marco across different CEFR levels...\")\n",
        "\n",
        "cefr_tests = {\n",
        "    \"A1\": \"Help me learn basic Italian greetings\",\n",
        "    \"A2\": \"Explain how to talk about daily routines in Italian\",\n",
        "    \"B1\": \"What's the difference between passato prossimo and imperfetto?\",\n",
        "    \"B2\": \"Explain the use of the conditional mood in Italian\"\n",
        "}\n",
        "\n",
        "for level, question in cefr_tests.items():\n",
        "    print(f\"\\n📚 {level} Level Test:\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        response = marco.chat(f\"At {level} level: {question}\")\n",
        "        print(f\"Marco: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(\"✅ CEFR level testing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH1GZCjqozQt"
      },
      "source": [
        "## 8. Final Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a9Cs1lpGozQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42db300d-7e7a-48ff-8f51-ca5bbb3b5acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 Marco LoRA Fine-Tuning Complete!\n",
            "==================================================\n",
            "📁 Model Location: ./marco_lora_checkpoints\n",
            "🤖 Base Model: Qwen/Qwen2.5-7B-Instruct\n",
            "⚙️  LoRA Configuration: r=16, alpha=32\n",
            "📊 Training Data: 8,104 total samples\n",
            "⏱️  Training Duration: 3 epochs\n",
            "💾 Model Size: 1124.5 MB\n",
            "\n",
            "🚀 Next Steps:\n",
            "1. ✅ Test the model with your own Italian questions\n",
            "2. 📊 Run more comprehensive evaluation if needed\n",
            "3. 🔄 Integrate with your Italian Teacher application\n",
            "4. 📈 Consider training for more epochs if performance needs improvement\n",
            "5. 🎯 Add specialized question generation training\n",
            "\n",
            "💡 To use this model in your app:\n",
            "marco = MarcoInference(lora_adapter_path=\"./marco_lora_checkpoints\")\n",
            "response = marco.chat(\"Your Italian question here\")\n",
            "\n",
            "🎊 Congratulations on completing Marco's fine-tuning!\n"
          ]
        }
      ],
      "source": [
        "# Training completion summary\n",
        "print(\"🎉 Marco LoRA Fine-Tuning Complete!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Model info\n",
        "print(f\"📁 Model Location: {config.training.output_dir}\")\n",
        "print(f\"🤖 Base Model: {config.training.model_name}\")\n",
        "print(f\"⚙️  LoRA Configuration: r={config.lora.r}, alpha={config.lora.lora_alpha}\")\n",
        "print(f\"📊 Training Data: {total_samples:,} total samples\")\n",
        "print(f\"⏱️  Training Duration: {config.training.num_train_epochs} epochs\")\n",
        "\n",
        "# File sizes\n",
        "checkpoint_dir = Path(config.training.output_dir)\n",
        "if checkpoint_dir.exists():\n",
        "    total_size = sum(f.stat().st_size for f in checkpoint_dir.glob('**/*') if f.is_file())\n",
        "    print(f\"💾 Model Size: {total_size / (1024**2):.1f} MB\")\n",
        "\n",
        "print(\"\\n🚀 Next Steps:\")\n",
        "print(\"1. ✅ Test the model with your own Italian questions\")\n",
        "print(\"2. 📊 Run more comprehensive evaluation if needed\")\n",
        "print(\"3. 🔄 Integrate with your Italian Teacher application\")\n",
        "print(\"4. 📈 Consider training for more epochs if performance needs improvement\")\n",
        "print(\"5. 🎯 Add specialized question generation training\")\n",
        "\n",
        "print(\"\\n💡 To use this model in your app:\")\n",
        "print(f'marco = MarcoInference(lora_adapter_path=\"{config.training.output_dir}\")')\n",
        "print('response = marco.chat(\"Your Italian question here\")')\n",
        "\n",
        "print(\"\\n🎊 Congratulations on completing Marco's fine-tuning!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dce87a0f8aca4a9ea0484b230090be06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50c6935df9f941948063dfd51fe2c835",
              "IPY_MODEL_59cdf7dce098429990e04dfa9f649e62",
              "IPY_MODEL_cfba2de461584b65aaf11513f78c7a09"
            ],
            "layout": "IPY_MODEL_f06518b21ed046a0b7a7e37292691c1a"
          }
        },
        "50c6935df9f941948063dfd51fe2c835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49754e059667485291195d226f6b45e9",
            "placeholder": "​",
            "style": "IPY_MODEL_93852ecd329a46cb9088c7ac4c5890de",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "59cdf7dce098429990e04dfa9f649e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aebb73f234ff44ddbad91c8ecd0b83d3",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_967f0462120744c294308bf5a989b44b",
            "value": 4
          }
        },
        "cfba2de461584b65aaf11513f78c7a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a460b935a9a49ec9ed1824fa3d1a99c",
            "placeholder": "​",
            "style": "IPY_MODEL_796e9fc4a32e45a8b86e942fc093d9db",
            "value": " 4/4 [00:16&lt;00:00,  4.09s/it]"
          }
        },
        "f06518b21ed046a0b7a7e37292691c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49754e059667485291195d226f6b45e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93852ecd329a46cb9088c7ac4c5890de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aebb73f234ff44ddbad91c8ecd0b83d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "967f0462120744c294308bf5a989b44b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a460b935a9a49ec9ed1824fa3d1a99c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "796e9fc4a32e45a8b86e942fc093d9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef78c826cb084607bbfe6a4970225d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f123c7f53a3421991433e3e35282721",
              "IPY_MODEL_412cd12a54794b7fb18570bb8d9a0e15",
              "IPY_MODEL_96249aeb437f42f589027ec219d4788b"
            ],
            "layout": "IPY_MODEL_2737964cc578420a945cf3312b7394e9"
          }
        },
        "9f123c7f53a3421991433e3e35282721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d19cee99b364c2a9974beed2c7a3192",
            "placeholder": "​",
            "style": "IPY_MODEL_5deec450d7d748f1ac472ea4de4da7b5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "412cd12a54794b7fb18570bb8d9a0e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_062cbc6a23e2441188de31ad4e1b4262",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46614f8808874dd5b42489ac3fa3b90c",
            "value": 4
          }
        },
        "96249aeb437f42f589027ec219d4788b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6880bbdd1b643da9d78b9b7ddb6689a",
            "placeholder": "​",
            "style": "IPY_MODEL_c3e569f60fbc48e2b37c4a697947fe58",
            "value": " 4/4 [00:16&lt;00:00,  3.94s/it]"
          }
        },
        "2737964cc578420a945cf3312b7394e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d19cee99b364c2a9974beed2c7a3192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5deec450d7d748f1ac472ea4de4da7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "062cbc6a23e2441188de31ad4e1b4262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46614f8808874dd5b42489ac3fa3b90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6880bbdd1b643da9d78b9b7ddb6689a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e569f60fbc48e2b37c4a697947fe58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daacf7dc4afe4ed98a70a6173b6a697f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75780c02748b4fc98ee24c491276202f",
              "IPY_MODEL_a4100e9d646f4c209e26c79339563f1a",
              "IPY_MODEL_f8c5e3db937b403cb5183f779c41185f"
            ],
            "layout": "IPY_MODEL_1a6521fe15034623b88beaa025a273d5"
          }
        },
        "75780c02748b4fc98ee24c491276202f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063e69493945473ca3f81b5e0911776e",
            "placeholder": "​",
            "style": "IPY_MODEL_32276e2d9ece452d8152560bd54e6ddb",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "a4100e9d646f4c209e26c79339563f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9c2ee44629d41ea8840fd43ffa1c5f2",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af42a22be407490586effb2caf439e18",
            "value": 2
          }
        },
        "f8c5e3db937b403cb5183f779c41185f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9615672a0fb45258d3c76ef10827f10",
            "placeholder": "​",
            "style": "IPY_MODEL_14e831a3057f4394a2874d3ee87c7254",
            "value": " 2/4 [00:12&lt;00:08,  4.37s/it]"
          }
        },
        "1a6521fe15034623b88beaa025a273d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063e69493945473ca3f81b5e0911776e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32276e2d9ece452d8152560bd54e6ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9c2ee44629d41ea8840fd43ffa1c5f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af42a22be407490586effb2caf439e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9615672a0fb45258d3c76ef10827f10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14e831a3057f4394a2874d3ee87c7254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abd56e86f06c49188256e5e377416a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de2c520fa93a487786067a9ddbdccfa2",
              "IPY_MODEL_8f2095778d84460eb151fc24ef77ecff",
              "IPY_MODEL_c99321a28a524aec83a5d43d07096cca"
            ],
            "layout": "IPY_MODEL_ca88aaaeb98247e5bdca810fed2b930f"
          }
        },
        "de2c520fa93a487786067a9ddbdccfa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6f7de3d2b834ed48cb4a291a1b70a20",
            "placeholder": "​",
            "style": "IPY_MODEL_824903a33ad04815aa904b644e81079b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8f2095778d84460eb151fc24ef77ecff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e6a7783030943fe816221436b2f6432",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05bcf9b7214841b184f26562f971f033",
            "value": 4
          }
        },
        "c99321a28a524aec83a5d43d07096cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f1513dcfae041fd9bccd2a1c8145a9a",
            "placeholder": "​",
            "style": "IPY_MODEL_e48398d39ec747a6af0aaef0d81250cd",
            "value": " 4/4 [00:04&lt;00:00,  1.12s/it]"
          }
        },
        "ca88aaaeb98247e5bdca810fed2b930f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6f7de3d2b834ed48cb4a291a1b70a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "824903a33ad04815aa904b644e81079b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e6a7783030943fe816221436b2f6432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05bcf9b7214841b184f26562f971f033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f1513dcfae041fd9bccd2a1c8145a9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48398d39ec747a6af0aaef0d81250cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0bf33b0435c429e958cefef7fe9071e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e42d63d0f7af432f94b50bed1a5f2b62",
              "IPY_MODEL_148e682d31144a0db13b17c99130e49e",
              "IPY_MODEL_5a86317710524108b14521ee943fbea9"
            ],
            "layout": "IPY_MODEL_3bef6b85d164472fac845afb6d575fce"
          }
        },
        "e42d63d0f7af432f94b50bed1a5f2b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d97fb02dd89f414a80e5f730dde9ce78",
            "placeholder": "​",
            "style": "IPY_MODEL_5acce7c6f6e4420f848692f19cd09f3f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "148e682d31144a0db13b17c99130e49e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f68506135b14ea79fc236fdb2186c84",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcec9858c1b145bf82c7fba5a3ff5e37",
            "value": 4
          }
        },
        "5a86317710524108b14521ee943fbea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34731bb48bc47b8a0863102ca86dee6",
            "placeholder": "​",
            "style": "IPY_MODEL_b0b2d7b95d4247bdb388407c04379520",
            "value": " 4/4 [00:05&lt;00:00,  1.26s/it]"
          }
        },
        "3bef6b85d164472fac845afb6d575fce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d97fb02dd89f414a80e5f730dde9ce78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5acce7c6f6e4420f848692f19cd09f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f68506135b14ea79fc236fdb2186c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcec9858c1b145bf82c7fba5a3ff5e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a34731bb48bc47b8a0863102ca86dee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b2d7b95d4247bdb388407c04379520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}