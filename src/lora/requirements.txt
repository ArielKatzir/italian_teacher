# LoRA Fine-tuning Requirements for Marco Italian Teacher
# For use with Colab Pro T4/A100 GPUs

# Core libraries
torch>=2.0.0
transformers>=4.36.0
datasets>=2.14.0
tokenizers>=0.15.0

# LoRA and PEFT
peft>=0.7.0
accelerate>=0.24.0
bitsandbytes>=0.41.0

# Experiment tracking
wandb>=0.16.0

# Utilities
numpy>=1.24.0
tqdm>=4.65.0
scipy>=1.11.0

# Optional but recommended for faster training
flash-attn>=2.0.0  # For supported GPUs (A100, not T4)

# Development dependencies (optional)
jupyter>=1.0.0
ipywidgets>=8.0.0