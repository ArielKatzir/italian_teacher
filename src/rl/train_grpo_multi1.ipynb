{"cells":[{"cell_type":"markdown","metadata":{"id":"mFalqG5F_A71"},"source":["# GRPO Training for Italian Exercise Generator"]},{"cell_type":"markdown","metadata":{"id":"J4B78ZgA_A73"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCJu42Bk_A74","executionInfo":{"status":"ok","timestamp":1762215839494,"user_tz":0,"elapsed":33894,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"3b3efed4-0324-43b5-c68e-dbefa680a828"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Changed directory to: /content/drive/MyDrive/Colab Notebooks/italian_teacher\n","Collecting it-core-news-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m135.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('it_core_news_sm')\n","\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Requirement already satisfied: anthropic in /usr/local/lib/python3.12/dist-packages (0.72.0)\n","Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.33.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n","Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n","Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.11.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n","\n","--- Environment Setup ---\n","PyTorch version: 2.8.0+cu126\n","CUDA available: True\n","GPU: NVIDIA A100-SXM4-80GB\n"]}],"source":["# --- Cell 1: Setup and Imports ---\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to your project directory\n","# Make sure this path is correct for your Google Drive setup\n","import os\n","project_path = '/content/drive/MyDrive/Colab Notebooks/italian_teacher'\n","os.chdir(project_path)\n","print(f\"Changed directory to: {os.getcwd()}\")\n","\n","# trl imported from local fork\n","# !pip install -e /content/drive/MyDrive/Colab\\ Notebooks/trl\n","\n","# Install dependencies (now includes google-generativeai for Gemini API)\n","!pip install -q trl transformers accelerate peft datasets spacy sentence-transformers bitsandbytes json5 openai google-generativeai tqdm nest_asyncio\n","!python -m spacy download it_core_news_sm\n","!pip install anthropic groq\n","\n","# Standard library imports\n","import json\n","import random\n","from getpass import getpass\n","\n","# Third-party imports\n","import torch\n","from datasets import Dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from trl import GRPOConfig, GRPOTrainer\n","\n","# Local module imports\n","from src.rl.multi_reward_async import create_async_multi_reward\n","from src.rl.prompt_formatter import format_prompt_with_chat_template\n","from src.rl.reward_function import ExerciseRewardFunction\n","\n","# Environment setup\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","print(\"\\n--- Environment Setup ---\")\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"t_HTIkZ0_A75","executionInfo":{"status":"ok","timestamp":1762215839582,"user_tz":0,"elapsed":111,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}}},"outputs":[],"source":["# --- Cell 2: Configuration ---\n","# All training parameters are here for easy modification.\n","\n","BASE_MODEL_PATH = \"./models/TeacherPet_italian_grpo\"  # Input model for this training run\n","OUTPUT_DIR = \"./models/TeacherPet_italian_grpo_round2\"      # Where the new model will be saved\n","NUM_SAMPLES = 1000                                    # Number of training requests to use\n","RANDOM_SEED = 42                                     # Seed for reproducibility\n","\n","# Scorer settings\n","DISABLED_SCORERS = []          # No scorers disabled\n","FLUENCY_USE_LLM = False        # Use rule-based checks only (fast, free)\n","\n","# --- GRPO Configuration ---\n","grpo_config = GRPOConfig(\n","    output_dir=OUTPUT_DIR,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=3,\n","    gradient_accumulation_steps=36,\n","    learning_rate=5e-6,\n","    warmup_steps=50,\n","    logging_steps=10,\n","    save_steps=25,\n","    save_total_limit=3,\n","    bf16=True,\n","    remove_unused_columns=False,\n","    report_to=\"none\",\n","\n","    # ‚Üê ADD THIS\n","    optim=\"paged_adamw_8bit\",  # Uses 8-bit optimizer (saves ~16GB)\n","\n","    num_generations=3,\n","    max_prompt_length=1024,\n","    max_completion_length=350,\n","    temperature=0.9,\n","    generation_batch_size=3,\n","    beta=0.05,\n","\n","    generation_kwargs={\n","        \"bos_token_id\": 128000,\n","        \"do_sample\": True,\n","        \"eos_token_id\": [128009, 128001],\n","        \"temperature\": 0.9,\n","        \"top_p\": 0.9,\n","        \"top_k\": 50,\n","        \"padding_side\": \"left\",\n","    }\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":795,"referenced_widgets":["64fc434ba5ba4b19ba16316250927734","54f7bc6c10264203957fd6ad3f55bd0f","64ba7d82a73f4d37849db328e01e3ac2","5225fba0f14b41128a7b683f4886ec37","87bc3a14d4254303adb76c7f3980bebb","2334120ace22428bba6a682576b4b564","5136314f923f423e99b93d5a4cbbd0ae","fcea2bf1ef2c4d73a4fe5799470bf65f","d89bf3b179bd495a944cdeec0ab30119","3ffa1462302643f893d592d1634344a6","1a0c58bd78ff4001afe2bfdb2c2a25f6"]},"id":"EyD8vpaC_A75","executionInfo":{"status":"ok","timestamp":1762215856765,"user_tz":0,"elapsed":17179,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"e0aee9cc-1ee6-4b7e-b86f-a63fda6994b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","üöÄ STARTING GRPO TRAINING\n","================================================================================\n","\n","--- Loading Secrets ---\n","‚úÖ Loading API keys from /content/drive/My Drive/.secrets.json\n","   Loaded 9 API key(s)\n","\n","--- Loading Model ---\n","Base model: ./models/TeacherPet_italian_grpo\n"]},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64fc434ba5ba4b19ba16316250927734"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Model and tokenizer loaded.\n","\n","--- Preparing Training Data ---\n","Loading existing training requests from src/rl/training_requests.json...\n","‚úÖ Loaded 2000 training requests.\n","Sampling 1000 requests (seed=42)...\n","\n","--- Initializing Reward Function ---\n","Loading spaCy model: it_core_news_sm...\n","‚úÖ spaCy model loaded\n","Reward function will use device: cuda\n","     ‚úÖ Gemini: 4 API key(s)\n","     ‚úÖ OpenAI: configured\n","     ‚úÖ Anthropic: configured\n","     ‚úÖ Groq: configured\n","     ‚úÖ DeepSeek: configured\n","     ‚úÖ Cerebras: configured\n","  ‚úÖ LLM API Handler initialized\n","     Providers: gemini, openai, anthropic, groq, deepseek, cerebras\n","     Total models: 12\n","Initializing scorers...\n","  ‚úÖ LLM scoring enabled for cefr_alignment (batch size: 10)\n","  ‚úÖ LLM scoring enabled for fluency (batch size: 10)\n","  ‚úÖ LLM scoring enabled for grammar_correctness (batch size: 10)\n","  ‚úÖ LLM scoring enabled for coherence (batch size: 10)\n","Loading sentence transformer for topic similarity...\n","‚úÖ Sentence transformer loaded in cuda\n","  ‚úÖ LLM topic checking enabled (OpenAI API)\n","‚úÖ Reward function initialized. Active scorers: ['json', 'quality', 'linguistic', 'cefr', 'fluency', 'grammar', 'coherence', 'topic']\n","‚úÖ Reward function ready.\n","Tokenizer type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n","Padding side: left\n"]}],"source":["# --- Cell 3: Helper Functions & Main Execution ---\n","\n","def load_secrets_from_file():\n","    \"\"\"\n","    Load API keys from .secrets.json file if it exists.\n","    Checks multiple locations: Google Drive root, then current directory.\n","    \"\"\"\n","    from pathlib import Path\n","\n","    secrets_paths = [\n","        Path.home() / \"Google Drive\" / \"My Drive\" / \".secrets.json\",  # Local path\n","        Path(\"/content/drive/My Drive/.secrets.json\"),                # Colab path\n","        Path('.secrets.json')                                         # Current directory\n","    ]\n","\n","    for path in secrets_paths:\n","        if path.exists():\n","            print(f\"‚úÖ Loading API keys from {path}\")\n","            with open(path, 'r') as f:\n","                secrets = json.load(f)\n","\n","            loaded_keys = []\n","            for key, value in secrets.items():\n","                if value and value not in [\"your-openai-key-here\", \"your-google-key-here\", \"\"]:\n","                    os.environ[key] = value\n","                    loaded_keys.append(key)\n","\n","            if loaded_keys:\n","                print(f\"   Loaded {len(loaded_keys)} API key(s)\")\n","                return True\n","\n","    print(\"‚ö†Ô∏è  No .secrets.json found. Make sure API keys are in Colab secrets or environment.\")\n","    return False\n","\n","\n","def load_training_data(tokenizer, num_samples: int, seed: int):\n","    \"\"\"Load or generate training requests and prepare dataset.\"\"\"\n","    requests_path = \"src/rl/training_requests.json\"\n","\n","    if os.path.exists(requests_path):\n","        print(f\"Loading existing training requests from {requests_path}...\")\n","        with open(requests_path, \"r\") as f:\n","            training_requests = json.load(f)\n","    else:\n","        from src.rl.generate_training_requests import generate_training_requests\n","        print(f\"Generating {num_samples} new training requests...\")\n","        training_requests = generate_training_requests(\n","            num_requests=num_samples,\n","            output_path=requests_path\n","        )\n","\n","    print(f\"‚úÖ Loaded {len(training_requests)} training requests.\")\n","\n","    # Format prompts\n","    prompts = [\n","        format_prompt_with_chat_template(req, tokenizer, add_examples=True)\n","        for req in training_requests\n","    ]\n","\n","    # Sample if needed\n","    if len(prompts) > num_samples:\n","        print(f\"Sampling {num_samples} requests (seed={seed})...\")\n","        random.seed(seed)\n","        random_indices = random.sample(range(len(prompts)), num_samples)\n","        prompts = [prompts[i] for i in random_indices]\n","        training_requests = [training_requests[i] for i in random_indices]\n","\n","    return Dataset.from_dict({\n","        \"prompt\": prompts,\n","        \"request\": training_requests,\n","    })\n","\n","\n","print(\"=\" * 80)\n","print(\"üöÄ STARTING GRPO TRAINING\")\n","print(\"=\" * 80)\n","\n","# Load API keys\n","print(\"\\n--- Loading Secrets ---\")\n","load_secrets_from_file()\n","\n","# Load Model and Tokenizer\n","print(f\"\\n--- Loading Model ---\")\n","print(f\"Base model: {BASE_MODEL_PATH}\")\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, padding_side='left')\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"left\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL_PATH,\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    use_cache=False,\n",")\n","model.gradient_checkpointing_enable()\n","model.config.pad_token_id = tokenizer.pad_token_id\n","model.config.padding_side = tokenizer.padding_side\n","print(\"‚úÖ Model and tokenizer loaded.\")\n","\n","# Prepare Training Data\n","print(\"\\n--- Preparing Training Data ---\")\n","train_dataset = load_training_data(tokenizer, num_samples=NUM_SAMPLES, seed=RANDOM_SEED)\n","\n","# Initialize Reward Function\n","print(\"\\n--- Initializing Reward Function ---\")\n","reward_fn_instance = ExerciseRewardFunction(\n","    device=\"cuda\",\n","    disabled_scorers=DISABLED_SCORERS,\n","    fluency_use_llm=FLUENCY_USE_LLM,\n","    concurrency_limit=3  # High concurrency for speed\n",")\n","reward_func = create_async_multi_reward(reward_fn_instance, use_openai=True)\n","print(\"‚úÖ Reward function ready.\")\n","\n","print(f'Tokenizer type: {type(tokenizer)}')\n","print(f'Padding side: {tokenizer.padding_side}')\n","if hasattr(tokenizer, 'tokenizer'):\n","    print(f'Has sub-tokenizer: {tokenizer.tokenizer.padding_side}')\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nl_z1JXZhcrC","executionInfo":{"status":"ok","timestamp":1762215857181,"user_tz":0,"elapsed":411,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"1b8d8859-57a5-4e5e-8d71-bca4991b20f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìä Selecting 10 validation samples from src/rl/training_requests.json...\n","   Total requests available: 2000\n","   Exercise types: unknown\n","   ‚úÖ Selected 10 diverse samples\n","   Distribution: {'unknown': 10}\n","\n","üìä Validation Callback initialized:\n","   10 validation samples\n","   3 generations per sample\n","   Results will be saved to: models/TeacherPet_italian_grpo_round2/validation_results\n","‚úÖ Validation callback ready.\n"]}],"source":["# --- Add Validation Tracking ---\n","from src.rl.validation_callback import ValidationCallback, select_validation_samples\n","from src.rl.prompt_formatter import format_prompt_with_chat_template\n","\n","# Select 10 diverse validation samples\n","validation_samples = select_validation_samples(\n","    training_requests_path=\"src/rl/training_requests.json\",\n","    num_samples=10,  # Adjust this number (5-15 recommended)\n","    seed=RANDOM_SEED\n",")\n","\n","# Format prompts for validation samples\n","validation_prompts = [\n","    format_prompt_with_chat_template(req, tokenizer, add_examples=True)\n","    for req in validation_samples\n","]\n","\n","# Create validation callback\n","validation_callback = ValidationCallback(\n","    validation_samples=validation_samples,\n","    validation_prompts=validation_prompts,\n","    reward_function=reward_func,\n","    tokenizer=tokenizer,\n","    output_dir=OUTPUT_DIR,\n","    num_generations=3  # Generate 3 completions per sample for comparison\n",")\n","\n","print(\"‚úÖ Validation callback ready.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"OgdhaHOWO_PS","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["48979b392b7b4825a742a65e7883d65a","f8489ae33b3a48f08ff2d769c4d8679d","d59822d4e12447648645daeca63f5012","40d76d52a4214f9d933778c269eed570","d8f665064ccf416eb1580b3ba7797a86","02f9d6eb1ac344dc95abdbc58b4d2728","1b973f68ecbe4b589ee6b13921042cc7","9db5a745cc514770a78eb8c67e45f73f","8e2f7403a9aa4196bb03d2e3318c77d7","084f751fa5394d73ad47e876efb4130c","cf8928e4ef7f43cf80803556e632f72b"]},"executionInfo":{"status":"error","timestamp":1762217077735,"user_tz":0,"elapsed":1220550,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"8020c7f0-98cc-477c-d842-62e2dc807ab6"},"outputs":[{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Initializing GRPO Trainer ---\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48979b392b7b4825a742a65e7883d65a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","üîç GRPO Generation Settings:\n","   num_generations (config): 3\n","   generation_batch_size: 3\n","   trainer.generation_config: GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128009,\n","    128001\n","  ],\n","  \"max_new_tokens\": 350,\n","  \"pad_token_id\": 128009,\n","  \"padding_side\": \"left\",\n","  \"temperature\": 0.9,\n","  \"top_p\": 0.9\n","}\n","\n","‚úÖ GRPO Trainer initialized.\n","\n","================================================================================\n","üî• TRAINING BEGINS\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["`generation_config` default values have been modified to match model-specific defaults: {'max_length': 8192}. If this is not desired, please set these values explicitly.\n"]},{"output_type":"stream","name":"stdout","text":["   [Call #1 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 2/3 valid JSON (66.7%), 0 empty, 1 failed ‚Üí 2 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (6 total requests):\n","      üîµ Gemini: 2/6 (33.3%)\n","      üü¢ Openai: 2/6 (33.3%)\n","      ‚ö° Groq: 1/6 (16.7%)\n","      ‚ùì Deepseek: 1/6 (16.7%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (24.0s):\n","   Grammar   : min=40.0, max=100.0, avg=70.0\n","   Coherence : min=76.7, max=88.0, avg=82.3\n","   Topic     : min=93.3, max=94.0, avg=93.7\n","   Quality   : min=83.3, max=90.0, avg=86.7\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=0.000, max=72.222, avg=47.213\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #2 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (21.7s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=55.0, max=65.0, avg=61.7\n","   Topic     : min=0.0, max=10.0, avg=6.7\n","   Quality   : min=75.0, max=100.0, avg=91.7\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=49.167, max=54.583, avg=52.569\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #3 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 2/9 (22.2%)\n","      ‚ö° Groq: 1/9 (11.1%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (28.7s):\n","   Grammar   : min=87.5, max=100.0, avg=95.8\n","   Coherence : min=77.5, max=85.0, avg=80.8\n","   Topic     : min=97.5, max=100.0, avg=99.2\n","   Quality   : min=87.5, max=100.0, avg=95.8\n","   Diversity : min=86.7, max=100.0, avg=91.1\n","   TOTAL     : min=73.229, max=76.597, avg=74.468\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #4 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (31.3s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=35.0, max=55.0, avg=44.2\n","   Topic     : min=20.0, max=55.0, avg=31.7\n","   Quality   : min=50.0, max=50.0, avg=50.0\n","   Diversity : min=85.0, max=100.0, avg=95.0\n","   TOTAL     : min=45.938, max=53.021, avg=50.139\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #5 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (26.2s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=43.3, max=53.3, avg=48.9\n","   Topic     : min=30.0, max=40.0, avg=36.7\n","   Quality   : min=50.0, max=50.0, avg=50.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=51.806, max=52.361, avg=52.176\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #6 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      ‚ö° Groq: 4/9 (44.4%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      üîµ Gemini: 1/9 (11.1%)\n","      ‚ùì Deepseek: 1/9 (11.1%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (38.1s):\n","   Grammar   : min=0.0, max=60.0, avg=31.3\n","   Coherence : min=62.0, max=82.0, avg=72.0\n","   Topic     : min=28.0, max=82.0, avg=56.0\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=61.583, max=70.333, avg=66.778\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #7 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      ‚ö° Groq: 4/9 (44.4%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      üîµ Gemini: 1/9 (11.1%)\n","      ‚ùì Deepseek: 1/9 (11.1%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (37.3s):\n","   Grammar   : min=0.0, max=100.0, avg=58.3\n","   Coherence : min=50.0, max=95.0, avg=76.7\n","   Topic     : min=97.5, max=100.0, avg=98.3\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=69.271, max=77.708, avg=73.299\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #8 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      ‚ö° Groq: 6/9 (66.7%)\n","      üü¢ Openai: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (39.0s):\n","   Grammar   : min=20.0, max=65.0, avg=45.0\n","   Coherence : min=75.0, max=85.0, avg=80.0\n","   Topic     : min=80.0, max=100.0, avg=92.5\n","   Quality   : min=87.5, max=100.0, avg=91.7\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=67.708, max=78.438, avg=73.056\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #9 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (28.4s):\n","   Grammar   : min=16.7, max=100.0, avg=57.8\n","   Coherence : min=73.3, max=80.0, avg=77.8\n","   Topic     : min=0.0, max=6.7, avg=4.4\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=86.7, max=86.7, avg=86.7\n","   TOTAL     : min=59.444, max=66.389, avg=62.685\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #10 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (28.0s):\n","   Grammar   : min=66.7, max=80.0, avg=71.1\n","   Coherence : min=60.0, max=80.0, avg=71.1\n","   Topic     : min=70.0, max=100.0, avg=90.0\n","   Quality   : min=83.3, max=100.0, avg=94.4\n","   Diversity : min=86.7, max=100.0, avg=91.1\n","   TOTAL     : min=68.333, max=71.389, avg=70.000\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #11 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ö° Groq: 2/9 (22.2%)\n","      ‚ùì Deepseek: 1/9 (11.1%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (31.7s):\n","   Grammar   : min=0.0, max=28.0, avg=9.3\n","   Coherence : min=34.0, max=44.0, avg=39.3\n","   Topic     : min=84.0, max=92.0, avg=86.7\n","   Quality   : min=50.0, max=50.0, avg=50.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=51.833, max=53.083, avg=52.528\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #12 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (26.7s):\n","   Grammar   : min=35.0, max=50.0, avg=41.7\n","   Coherence : min=50.0, max=60.0, avg=56.7\n","   Topic     : min=0.0, max=50.0, avg=20.0\n","   Quality   : min=75.0, max=75.0, avg=75.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=55.625, max=58.542, avg=57.292\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #13 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (22.3s):\n","   Grammar   : min=50.0, max=100.0, avg=83.3\n","   Coherence : min=50.0, max=65.0, avg=55.0\n","   Topic     : min=20.0, max=20.0, avg=20.0\n","   Quality   : min=75.0, max=75.0, avg=75.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=56.042, max=62.917, avg=60.625\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #14 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (26.9s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=60.0, max=65.0, avg=61.7\n","   Topic     : min=70.0, max=100.0, avg=86.7\n","   Quality   : min=75.0, max=75.0, avg=75.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=56.042, max=62.500, avg=59.792\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #15 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (22.8s):\n","   Grammar   : min=65.0, max=100.0, avg=88.3\n","   Coherence : min=30.0, max=75.0, avg=46.7\n","   Topic     : min=35.0, max=100.0, avg=78.3\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=80.0, max=100.0, avg=93.3\n","   TOTAL     : min=58.542, max=72.083, avg=67.569\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #16 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (25.1s):\n","   Grammar   : min=0.0, max=50.0, avg=31.7\n","   Coherence : min=35.0, max=65.0, avg=50.0\n","   Topic     : min=35.0, max=80.0, avg=50.0\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=52.917, max=65.000, avg=59.722\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #17 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (17.6s):\n","   Grammar   : min=100.0, max=100.0, avg=100.0\n","   Coherence : min=40.0, max=90.0, avg=70.0\n","   Topic     : min=20.0, max=100.0, avg=70.0\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=70.0, max=70.0, avg=70.0\n","   TOTAL     : min=75.833, max=82.500, avg=78.889\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #18 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 2/3 valid JSON (66.7%), 0 empty, 1 failed ‚Üí 2 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (6 total requests):\n","      üîµ Gemini: 2/6 (33.3%)\n","      üü¢ Openai: 2/6 (33.3%)\n","      ‚ùì Deepseek: 2/6 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (22.0s):\n","   Grammar   : min=0.0, max=25.0, avg=12.5\n","   Coherence : min=62.5, max=77.5, avg=70.0\n","   Topic     : min=37.5, max=55.0, avg=46.2\n","   Quality   : min=75.0, max=77.5, avg=76.2\n","   Diversity : min=86.7, max=100.0, avg=93.3\n","   TOTAL     : min=0.000, max=58.438, avg=38.900\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #19 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (6 total requests):\n","      üü¢ Openai: 3/6 (50.0%)\n","      ‚ùì Deepseek: 3/6 (50.0%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (16.0s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=35.0, max=65.0, avg=51.7\n","   Topic     : min=85.0, max=95.0, avg=91.7\n","   Quality   : min=50.0, max=50.0, avg=50.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=51.875, max=58.333, avg=55.417\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #20 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (6 total requests):\n","      üü¢ Openai: 3/6 (50.0%)\n","      ‚ùì Deepseek: 3/6 (50.0%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (20.4s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=36.7, max=43.3, avg=40.0\n","   Topic     : min=30.0, max=93.3, avg=65.6\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=59.167, max=64.861, avg=61.250\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #21 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (20.0s):\n","   Grammar   : min=5.0, max=100.0, avg=56.7\n","   Coherence : min=60.0, max=65.0, avg=61.7\n","   Topic     : min=85.0, max=90.0, avg=88.3\n","   Quality   : min=75.0, max=75.0, avg=75.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=62.292, max=66.250, avg=64.583\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #22 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (6 total requests):\n","      üü¢ Openai: 3/6 (50.0%)\n","      ‚ùì Deepseek: 3/6 (50.0%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (21.9s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=70.0, max=86.7, avg=80.0\n","   Topic     : min=56.7, max=90.0, avg=71.1\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=62.778, max=67.500, avg=65.370\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #23 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (21.6s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=35.0, max=100.0, avg=63.3\n","   Topic     : min=70.0, max=100.0, avg=85.0\n","   Quality   : min=50.0, max=83.3, avg=69.4\n","   Diversity : min=80.0, max=100.0, avg=93.3\n","   TOTAL     : min=56.875, max=62.639, avg=59.005\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #24 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (26.4s):\n","   Grammar   : min=50.0, max=100.0, avg=70.0\n","   Coherence : min=60.0, max=85.0, avg=68.3\n","   Topic     : min=95.0, max=100.0, avg=98.3\n","   Quality   : min=50.0, max=75.0, avg=66.7\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=62.500, max=71.250, avg=65.486\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #25 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (23.2s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=35.0, max=70.0, avg=53.3\n","   Topic     : min=50.0, max=100.0, avg=78.3\n","   Quality   : min=75.0, max=75.0, avg=75.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=53.750, max=60.208, avg=57.569\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #26 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (26.1s):\n","   Grammar   : min=0.0, max=0.0, avg=0.0\n","   Coherence : min=53.3, max=70.0, avg=60.0\n","   Topic     : min=46.7, max=70.0, avg=62.2\n","   Quality   : min=83.3, max=83.3, avg=83.3\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=58.333, max=60.556, avg=59.722\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #27 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ö° Groq: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (34.4s):\n","   Grammar   : min=62.5, max=75.0, avg=66.7\n","   Coherence : min=60.0, max=85.0, avg=70.0\n","   Topic     : min=52.5, max=82.5, avg=68.3\n","   Quality   : min=75.0, max=100.0, avg=83.3\n","   Diversity : min=86.7, max=100.0, avg=95.6\n","   TOTAL     : min=67.812, max=78.472, avg=72.025\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #28 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (28.9s):\n","   Grammar   : min=0.0, max=32.5, avg=10.8\n","   Coherence : min=70.0, max=85.0, avg=76.1\n","   Topic     : min=92.5, max=97.5, avg=94.4\n","   Quality   : min=77.5, max=83.3, avg=79.4\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=63.958, max=68.333, avg=65.856\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #29 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (24.8s):\n","   Grammar   : min=50.0, max=100.0, avg=71.7\n","   Coherence : min=60.0, max=70.0, avg=65.0\n","   Topic     : min=60.0, max=100.0, avg=80.0\n","   Quality   : min=55.0, max=75.0, avg=68.3\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=63.750, max=68.333, avg=66.111\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #30 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (21.3s):\n","   Grammar   : min=0.0, max=50.0, avg=16.7\n","   Coherence : min=65.0, max=70.0, avg=66.7\n","   Topic     : min=10.0, max=95.0, avg=55.0\n","   Quality   : min=80.0, max=100.0, avg=93.3\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=59.792, max=67.083, avg=62.986\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #31 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 2/3 valid JSON (66.7%), 0 empty, 1 failed ‚Üí 2 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (6 total requests):\n","      üîµ Gemini: 2/6 (33.3%)\n","      üü¢ Openai: 2/6 (33.3%)\n","      ‚ö° Groq: 1/6 (16.7%)\n","      ‚ùì Deepseek: 1/6 (16.7%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (26.3s):\n","   Grammar   : min=60.0, max=100.0, avg=80.0\n","   Coherence : min=53.3, max=80.0, avg=66.7\n","   Topic     : min=63.3, max=75.0, avg=69.2\n","   Quality   : min=83.3, max=87.5, avg=85.4\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=0.000, max=78.646, avg=46.863\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #32 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ö° Groq: 2/9 (22.2%)\n","      ‚ùì Deepseek: 1/9 (11.1%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (29.8s):\n","   Grammar   : min=0.0, max=100.0, avg=33.3\n","   Coherence : min=47.5, max=75.0, avg=62.5\n","   Topic     : min=5.0, max=30.0, avg=15.0\n","   Quality   : min=10.0, max=100.0, avg=65.8\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=40.208, max=66.354, avg=55.312\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #33 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["\n","   üìä Model Usage Distribution (9 total requests):\n","      üîµ Gemini: 3/9 (33.3%)\n","      üü¢ Openai: 3/9 (33.3%)\n","      ‚ùì Deepseek: 3/9 (33.3%)\n","‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"]},{"output_type":"stream","name":"stderr","text":["                                                          "]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Reward calculation complete (23.6s):\n","   Grammar   : min=0.0, max=66.7, avg=22.2\n","   Coherence : min=36.7, max=50.0, avg=43.3\n","   Topic     : min=70.0, max=90.0, avg=77.8\n","   Quality   : min=100.0, max=100.0, avg=100.0\n","   Diversity : min=100.0, max=100.0, avg=100.0\n","   TOTAL     : min=58.889, max=68.889, avg=64.120\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["   [Call #34 for step 0: scoring 3 completions]\n","\n","‚è≥ Step 1/3: Parsing 3 JSON completions...\n"]},{"output_type":"stream","name":"stderr","text":["                                                   "]},{"output_type":"stream","name":"stdout","text":["   Parse stats: 3/3 valid JSON (100.0%), 0 empty, 0 failed ‚Üí 3 scorable\n","‚è≥ Step 2/3: Scoring 3 completions with batched reward function...\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2208776593.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üî• TRAINING BEGINS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üéâ TRAINING COMPLETE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4012\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4014\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4015\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4016\u001b[0m                 \u001b[0mloss_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmp_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/extras/profiling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mprofiling_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, generation_batch)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgenerate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;31m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                 \u001b[0mgeneration_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_and_score_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mgeneration_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_pixel_values_by_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                 \u001b[0mgeneration_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_sequence_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# important because rewards will be normalized per group, and completions are distributed. We will later slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# rewards_per_func to extract each process's subset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m         \u001b[0mrewards_per_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletion_ids_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;31m# Apply weights to each reward function's output and sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/extras/profiling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mprofiling_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_calculate_rewards\u001b[0;34m(self, inputs, prompts, completions, completion_ids_list)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                         \u001b[0mrewards_per_func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mreward_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape (B*G,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     output_reward_func = reward_func(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                         \u001b[0mprompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletion_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompletion_ids_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mreward_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     )\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/italian_teacher/src/rl/multi_reward_async.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompts, completions, completion_ids, trainer_state, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Initialize Trainer\n","print(\"\\n--- Initializing GRPO Trainer ---\")\n","trainer = GRPOTrainer(\n","    model=model,\n","    args=grpo_config,\n","    reward_funcs=reward_func,\n","    train_dataset=train_dataset,\n","    processing_class=tokenizer,\n",")\n","\n","\n","# After: trainer = GRPOTrainer(...)\n","print(f\"\\nüîç GRPO Generation Settings:\")\n","print(f\"   num_generations (config): {grpo_config.num_generations}\")\n","print(f\"   generation_batch_size: {grpo_config.generation_batch_size}\")\n","if hasattr(trainer, 'generation_config'):\n","    print(f\"   trainer.generation_config: {trainer.generation_config}\")\n","print(\"‚úÖ GRPO Trainer initialized.\")\n","\n","\n","# Start Training\n","print(\"\\n\" + \"=\" * 80)\n","print(\"üî• TRAINING BEGINS\")\n","print(\"=\" * 80)\n","trainer.train()\n","print(\"\\n\" + \"=\" * 80)\n","print(\"üéâ TRAINING COMPLETE\")\n","print(\"=\" * 80)\n","\n","# Save Final Model\n","print(f\"\\n--- Saving Model ---\")\n","print(f\"Output directory: {OUTPUT_DIR}\")\n","trainer.save_model(OUTPUT_DIR)\n","tokenizer.save_pretrained(OUTPUT_DIR)\n","print(\"‚úÖ Model saved successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pNEK24v4JNR","executionInfo":{"status":"aborted","timestamp":1762217078002,"user_tz":0,"elapsed":1272707,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}}},"outputs":[],"source":["import time, os, signal\n","from google.colab import runtime\n","\n","print(\"‚è≥ Waiting 3 minutes (180 seconds) before disconnecting...\")\n","time.sleep(180)\n","\n","print(\"üîå Attempting clean disconnect...\")\n","try:\n","    runtime.disconnect()\n","    print(\"‚úÖ Clean disconnect attempted. Waiting 5 seconds to verify...\")\n","    time.sleep(5)\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è Clean disconnect failed: {e}\")\n","\n","# Final guarantee: forcefully kill the process\n","print(\"üíÄ Forcing runtime shutdown now...\")\n","os.kill(os.getpid(), signal.SIGKILL)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"64fc434ba5ba4b19ba16316250927734":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54f7bc6c10264203957fd6ad3f55bd0f","IPY_MODEL_64ba7d82a73f4d37849db328e01e3ac2","IPY_MODEL_5225fba0f14b41128a7b683f4886ec37"],"layout":"IPY_MODEL_87bc3a14d4254303adb76c7f3980bebb"}},"54f7bc6c10264203957fd6ad3f55bd0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2334120ace22428bba6a682576b4b564","placeholder":"‚Äã","style":"IPY_MODEL_5136314f923f423e99b93d5a4cbbd0ae","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"64ba7d82a73f4d37849db328e01e3ac2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcea2bf1ef2c4d73a4fe5799470bf65f","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d89bf3b179bd495a944cdeec0ab30119","value":4}},"5225fba0f14b41128a7b683f4886ec37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ffa1462302643f893d592d1634344a6","placeholder":"‚Äã","style":"IPY_MODEL_1a0c58bd78ff4001afe2bfdb2c2a25f6","value":"‚Äá4/4‚Äá[00:12&lt;00:00,‚Äá‚Äá2.66s/it]"}},"87bc3a14d4254303adb76c7f3980bebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2334120ace22428bba6a682576b4b564":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5136314f923f423e99b93d5a4cbbd0ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcea2bf1ef2c4d73a4fe5799470bf65f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d89bf3b179bd495a944cdeec0ab30119":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ffa1462302643f893d592d1634344a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a0c58bd78ff4001afe2bfdb2c2a25f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48979b392b7b4825a742a65e7883d65a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8489ae33b3a48f08ff2d769c4d8679d","IPY_MODEL_d59822d4e12447648645daeca63f5012","IPY_MODEL_40d76d52a4214f9d933778c269eed570"],"layout":"IPY_MODEL_d8f665064ccf416eb1580b3ba7797a86"}},"f8489ae33b3a48f08ff2d769c4d8679d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02f9d6eb1ac344dc95abdbc58b4d2728","placeholder":"‚Äã","style":"IPY_MODEL_1b973f68ecbe4b589ee6b13921042cc7","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"d59822d4e12447648645daeca63f5012":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9db5a745cc514770a78eb8c67e45f73f","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e2f7403a9aa4196bb03d2e3318c77d7","value":4}},"40d76d52a4214f9d933778c269eed570":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_084f751fa5394d73ad47e876efb4130c","placeholder":"‚Äã","style":"IPY_MODEL_cf8928e4ef7f43cf80803556e632f72b","value":"‚Äá4/4‚Äá[00:08&lt;00:00,‚Äá‚Äá1.88s/it]"}},"d8f665064ccf416eb1580b3ba7797a86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02f9d6eb1ac344dc95abdbc58b4d2728":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b973f68ecbe4b589ee6b13921042cc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9db5a745cc514770a78eb8c67e45f73f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e2f7403a9aa4196bb03d2e3318c77d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"084f751fa5394d73ad47e876efb4130c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf8928e4ef7f43cf80803556e632f72b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}