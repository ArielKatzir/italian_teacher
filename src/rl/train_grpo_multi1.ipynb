{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFalqG5F_A71"
   },
   "source": [
    "# GRPO Training for Italian Exercise Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4B78ZgA_A73"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 3026,
     "status": "error",
     "timestamp": 1760974360509,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "cCJu42Bk_A74",
    "outputId": "108e79dc-7c36-47bd-f7c7-6a01e74f29c8"
   },
   "outputs": [
    {
     "ename": "MessageError",
     "evalue": "Error: credential propagation was unsuccessful",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-269423285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Navigate to project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to project\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/italian_teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "aborted",
     "timestamp": 1760974360571,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "t_HTIkZ0_A75"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers trl accelerate peft datasets spacy sentence-transformers bitsandbytes json5 openai tqdm nest_asyncio\n",
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 65,
     "status": "aborted",
     "timestamp": 1760974360573,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "EyD8vpaC_A75"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1760974360575,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "cCdP-H6OYqXI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# You can enable/disable OpenAI here:\n",
    "USE_OPENAI = True  # Set to False for faster training without OpenAI\n",
    "\n",
    "if USE_OPENAI:\n",
    "    if \"OPENAI_API_KEY\" not in os.environ:\n",
    "        OPENAI_API_KEY = \"\"\n",
    "        os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(\"‚úÖ OpenAI API enabled - Professional quality with async batching\")\n",
    "    print(\"   OPTIMIZED: Samples 1 exercise/completion (70% reduction in API calls)\")\n",
    "    print(\"   Expected training time: ~2-3 hours\")\n",
    "else:\n",
    "    if \"OPENAI_API_KEY\" in os.environ:\n",
    "        del os.environ[\"OPENAI_API_KEY\"]\n",
    "    print(\"‚úÖ OpenAI API disabled - Fast rule-based rewards\")\n",
    "    print(\"   Expected training time: ~60-90 min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCZPxwiC_A75"
   },
   "source": [
    "## Load Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3445,
     "status": "aborted",
     "timestamp": 1760974360577,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "grleYTMt_A76"
   },
   "outputs": [],
   "source": [
    "from src.rl.reward_function import ExerciseRewardFunction\n",
    "from src.rl.prompt_formatter import format_prompt_with_chat_template  # ‚Üê ROUND 3: Enhanced V1 (not V3!)\n",
    "from src.rl.multi_reward_async import create_async_multi_reward\n",
    "import os\n",
    "\n",
    "reward_fn = ExerciseRewardFunction(device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01xZRVhP_A76"
   },
   "source": [
    "## Load Training Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3443,
     "status": "aborted",
     "timestamp": 1760974360578,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "MPsM6AE5_A77"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load pre-generated training requests\n",
    "if os.path.exists(\"src/rl/training_requests.json\"):\n",
    "    print(\"Loading existing training requests...\")\n",
    "    with open(\"src/rl/training_requests.json\", \"r\") as f:\n",
    "        training_requests = json.load(f)\n",
    "else:\n",
    "    # If not exists, generate them\n",
    "    from src.rl.generate_training_requests import generate_training_requests\n",
    "    print(\"Generating new training requests...\")\n",
    "    training_requests = generate_training_requests(\n",
    "        num_requests=2000,\n",
    "        output_path=\"src/rl/training_requests.json\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(training_requests)} training requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3442,
     "status": "aborted",
     "timestamp": 1760974360580,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "C3BH36ql_A78"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ROUND 3: Start from Round 2 model (86.5/100 baseline)\n",
    "# models/italian_v8_grpo_round2\n",
    "MODEL_PATH = \"./models/italian_v8_grpo_round2\"  # ‚Üê Round 2 GRPO model (best so far)\n",
    "temp_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã ROUND 3 DATASET PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use V3 prompt formatter with ENHANCED guidance!\n",
    "prompts = [\n",
    "    format_prompt_with_chat_template(req, temp_tokenizer, add_examples=True)\n",
    "    for req in training_requests\n",
    "]\n",
    "\n",
    "# Round 3: Use 1000 samples (balanced between quality and training time)\n",
    "ROUND3_SIZE = 2000\n",
    "if len(prompts) > ROUND3_SIZE:\n",
    "    random.seed(44)  # ‚Üê NEW seed for Round 3 (fresh samples)\n",
    "    random_indices = random.sample(range(len(prompts)), ROUND3_SIZE)\n",
    "    prompts = [prompts[i] for i in random_indices]\n",
    "    training_requests_subset = [training_requests[i] for i in random_indices]\n",
    "else:\n",
    "    training_requests_subset = training_requests\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"prompt\": prompts,\n",
    "    \"request\": training_requests_subset,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3442,
     "status": "aborted",
     "timestamp": 1760974360582,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "VM6OGm4Rg5h7"
   },
   "outputs": [],
   "source": [
    "reward_func = create_async_multi_reward(\n",
    "    reward_fn,\n",
    "    use_openai=USE_OPENAI,\n",
    "    openai_batch_size=20,\n",
    "    soft_penalties=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3440,
     "status": "aborted",
     "timestamp": 1760974360583,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "vmzHHQlq_A78"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with MEMORY OPTIMIZATIONS\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,  # ‚ö†Ô∏è Disable KV cache during training (saves memory)\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing (trades compute for memory)\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3432,
     "status": "aborted",
     "timestamp": 1760974360584,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "K1tLdpwg_A79"
   },
   "outputs": [],
   "source": [
    "# GRPO Configuration - ROUND 3 (Optimized for 90+ score)\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=\"./models/italian_grpo_v4\",  # ‚Üê Round 3 output\n",
    "\n",
    "    # Training schedule\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # Learning rate - FINE-TUNED for Round 3\n",
    "    learning_rate=9e-6,  # ‚Üê Lower than Round 2 (5e-6) for stability\n",
    "    warmup_steps=50,     # ‚Üê More warmup for smooth convergence\n",
    "\n",
    "    # Logging & checkpoints\n",
    "    logging_steps=5,     # ‚Üê More frequent logging to catch issues early\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,  # ‚Üê Keep more checkpoints\n",
    "\n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    # Disable wandb\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # GRPO-specific - OPTIMIZED\n",
    "    num_generations=4,              # Keep same (good variance)\n",
    "    max_completion_length=350,      # ‚Üê REDUCED from 1000 (prevent rambling)\n",
    "    temperature=0.7,                # Keep same\n",
    "    generation_batch_size=32,       # Keep same\n",
    "\n",
    "    # Stop tokens\n",
    "    generation_kwargs={\n",
    "      \"bos_token_id\": 128000,\n",
    "      \"do_sample\": True,\n",
    "      \"eos_token_id\": [\n",
    "        128009,\n",
    "        128001,\n",
    "        128009\n",
    "      ],\n",
    "      \"max_new_tokens\": 350,\n",
    "      \"pad_token_id\": 128009,\n",
    "      \"temperature\": 0.7,\n",
    "      \"top_p\": 0.9,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVbb1_DM_A79"
   },
   "source": [
    "## Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3428,
     "status": "aborted",
     "timestamp": 1760974360585,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "OCKUdQbi_A79"
   },
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=grpo_config,\n",
    "    reward_funcs=reward_func,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ GRPO Trainer initialized\")\n",
    "print(\"   Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhx2JyyG_A7-"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3416,
     "status": "aborted",
     "timestamp": 1760974360586,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "3aZ144z6_A7-"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save model\n",
    "output_dir = \"./models/italian_grpo_v4\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
