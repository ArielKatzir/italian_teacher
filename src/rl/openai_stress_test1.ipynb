{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2076,
     "status": "ok",
     "timestamp": 1761006462674,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "cgglLZveCxEf",
    "outputId": "2b4729e8-5379-483e-999e-81aa1ff4a205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/Colab Notebooks/italian_teacher\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to project\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/italian_teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16850,
     "status": "ok",
     "timestamp": 1761006479528,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "nRsfgtcKC3v-",
    "outputId": "addd9f11-067e-4fa1-e0f9-6a7f7ad6f5ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers trl accelerate peft datasets spacy sentence-transformers bitsandbytes json5 openai tqdm nest_asyncio\n",
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 15309,
     "status": "ok",
     "timestamp": 1761006494846,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "V4pamFwuDOBv"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from .scorers import (\n",
    "        CEFRScorer,\n",
    "        CoherenceScorer,\n",
    "        ExerciseQualityScorer,\n",
    "        FluencyScorer,\n",
    "        GrammarScorer,\n",
    "        JSONScorer,\n",
    "        LinguisticScorer,\n",
    "        TopicScorer,\n",
    "    )\n",
    "except ImportError:\n",
    "    from src.rl.reward_function.scorers import (\n",
    "        CEFRScorer,\n",
    "        CoherenceScorer,\n",
    "        ExerciseQualityScorer,\n",
    "        FluencyScorer,\n",
    "        GrammarScorer,\n",
    "        JSONScorer,\n",
    "        LinguisticScorer,\n",
    "        TopicScorer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761006494858,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "lWP0FlPmDS4m",
    "outputId": "776a8f62-d773-469b-e425-fa33db68d75d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API enabled - Professional quality with async batching\n",
      "   OPTIMIZED: Samples 1 exercise/completion (70% reduction in API calls)\n",
      "   Expected training time: ~2-3 hours\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# You can enable/disable OpenAI here:\n",
    "USE_OPENAI = True  # Set to False for faster training without OpenAI\n",
    "\n",
    "if USE_OPENAI:\n",
    "    if \"OPENAI_API_KEY\" not in os.environ:\n",
    "        OPENAI_API_KEY = \"\"\n",
    "        os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(\"‚úÖ OpenAI API enabled - Professional quality with async batching\")\n",
    "    print(\"   OPTIMIZED: Samples 1 exercise/completion (70% reduction in API calls)\")\n",
    "    print(\"   Expected training time: ~2-3 hours\")\n",
    "else:\n",
    "    if \"OPENAI_API_KEY\" in os.environ:\n",
    "        del os.environ[\"OPENAI_API_KEY\"]\n",
    "    print(\"‚úÖ OpenAI API disabled - Fast rule-based rewards\")\n",
    "    print(\"   Expected training time: ~60-90 min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199416,
     "status": "ok",
     "timestamp": 1761006694275,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "CdIqiDaPCoIB",
    "outputId": "fc81df2f-2dbc-41f7-cda9-70132f675d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added '/content/drive/MyDrive/Colab Notebooks/italian_teacher/src' to Python path\n",
      "‚úÖ Successfully imported reward function components.\n",
      "‚úÖ Found OPENAI_API_KEY.\n",
      "\n",
      "Generating 64 mock data samples...\n",
      "‚úÖ Mock data ready.\n",
      "\n",
      "Applied nest_asyncio patch for notebook compatibility.\n",
      "\n",
      "================================================================================\n",
      "üî¨ RUNNING STRESS TEST SUITE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- TEST 2: FINDING THE BREAKING POINT ---\n",
      "We will now increase concurrency to find your account's stable limit.\n",
      "--------------------------------------------------------------------------------\n",
      "üöÄ Starting Test: Concurrency = 4, Client Timeout = 60s\n",
      "--------------------------------------------------------------------------------\n",
      "Instantiating ExerciseRewardFunction (one-time setup)...\n",
      "Loading spaCy model: it_core_news_sm...\n",
      "‚úÖ spaCy model loaded\n",
      "Reward function will use device: cpu\n",
      "Initializing scorers...\n",
      "Pre-loading CEFR vocabulary (16,887 words)...\n",
      "‚úÖ Loaded 16887 Italian words from vocabulary list\n",
      "‚úÖ Loaded vocabulary for all CEFR levels\n",
      "  ‚úÖ OpenAI validation enabled for B2+ levels\n",
      "  ‚úÖ LLM fluency checking enabled (OpenAI API)\n",
      "  ‚úÖ LLM grammar checking enabled (OpenAI API)\n",
      "Loading sentence transformer for topic similarity...\n",
      "‚úÖ Sentence transformer loaded in cpu\n",
      "  ‚úÖ LLM topic checking enabled (OpenAI API)\n",
      "‚úÖ Reward function initialized with 8 professional scorers\n",
      "‚úÖ Base reward function ready.\n",
      "\n",
      "‚è≥ Step 1/3: Parsing 64 JSON completions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Populating work queue with OpenAI tasks (coherence, grammar)...\n",
      "‚è≥ Step 2/3: Processing 192 OpenAI tasks with 4 parallel workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Multi-Reward (Async OpenAI, batch=4, 78.9s):\n",
      "   Grammar   : min=0.583, max=0.583, avg=0.583 (weight=2.0)\n",
      "   Coherence : min=1.000, max=1.000, avg=1.000 (weight=2.5)\n",
      "   Topic     : min=0.150, max=0.150, avg=0.150 (weight=1.5)\n",
      "   Quality   : min=0.870, max=0.870, avg=0.870 (weight=1.0)\n",
      "   Diversity : min=1.000, max=1.000, avg=1.000 (weight=0.5)\n",
      "   TOTAL     : min=5.262, max=5.262, avg=5.262\n",
      "\n",
      "--- Test Results ---\n",
      "‚úÖ Test completed successfully.\n",
      "‚è±Ô∏è  Total Time: 78.95 seconds\n",
      "üì¶ Samples Processed: 64\n",
      "‚ùå Timeout Errors: 0\n",
      "‚öôÔ∏è  Avg Time/Sample: 1.23 seconds\n",
      "\n",
      "üëç SUCCESS: No timeout errors. Concurrency limit of 4 appears stable.\n",
      "--------------------------------------------------------------------------------\n",
      "üöÄ Starting Test: Concurrency = 16, Client Timeout = 60s\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚è≥ Step 1/3: Parsing 64 JSON completions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Populating work queue with OpenAI tasks (coherence, grammar)...\n",
      "‚è≥ Step 2/3: Processing 192 OpenAI tasks with 16 parallel workers...\n",
      "‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Multi-Reward (Async OpenAI, batch=16, 56.9s):\n",
      "   Grammar   : min=0.583, max=0.583, avg=0.583 (weight=2.0)\n",
      "   Coherence : min=1.000, max=1.000, avg=1.000 (weight=2.5)\n",
      "   Topic     : min=0.150, max=0.150, avg=0.150 (weight=1.5)\n",
      "   Quality   : min=0.870, max=0.870, avg=0.870 (weight=1.0)\n",
      "   Diversity : min=1.000, max=1.000, avg=1.000 (weight=0.5)\n",
      "   TOTAL     : min=5.262, max=5.262, avg=5.262\n",
      "\n",
      "--- Test Results ---\n",
      "‚úÖ Test completed successfully.\n",
      "‚è±Ô∏è  Total Time: 56.89 seconds\n",
      "üì¶ Samples Processed: 64\n",
      "‚ùå Timeout Errors: 0\n",
      "‚öôÔ∏è  Avg Time/Sample: 0.89 seconds\n",
      "\n",
      "üëç SUCCESS: No timeout errors. Concurrency limit of 16 appears stable.\n",
      "--------------------------------------------------------------------------------\n",
      "üöÄ Starting Test: Concurrency = 32, Client Timeout = 60s\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚è≥ Step 1/3: Parsing 64 JSON completions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Populating work queue with OpenAI tasks (coherence, grammar)...\n",
      "‚è≥ Step 2/3: Processing 192 OpenAI tasks with 32 parallel workers...\n",
      "‚è≥ Step 3/3: Computing CPU-bound rewards and aggregating results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Multi-Reward (Async OpenAI, batch=32, 58.3s):\n",
      "   Grammar   : min=0.583, max=0.583, avg=0.583 (weight=2.0)\n",
      "   Coherence : min=1.000, max=1.000, avg=1.000 (weight=2.5)\n",
      "   Topic     : min=0.150, max=0.150, avg=0.150 (weight=1.5)\n",
      "   Quality   : min=0.870, max=0.870, avg=0.870 (weight=1.0)\n",
      "   Diversity : min=1.000, max=1.000, avg=1.000 (weight=0.5)\n",
      "   TOTAL     : min=5.262, max=5.262, avg=5.262\n",
      "\n",
      "--- Test Results ---\n",
      "‚úÖ Test completed successfully.\n",
      "‚è±Ô∏è  Total Time: 58.33 seconds\n",
      "üì¶ Samples Processed: 64\n",
      "‚ùå Timeout Errors: 0\n",
      "‚öôÔ∏è  Avg Time/Sample: 0.91 seconds\n",
      "\n",
      "üëç SUCCESS: No timeout errors. Concurrency limit of 32 appears stable.\n",
      "\n",
      "================================================================================\n",
      "üìä FINAL ANALYSIS & RECOMMENDATION\n",
      "================================================================================\n",
      "Concurrency Level | Timeout Errors | Total Time (s)\n",
      "------------------|----------------|---------------\n",
      "4                 | 0              | 78.95         \n",
      "16                | 0              | 56.89         \n",
      "32                | 0              | 58.33         \n",
      "\n",
      "‚úÖ Recommendation: Your maximum stable concurrency limit appears to be around 32.\n",
      "   Set `openai_batch_size = 32` in your training script for the best balance of speed and stability.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Stress Test Notebook for Async Reward Function\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import traceback\n",
    "import httpx\n",
    "import random\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "# Add your project's root directory to the Python path\n",
    "# Adjust this path if your notebook is in a different location\n",
    "try:\n",
    "    # Assumes notebook is in the root of the 'italian_teacher' project\n",
    "    project_root = Path.cwd()\n",
    "    src_path = project_root / \"src\"\n",
    "    if str(src_path) not in sys.path:\n",
    "        sys.path.insert(0, str(src_path))\n",
    "    print(f\"‚úÖ Added '{src_path}' to Python path\")\n",
    "\n",
    "    from rl.multi_reward_async import AsyncMultiReward\n",
    "    from rl.reward_function.reward_function_modular import ExerciseRewardFunction\n",
    "    print(\"‚úÖ Successfully imported reward function components.\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"\\n‚ùå FAILED TO IMPORT PROJECT MODULES.\")\n",
    "    print(\"Please ensure that your 'src' directory is accessible from this notebook.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # Stop execution if imports fail\n",
    "    raise\n",
    "\n",
    "# Check for OpenAI API Key\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"\\n‚ùå ERROR: OPENAI_API_KEY environment variable not set.\")\n",
    "    print(\"The stress test cannot run without an API key.\")\n",
    "    # Stop execution\n",
    "    raise ValueError(\"OPENAI_API_KEY not set\")\n",
    "else:\n",
    "    print(\"‚úÖ Found OPENAI_API_KEY.\")\n",
    "\n",
    "\n",
    "# --- 2. MOCK DATA ---\n",
    "# We'll create a realistic set of mock data to simulate the trainer's output.\n",
    "NUM_SAMPLES = 64  # A good number to stress the API\n",
    "\n",
    "print(f\"\\nGenerating {NUM_SAMPLES} mock data samples...\")\n",
    "\n",
    "# Mock completions (a mix of good and slightly malformed JSON)\n",
    "mock_completions = [\n",
    "    \"\"\"\n",
    "[\n",
    "  {\"type\": \"fill_in_blank\", \"question\": \"Ieri, io ___ (andare) al cinema.\", \"correct_answer\": \"sono andato\"},\n",
    "  {\"type\": \"translation\", \"question\": \"The cat is on the table.\", \"correct_answer\": \"Il gatto √® sul tavolo.\"}\n",
    "]\n",
    "    \"\"\"\n",
    "] * NUM_SAMPLES\n",
    "\n",
    "# Mock requests that correspond to the completions\n",
    "mock_requests = [\n",
    "    {\"topic\": \"Daily Life\", \"level\": \"A2\", \"grammar_focus\": \"past_tense\"}\n",
    "] * NUM_SAMPLES\n",
    "\n",
    "print(\"‚úÖ Mock data ready.\")\n",
    "\n",
    "\n",
    "# --- 3. CORE TEST FUNCTION ---\n",
    "\n",
    "# We need to slightly modify the reward function to count errors instead of just printing them\n",
    "class StressTestReward(AsyncMultiReward):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.timeout_errors = 0\n",
    "\n",
    "    async def _check_coherence_async(self, *args, **kwargs):\n",
    "        try:\n",
    "            return await super()._check_coherence_async(*args, **kwargs)\n",
    "        except (asyncio.TimeoutError, httpx.TimeoutException):\n",
    "            self.timeout_errors += 1\n",
    "            return 0.0 # Return a penalty score on timeout\n",
    "\n",
    "# We also need to modify the grammar scorer to count errors\n",
    "# This is a bit more involved due to the class structure, so we'll patch it.\n",
    "original_check_grammar_with_llm = GrammarScorer._check_grammar_with_llm\n",
    "async def patched_check_grammar_with_llm(self, *args, **kwargs):\n",
    "    try:\n",
    "        return await original_check_grammar_with_llm(self, *args, **kwargs)\n",
    "    except (asyncio.TimeoutError, httpx.TimeoutException) as e:\n",
    "        # This is a bit of a hack to access the parent's error counter\n",
    "        # In a real app, you'd use a more robust logging/metrics system\n",
    "        if hasattr(self, '_reward_function_instance_for_test'):\n",
    "             self._reward_function_instance_for_test.timeout_errors += 1\n",
    "        print(f\"  ‚ö†Ô∏è (Patched) LLM grammar check timed out: {e}\")\n",
    "        return 5.0, [f\"LLM grammar check timed out: {e}\"]\n",
    "\n",
    "GrammarScorer._check_grammar_with_llm = patched_check_grammar_with_llm\n",
    "\n",
    "\n",
    "async def run_stress_test(concurrency_limit: int, client_timeout: int):\n",
    "    \"\"\"\n",
    "    Runs a single stress test with a given concurrency limit and timeout.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"üöÄ Starting Test: Concurrency = {concurrency_limit}, Client Timeout = {client_timeout}s\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Instantiate the base reward function (loads models, etc.)\n",
    "    # We only do this once to be efficient\n",
    "    if 'base_reward_fn' not in globals():\n",
    "        print(\"Instantiating ExerciseRewardFunction (one-time setup)...\")\n",
    "        global base_reward_fn\n",
    "        base_reward_fn = ExerciseRewardFunction()\n",
    "        print(\"‚úÖ Base reward function ready.\")\n",
    "\n",
    "    # Instantiate our test reward function\n",
    "    reward_func = StressTestReward(\n",
    "        reward_fn=base_reward_fn,\n",
    "        use_openai=True,\n",
    "        openai_batch_size=concurrency_limit,\n",
    "        openai_timeout=client_timeout\n",
    "    )\n",
    "\n",
    "    # This is the hack to allow the patched grammar scorer to count errors\n",
    "    base_reward_fn.scorers['grammar']._reward_function_instance_for_test = reward_func\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # We call `_process_batch` directly to test the core async logic\n",
    "        rewards = await reward_func._process_batch(mock_completions, mock_requests)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(\"\\n--- Test Results ---\")\n",
    "        print(f\"‚úÖ Test completed successfully.\")\n",
    "        print(f\"‚è±Ô∏è  Total Time: {elapsed:.2f} seconds\")\n",
    "        print(f\"üì¶ Samples Processed: {len(rewards)}\")\n",
    "        print(f\"‚ùå Timeout Errors: {reward_func.timeout_errors}\")\n",
    "\n",
    "        avg_time_per_sample = elapsed / len(rewards) if rewards else 0\n",
    "        print(f\"‚öôÔ∏è  Avg Time/Sample: {avg_time_per_sample:.2f} seconds\")\n",
    "\n",
    "        if reward_func.timeout_errors > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è WARNING: Encountered {reward_func.timeout_errors} timeout errors. The concurrency limit of {concurrency_limit} is likely too high.\")\n",
    "        else:\n",
    "            print(f\"\\nüëç SUCCESS: No timeout errors. Concurrency limit of {concurrency_limit} appears stable.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚ùå TEST FAILED after {elapsed:.2f} seconds.\")\n",
    "        print(\"An unexpected error occurred during the test:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return reward_func.timeout_errors, elapsed\n",
    "\n",
    "\n",
    "# --- 4. RUN THE EXPERIMENTS ---\n",
    "\n",
    "async def main():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üî¨ RUNNING STRESS TEST SUITE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # --- Test 2: Finding the Breaking Point ---\n",
    "    print(\"\\n\\n--- TEST 2: FINDING THE BREAKING POINT ---\")\n",
    "    print(\"We will now increase concurrency to find your account's stable limit.\")\n",
    "    concurrency_levels = [4, 16, 32]\n",
    "    results = {}\n",
    "    for level in concurrency_levels:\n",
    "        errors, elapsed = await run_stress_test(concurrency_limit=level, client_timeout=60)\n",
    "        results[level] = (errors, elapsed)\n",
    "\n",
    "    # --- Final Analysis ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä FINAL ANALYSIS & RECOMMENDATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"Concurrency Level | Timeout Errors | Total Time (s)\")\n",
    "    print(\"------------------|----------------|---------------\")\n",
    "    for level, (errors, elapsed) in results.items():\n",
    "        print(f\"{level:<17} | {errors:<14} | {elapsed:<14.2f}\")\n",
    "\n",
    "    stable_limit = 0\n",
    "    for level, (errors, elapsed) in results.items():\n",
    "        if errors == 0:\n",
    "            stable_limit = level\n",
    "        else:\n",
    "            # The first level with errors is the breaking point\n",
    "            break\n",
    "\n",
    "    if stable_limit > 0:\n",
    "        print(f\"\\n‚úÖ Recommendation: Your maximum stable concurrency limit appears to be around {stable_limit}.\")\n",
    "        print(f\"   Set `openai_batch_size = {stable_limit}` in your training script for the best balance of speed and stability.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Warning: Timeouts occurred even at the lowest tested concurrency level (5).\")\n",
    "        print(\"   This may indicate a very strict rate limit on your OpenAI account or a network issue.\")\n",
    "        print(\"   Try setting `openai_batch_size = 2` in your training script.\")\n",
    "\n",
    "# Run the main async function\n",
    "if __name__ == \"__main__\":\n",
    "    # Using nest_asyncio to allow running asyncio.run() in a notebook\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        print(\"\\nApplied nest_asyncio patch for notebook compatibility.\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    asyncio.run(main())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMSArrE6NY36PEROkhppQ/c",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
