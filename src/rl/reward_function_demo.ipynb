{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1834,
     "status": "ok",
     "timestamp": 1760958528581,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "4BbdF9OeIStw",
    "outputId": "bcaeca21-41ed-4267-b0e2-44cac8f13e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/Colab Notebooks/italian_teacher\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to project\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/italian_teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12192,
     "status": "ok",
     "timestamp": 1760958540775,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "zCLxVFRqIdEC",
    "outputId": "f0bcb785-73fc-4eb2-ec88-0889116b50ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m145.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers trl accelerate peft datasets spacy sentence-transformers bitsandbytes json5 openai tqdm nest_asyncio\n",
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12196,
     "status": "ok",
     "timestamp": 1760958552975,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "FwsaVmMxIduI",
    "outputId": "0eb477af-c356-4193-9b1e-9a7dbd106982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1760958552986,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "CeLrg9I1IfYj",
    "outputId": "7cfdfaf5-f6c4-4b94-bd61-668045c6c6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API enabled - Professional quality with async batching\n",
      "   OPTIMIZED: Samples 1 exercise/completion (70% reduction in API calls)\n",
      "   Expected training time: ~2-3 hours\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# You can enable/disable OpenAI here:\n",
    "USE_OPENAI = True  # Set to False for faster training without OpenAI\n",
    "\n",
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6000,
     "status": "ok",
     "timestamp": 1760958558988,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "0g_knnwdIhX1",
    "outputId": "976b2968-58c6-4ce8-91ad-12477382a5c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model: it_core_news_sm...\n",
      "‚úÖ spaCy model loaded\n",
      "Reward function will use device: cuda\n",
      "Initializing scorers...\n",
      "Pre-loading CEFR vocabulary (16,887 words)...\n",
      "‚úÖ Loaded 16887 Italian words from vocabulary list\n",
      "‚úÖ Loaded vocabulary for all CEFR levels\n",
      "  ‚úÖ OpenAI validation enabled for B2+ levels\n",
      "  ‚úÖ LLM fluency checking enabled (OpenAI API)\n",
      "  ‚úÖ LLM grammar checking enabled (OpenAI API)\n",
      "Loading sentence transformer for topic similarity...\n",
      "‚úÖ Sentence transformer loaded in cuda\n",
      "  ‚úÖ LLM topic checking enabled (OpenAI API)\n",
      "  ‚úÖ LLM coherence checking enabled (OpenAI API)\n",
      "‚úÖ Reward function initialized with 8 professional scorers\n",
      "‚úÖ Reward function ready (running on GPU)\n"
     ]
    }
   ],
   "source": [
    "from src.rl.reward_function import ExerciseRewardFunction\n",
    "from src.rl.prompt_formatter import format_prompt_with_chat_template  # ‚Üê ROUND 3: Enhanced V1 (not V3!)\n",
    "from src.rl.multi_reward_async import create_async_multi_reward\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "reward_fn = ExerciseRewardFunction(device=\"cuda\")\n",
    "print(\"‚úÖ Reward function ready (running on GPU)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1760958558992,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "HQhBQlMPIuB2"
   },
   "outputs": [],
   "source": [
    "training_requests = [  {\n",
    "    \"level\": \"A2\",\n",
    "    \"grammar_focus\": \"past_tense\",\n",
    "    \"topic\": \"abbigliamento\",\n",
    "    \"num_exercises\": 4,\n",
    "    \"exercise_types\": [\n",
    "      \"multiple_choice\",\n",
    "      \"fill_in_blank\",\n",
    "      \"translation\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"level\": \"B2\",\n",
    "    \"grammar_focus\": \"conditional\",\n",
    "    \"topic\": \"casa\",\n",
    "    \"num_exercises\": 3,\n",
    "    \"exercise_types\": [\n",
    "      \"fill_in_blank\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"level\": \"A2\",\n",
    "    \"grammar_focus\": \"subjunctive\",\n",
    "    \"topic\": \"casa\",\n",
    "    \"num_exercises\": 5,\n",
    "    \"exercise_types\": [\n",
    "      \"translation\",\n",
    "      \"multiple_choice\",\n",
    "      \"fill_in_blank\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"level\": \"B2\",\n",
    "    \"grammar_focus\": \"conditional\",\n",
    "    \"topic\": \"inganno\",\n",
    "    \"num_exercises\": 5,\n",
    "    \"exercise_types\": [\n",
    "      \"translation\"\n",
    "    ]\n",
    "  }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1760958559764,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "umESg23vI6TP",
    "outputId": "d217b1ec-4233-4140-ca6e-35bfe85605f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìã ROUND 3 DATASET PREPARATION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ROUND 3: Start from Round 2 model (86.5/100 baseline)\n",
    "# models/italian_v8_grpo_round2\n",
    "MODEL_PATH = \"./models/italian_v8_grpo_round2\"  # ‚Üê Round 2 GRPO model (best so far)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã ROUND 3 DATASET PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use V3 prompt formatter with ENHANCED guidance!\n",
    "prompts = [\n",
    "    format_prompt_with_chat_template(req, tokenizer, add_examples=True)\n",
    "    for req in training_requests\n",
    "]\n",
    "\n",
    "reward_func = create_async_multi_reward(\n",
    "    reward_fn,\n",
    "    use_openai=USE_OPENAI,\n",
    "    openai_batch_size=4,\n",
    "    soft_penalties=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "2f32b2b342f942fa8592af678165df65",
      "f12e768e3d044b89aa59ce86dc803ac7",
      "a41a82a4903f494aac1fab4998a71714",
      "e82b9e47dce24773ac8295ca9f982c4f",
      "95d34cd1c0db45f9a6ffadc99bd85e2f",
      "0c16c6128110405ba65560a2e8665f39",
      "5d2df6d0c30f4ed39051d4d16030440f",
      "21a6cc6f531d4bb79d667e5eda1c47f6",
      "4405320007804420ba79e176c11393a6",
      "090c6039f06f45e59b9a5c7519717fe4",
      "3c7e73777d1a46178bc72cf0c7f6415d"
     ]
    },
    "executionInfo": {
     "elapsed": 12022,
     "status": "ok",
     "timestamp": 1760958571790,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "rT4dNmfGJVvC",
    "outputId": "8cdbcc8b-d046-4e78-b3d4-c5d318f83084"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f32b2b342f942fa8592af678165df65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with MEMORY OPTIMIZATIONS\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,  # ‚ö†Ô∏è Disable KV cache during training (saves memory)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24572,
     "status": "ok",
     "timestamp": 1760958596370,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "SkOsh2HZKJak",
    "outputId": "c4fe3eef-359a-4703-a009-f3ac505b6510"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing prompts in a batch...\n",
      "Generating 4 completions in a single batch...\n",
      "Generation complete.\n",
      "\n",
      "--- Generated Exercise 1 ---\n",
      "n\": \"past_tense of 'comprare'\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"Ieri, ___ ho portato ___ camicia preferita.\", \"correct_answer\": \"ho\", \"options\": null, \"explanation\": \"past_tense of 'portare'\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"La settimana scorsa, ___ ho cambiato ___ vestito.\", \"correct_answer\": \"ho\", \"options\": null, \"explanation\": \"past_tense of 'cambiare'\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"L'anno scorso, ___ non ho indossato ___ scarpa con la giacca.\", \"correct_answer\": \"non ho\", \"options\": null, \"explanation\": \"past_tense of 'indossare'\" }\n",
      "]\n",
      "\n",
      "\n",
      "--- Generated Exercise 2 ---\n",
      "ion\": \"Subject-verb agreement in conditional\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"Se ___ avessi una casa, avrei potuto organizzare una festa.\", \"correct_answer\": \"tu\", \"options\": null, \"explanation\": \"Subject-verb agreement in conditional\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"Se avessimo una casa, ___ avremmo potuto affittare una stanza.\", \"correct_answer\": \"noi\", \"options\": null, \"explanation\": \"Subject-verb agreement in conditional\" }\n",
      "]\n",
      "\n",
      "\n",
      "--- Generated Exercise 3 ---\n",
      "tion\": \"Subjunctive in relative clauses.\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"Spero che la casa ___ sia venduta presto.\", \"correct_answer\": \"che\", \"options\": null, \"explanation\": \"Subjunctive in subordinate clauses.\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"La casa ___ si possa vedere √® stata ristrutturata.\", \"correct_answer\": \"che\", \"options\": null, \"explanation\": \"Subjunctive in subordinate clauses.\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"La casa ___ sia grande, non credo che sia necessaria.\", \"correct_answer\": \"che\", \"options\": null, \"explanation\": \"Subjunctive in subordinate clauses.\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"La casa ___ sia accogliente, √® un luogo speciale.\", \"correct_answer\": \"che\", \"options\": null, \"explanation\": \"Subjunctive in subordinate clauses.\" }\n",
      "]\n",
      "\n",
      "\n",
      "--- Generated Exercise 4 ---\n",
      "planation\": \"Conditional past of 'avere'\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"Se avessi visto la trappola, non ci sarebbe stato inganno.\", \"correct_answer\": \"avessi\", \"options\": null, \"explanation\": \"Conditional past of 'avere'\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"Se ___ avessimo saputo, non ci sarebbe stato inganno.\", \"correct_answer\": \"avessimo\", \"options\": null, \"explanation\": \"Conditional past of 'avere'\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"Se ___ avessi visto, non ci sarebbe stato inganno.\", \"correct_answer\": \"avessi\", \"options\": null, \"explanation\": \"Conditional past of 'avere'\" },\n",
      "  {\"type\": \"fill_in_blank\", \"question\": \"Se ___ avessimo capito, non ci sarebbe stato inganno.\", \"correct_answer\": \"avessimo\", \"options\": null, \"explanation\": \"Conditional past of 'avere'\" }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenize all prompts at once\n",
    "#    - padding=True ensures all sequences in the batch have the same length.\n",
    "#    - truncation=True prevents prompts from being longer than the model's max length.\n",
    "print(\"Tokenizing prompts in a batch...\")\n",
    "inputs = tokenizer(\n",
    "    prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ").to(model.device)\n",
    "\n",
    "# 2. Generate all outputs in a single, parallelized batch call\n",
    "print(f\"Generating {len(prompts)} completions in a single batch...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=350,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(\"Generation complete.\")\n",
    "\n",
    "# 3. Decode all generated texts at once\n",
    "#    Use batch_decode for efficiency.\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Print generated exercises\n",
    "for i, gen in enumerate(generated_texts):\n",
    "    # The generated text will include the original prompt. We can clean it up.\n",
    "    clean_gen = gen[len(prompts[i]):].strip()\n",
    "    print(f\"\\n--- Generated Exercise {i+1} ---\\n{clean_gen}\\n\")\n",
    "\n",
    "# Now you can pass the full lists to your reward function\n",
    "# rewards = reward_func(prompts=prompts, completions=generated_texts, request=training_requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5400,
     "status": "ok",
     "timestamp": 1760958601777,
     "user": {
      "displayName": "Ariel Katzir",
      "userId": "13010007500212358071"
     },
     "user_tz": -60
    },
    "id": "Dk6a0jfHKL2l",
    "outputId": "35c5a0fd-f298-4f2b-ae8d-aabfbf070bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating rewards...\n",
      "\n",
      "‚è≥ Step 1/3: Parsing 4 JSON completions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Checking 4 exercises (1 per completion, ~70% reduction)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Step 3/3: Computing rule-based and grammar rewards (in parallel)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rComputing Rewards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM coherence SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM coherence SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rComputing Rewards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:03<00:10,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM coherence SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM grammer SUCCESS\n",
      "LLM grammer SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rComputing Rewards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM grammer SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM grammer SUCCESS\n",
      "\n",
      "üéØ Multi-Reward (Async OpenAI, batch=4, 5.4s):\n",
      "   Grammar   : min=0.500, max=0.500, avg=0.500 (weight=2.0)\n",
      "   Coherence : min=1.000, max=1.000, avg=1.000 (weight=2.5)\n",
      "   Topic     : min=0.067, max=0.467, avg=0.300 (weight=1.5)\n",
      "   Quality   : min=0.792, max=0.843, avg=0.812 (weight=1.0)\n",
      "   Diversity : min=1.000, max=1.000, avg=1.000 (weight=0.5)\n",
      "   TOTAL     : min=4.892, max=5.523, avg=5.263\n",
      "\n",
      "--- Rewards ---\n",
      "Reward for Exercise 1: 5.1433\n",
      "Reward for Exercise 2: 5.4917\n",
      "Reward for Exercise 3: 5.5233\n",
      "Reward for Exercise 4: 4.8917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Apply reward function to each generated output\n",
    "# The reward_func is a synchronous function that handles async internally.\n",
    "# Just call it directly.\n",
    "print(\"\\nEvaluating rewards...\")\n",
    "rewards = reward_func(prompts=prompts, completions=generated_texts, request=training_requests) # Assuming 'requests' is available\n",
    "\n",
    "# Print rewards\n",
    "print(\"\\n--- Rewards ---\")\n",
    "for i, r in enumerate(rewards):\n",
    "    print(f\"Reward for Exercise {i+1}: {r:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPtjgK/Ea2zzjf24Y5GATFi",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "090c6039f06f45e59b9a5c7519717fe4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c16c6128110405ba65560a2e8665f39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21a6cc6f531d4bb79d667e5eda1c47f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f32b2b342f942fa8592af678165df65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f12e768e3d044b89aa59ce86dc803ac7",
       "IPY_MODEL_a41a82a4903f494aac1fab4998a71714",
       "IPY_MODEL_e82b9e47dce24773ac8295ca9f982c4f"
      ],
      "layout": "IPY_MODEL_95d34cd1c0db45f9a6ffadc99bd85e2f"
     }
    },
    "3c7e73777d1a46178bc72cf0c7f6415d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4405320007804420ba79e176c11393a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d2df6d0c30f4ed39051d4d16030440f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95d34cd1c0db45f9a6ffadc99bd85e2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a41a82a4903f494aac1fab4998a71714": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21a6cc6f531d4bb79d667e5eda1c47f6",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4405320007804420ba79e176c11393a6",
      "value": 4
     }
    },
    "e82b9e47dce24773ac8295ca9f982c4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_090c6039f06f45e59b9a5c7519717fe4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3c7e73777d1a46178bc72cf0c7f6415d",
      "value": "‚Äá4/4‚Äá[00:11&lt;00:00,‚Äá‚Äá2.54s/it]"
     }
    },
    "f12e768e3d044b89aa59ce86dc803ac7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c16c6128110405ba65560a2e8665f39",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5d2df6d0c30f4ed39051d4d16030440f",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
