{"cells":[{"cell_type":"markdown","metadata":{"id":"WiSEitdjH7fR"},"source":["# Model Performance Benchmark\n","\n","This notebook tests different LLM models for scoring Italian exercises.\n","\n","**Models tested:**\n","- gpt-4o-mini\n","- gpt-3.5-turbo-0125\n","- gpt-3.5-turbo\n","- gpt-4.1-mini\n","\n","**Metrics:**\n","- Inference time per model\n","- Scoring breakdown per test case"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XExbGa_iH7fT","executionInfo":{"status":"ok","timestamp":1761492893923,"user_tz":0,"elapsed":13974,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"4d4be593-27f8-4da3-f2a2-8ba13425f02d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running in Google Colab. Installing dependencies...\n","Collecting it-core-news-sm==3.8.0\n","  Using cached https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('it_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Setup complete\n"]}],"source":["# Install dependencies if running in Colab\n","import sys\n","\n","if 'google.colab' in sys.modules:\n","    print(\"Running in Google Colab. Installing dependencies...\")\n","    !pip install -q spacy transformers torch sentence-transformers openai httpx json5\n","    !python -m spacy download it_core_news_sm\n","\n","    # Mount Google Drive to access the files\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Set the working directory to your project\n","    import os\n","    os.chdir('/content/drive/My Drive/Colab Notebooks/italian_teacher')\n","    sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/italian_teacher')\n","\n","    # Set OpenAI API key\n","    from google.colab import userdata\n","    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n","\n","print(\"✅ Setup complete\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQHZAK2WH7fV"},"outputs":[],"source":["import asyncio\n","import json\n","import time\n","import random\n","from pathlib import Path\n","from typing import Dict, List\n","\n","# Import the reward function\n","from src.rl.reward_function.reward_function_modular import ExerciseRewardFunction\n","from src.rl.reward_function.scorers.base_llm_scorer import BaseLLMScorer\n","\n","print(\"✅ Imports successful\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GoGQOiuaH7fW"},"outputs":[],"source":["# Load validation requests and sample 3\n","validation_path = Path(\"src/rl/validation_requests.json\")\n","\n","with open(validation_path, 'r') as f:\n","    all_requests = json.load(f)\n","\n","# Sample 3 requests\n","sample_requests = random.sample(all_requests, 3)\n","\n","print(f\"Loaded {len(all_requests)} validation requests\")\n","print(f\"\\nSelected 3 sample requests:\")\n","for i, req in enumerate(sample_requests, 1):\n","    print(f\"  {i}. Level: {req['level']}, Grammar: {req['grammar_focus']}, Topic: {req['topic']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGt0whQeH7fW"},"outputs":[],"source":["# Define models to test\n","models_to_test = [\n","    \"gpt-4o-mini\",\n","    \"gpt-3.5-turbo-0125\",\n","    \"gpt-3.5-turbo\",\n","    \"gpt-4.1-mini\"\n","]\n","\n","print(f\"Models to test: {models_to_test}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvUJjdVlH7fW"},"outputs":[],"source":["# Create mock exercises for testing\n","# In a real scenario, you would generate these using your model\n","def create_mock_exercises(request: Dict) -> List[Dict]:\n","    \"\"\"\n","    Create mock exercises based on the request.\n","    Replace this with actual model generation in production.\n","    \"\"\"\n","    exercises = []\n","    num_exercises = request.get(\"num_exercises\", 3)\n","    topic = request.get(\"topic\", \"general\")\n","    grammar_focus = request.get(\"grammar_focus\", \"general\")\n","    exercise_types = request.get(\"exercise_types\", [\"fill_in_blank\"])\n","\n","    for i in range(num_exercises):\n","        ex_type = exercise_types[i % len(exercise_types)]\n","\n","        if ex_type == \"fill_in_blank\":\n","            exercise = {\n","                \"type\": \"fill_in_blank\",\n","                \"question\": f\"La {topic} ___ molto bella. (essere)\",\n","                \"correct_answer\": \"è\",\n","            }\n","        elif ex_type == \"translation\":\n","            exercise = {\n","                \"type\": \"translation\",\n","                \"question\": f\"Translate to Italian: The {topic} is beautiful.\",\n","                \"correct_answer\": f\"Il {topic} è bello.\",\n","            }\n","        elif ex_type == \"multiple_choice\":\n","            exercise = {\n","                \"type\": \"multiple_choice\",\n","                \"question\": f\"Quale parola completa la frase? 'Il {topic} ___ interessante.'\",\n","                \"correct_answer\": \"è\",\n","                \"options\": [\"è\", \"sei\", \"siamo\", \"sono\"],\n","            }\n","        else:\n","            exercise = {\n","                \"type\": \"fill_in_blank\",\n","                \"question\": f\"La {topic} ___ molto bella. (essere)\",\n","                \"correct_answer\": \"è\",\n","            }\n","\n","        exercises.append(exercise)\n","\n","    return exercises\n","\n","print(\"✅ Mock exercise generator ready\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phenMjkuH7fX"},"outputs":[],"source":["async def benchmark_model(model_name: str, reward_fn: ExerciseRewardFunction, request: Dict, exercises: List[Dict]):\n","    \"\"\"\n","    Benchmark a single model configuration.\n","    \"\"\"\n","    # Set the model override for all scorers\n","    BaseLLMScorer.set_model_override(model_name)\n","\n","    # Time the scoring\n","    start_time = time.time()\n","\n","    try:\n","        avg_score, results = await reward_fn.score_exercises(exercises, request)\n","\n","        elapsed_time = time.time() - start_time\n","\n","        # Get the breakdown from the first exercise\n","        first_breakdown = results[0][1] if results else None\n","\n","        return {\n","            \"model\": model_name,\n","            \"avg_score\": avg_score,\n","            \"elapsed_time\": elapsed_time,\n","            \"breakdown\": str(first_breakdown) if first_breakdown else \"N/A\",\n","            \"all_results\": results,\n","            \"success\": True,\n","            \"error\": None\n","        }\n","    except Exception as e:\n","        elapsed_time = time.time() - start_time\n","        return {\n","            \"model\": model_name,\n","            \"avg_score\": 0.0,\n","            \"elapsed_time\": elapsed_time,\n","            \"breakdown\": \"Error\",\n","            \"all_results\": [],\n","            \"success\": False,\n","            \"error\": str(e)\n","        }\n","    finally:\n","        # Clear the model override\n","        BaseLLMScorer.clear_model_override()\n","\n","print(\"✅ Benchmark function ready\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"us1fHL9aH7fX"},"outputs":[],"source":["# Initialize the reward function\n","print(\"Initializing reward function...\")\n","reward_fn = ExerciseRewardFunction(\n","    spacy_model=\"it_core_news_sm\",\n","    device=\"cpu\",  # Use CPU for Colab compatibility\n","    disabled_scorers=[],  # Enable all scorers\n","    fluency_use_llm=False,  # Disable LLM for fluency to speed up testing\n","    concurrency_limit=20\n",")\n","print(\"✅ Reward function initialized\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqGRCfl1H7fX"},"outputs":[],"source":["# Run the benchmark\n","print(\"=\"*80)\n","print(\"STARTING BENCHMARK\")\n","print(\"=\"*80)\n","\n","all_results = []\n","\n","for test_idx, request in enumerate(sample_requests, 1):\n","    print(f\"\\n{'='*80}\")\n","    print(f\"TEST CASE {test_idx}\")\n","    print(f\"{'='*80}\")\n","    print(f\"Level: {request['level']}\")\n","    print(f\"Grammar Focus: {request['grammar_focus']}\")\n","    print(f\"Topic: {request['topic']}\")\n","    print(f\"Number of Exercises: {request.get('num_exercises', 3)}\")\n","    print(f\"Exercise Types: {request.get('exercise_types', [])}\")\n","\n","    # Create mock exercises\n","    exercises = create_mock_exercises(request)\n","    print(f\"\\nGenerated {len(exercises)} mock exercises\")\n","\n","    # Print out the exercises\n","    print(f\"\\n{'─'*80}\")\n","    print(\"EXERCISES:\")\n","    print(f\"{'─'*80}\")\n","    for i, ex in enumerate(exercises, 1):\n","        print(f\"\\nExercise {i}:\")\n","        print(f\"  Type: {ex['type']}\")\n","        print(f\"  Question: {ex['question']}\")\n","        print(f\"  Correct Answer: {ex['correct_answer']}\")\n","        if 'options' in ex:\n","            print(f\"  Options: {ex['options']}\")\n","    print(f\"{'─'*80}\\n\")\n","\n","    test_results = {\n","        \"request\": request,\n","        \"exercises\": exercises,\n","        \"model_results\": []\n","    }\n","\n","    # Test each model\n","    for model_name in models_to_test:\n","        print(f\"\\n{'-'*80}\")\n","        print(f\"Testing model: {model_name}\")\n","        print(f\"{'-'*80}\")\n","\n","        result = await benchmark_model(model_name, reward_fn, request, exercises)\n","        test_results[\"model_results\"].append(result)\n","\n","        if result[\"success\"]:\n","            print(f\"\\n✅ Model: {model_name}\")\n","            print(f\"   Elapsed Time: {result['elapsed_time']:.2f}s\")\n","            print(f\"   Average Score: {result['avg_score']:.2f}/100\")\n","            print(f\"\\n   Reward Breakdown (First Exercise):\")\n","            # Indent each line of the breakdown\n","            for line in result['breakdown'].split('\\n'):\n","                print(f\"   {line}\")\n","        else:\n","            print(f\"\\n❌ Model: {model_name}\")\n","            print(f\"   Elapsed Time: {result['elapsed_time']:.2f}s\")\n","            print(f\"   Error: {result['error']}\")\n","\n","    all_results.append(test_results)\n","\n","print(f\"\\n{'='*80}\")\n","print(\"BENCHMARK COMPLETE\")\n","print(f\"{'='*80}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nl1YlXDfH7fY"},"outputs":[],"source":["# Summary statistics\n","print(\"\\n\" + \"=\"*80)\n","print(\"SUMMARY STATISTICS\")\n","print(\"=\"*80)\n","\n","# Calculate average time and score per model across all tests\n","model_stats = {model: {\"times\": [], \"scores\": [], \"errors\": 0} for model in models_to_test}\n","\n","for test_result in all_results:\n","    for model_result in test_result[\"model_results\"]:\n","        model = model_result[\"model\"]\n","        if model_result[\"success\"]:\n","            model_stats[model][\"times\"].append(model_result[\"elapsed_time\"])\n","            model_stats[model][\"scores\"].append(model_result[\"avg_score\"])\n","        else:\n","            model_stats[model][\"errors\"] += 1\n","\n","print(f\"\\n{'Model':<25} {'Avg Time (s)':<15} {'Avg Score':<15} {'Errors':<10}\")\n","print(\"-\" * 70)\n","\n","for model in models_to_test:\n","    stats = model_stats[model]\n","    avg_time = sum(stats[\"times\"]) / len(stats[\"times\"]) if stats[\"times\"] else 0\n","    avg_score = sum(stats[\"scores\"]) / len(stats[\"scores\"]) if stats[\"scores\"] else 0\n","    errors = stats[\"errors\"]\n","\n","    print(f\"{model:<25} {avg_time:<15.2f} {avg_score:<15.2f} {errors:<10}\")\n","\n","print(\"\\n\" + \"=\"*80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VYB3TACH7fY"},"outputs":[],"source":["# Visualize timing comparison\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Prepare data for plotting\n","model_names = []\n","avg_times = []\n","avg_scores = []\n","\n","for model in models_to_test:\n","    stats = model_stats[model]\n","    if stats[\"times\"]:\n","        model_names.append(model)\n","        avg_times.append(sum(stats[\"times\"]) / len(stats[\"times\"]))\n","        avg_scores.append(sum(stats[\"scores\"]) / len(stats[\"scores\"]))\n","\n","# Create subplots\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Plot 1: Average inference time\n","ax1.bar(range(len(model_names)), avg_times, color='skyblue')\n","ax1.set_xlabel('Model')\n","ax1.set_ylabel('Average Time (seconds)')\n","ax1.set_title('Average Inference Time per Model')\n","ax1.set_xticks(range(len(model_names)))\n","ax1.set_xticklabels(model_names, rotation=45, ha='right')\n","\n","# Add value labels on bars\n","for i, v in enumerate(avg_times):\n","    ax1.text(i, v + 0.1, f'{v:.2f}s', ha='center', va='bottom')\n","\n","# Plot 2: Average score\n","ax2.bar(range(len(model_names)), avg_scores, color='lightcoral')\n","ax2.set_xlabel('Model')\n","ax2.set_ylabel('Average Score (out of 100)')\n","ax2.set_title('Average Score per Model')\n","ax2.set_xticks(range(len(model_names)))\n","ax2.set_xticklabels(model_names, rotation=45, ha='right')\n","ax2.set_ylim([0, 100])\n","\n","# Add value labels on bars\n","for i, v in enumerate(avg_scores):\n","    ax2.text(i, v + 1, f'{v:.2f}', ha='center', va='bottom')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"✅ Visualization complete\")"]},{"cell_type":"markdown","metadata":{"id":"lUViM3AdH7fY"},"source":["## Notes\n","\n","This notebook provides a baseline benchmark for comparing different LLM models.\n","\n","**Key findings:**\n","- Inference time varies by model\n","- Score consistency across models indicates scoring stability\n","- Mock exercises are used for testing; replace with actual model generation for production benchmarks\n","\n","**Configuration:**\n","- Model configurations are defined in `src/rl/reward_function/scorers/base_llm_scorer.py`\n","- All scorers now default to gpt-4.1-mini, with fallback to gpt-4o-mini, then turbo models"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}