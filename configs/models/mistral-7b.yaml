# Mistral 7B Model Configuration
model_name: "mistralai/Mistral-7B-Instruct-v0.3"
model_type: "mistral"

# Generation parameters
max_tokens: 300
temperature: 0.6  # Slightly lower for more focused responses
top_p: 0.85

# Model-specific settings
device: "auto"
quantization: "4bit"
cache_dir: null

# API settings (not needed for local models)
api_key: null
base_url: null

# Italian-specific settings
italian_system_prompt: true
cultural_context: true

# Additional metadata
description: "Mistral 7B model optimized for Italian language learning"
recommended_for:
  - "intermediate_conversation"
  - "grammar_explanations"
  - "structured_learning"
memory_requirements: "~4GB with 4-bit quantization"
performance_notes: "Good multilingual capabilities, efficient for Italian"