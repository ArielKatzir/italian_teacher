# Llama 3.1 3B Model Configuration
model_name: "meta-llama/Llama-3.1-3B-Instruct"
model_type: "llama"

# Generation parameters
max_tokens: 150
temperature: 0.8  # Slightly higher for more creativity with smaller model
top_p: 0.9

# Model-specific settings
device: "auto"
quantization: "4bit"
cache_dir: null

# API settings (not needed for local models)
api_key: null
base_url: null

# Italian-specific settings
italian_system_prompt: true
cultural_context: true

# Additional metadata
description: "Meta's Llama 3.1 3B model for lightweight Italian conversations"
recommended_for:
  - "basic_conversation"
  - "beginner_practice"
  - "local_development"
memory_requirements: "~2GB with 4-bit quantization"
performance_notes: "Faster inference, good for development and basic conversations"