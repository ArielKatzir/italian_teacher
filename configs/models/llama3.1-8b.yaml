# Llama 3.1 8B Model Configuration
model_name: "meta-llama/Llama-3.1-8B-Instruct"
model_type: "llama"

# Generation parameters
max_tokens: 200
temperature: 0.7
top_p: 0.9

# Model-specific settings
device: "auto"
quantization: "4bit"  # For GPU memory efficiency
cache_dir: null

# API settings (not needed for local models)
api_key: null
base_url: null

# Italian-specific settings
italian_system_prompt: true
cultural_context: true

# Additional metadata
description: "Meta's Llama 3.1 8B model optimized for Italian conversations"
recommended_for:
  - "general_conversation"
  - "cultural_discussions"
  - "grammar_correction"
memory_requirements: "~5GB with 4-bit quantization"
performance_notes: "Best balance of quality and speed for Italian learning"