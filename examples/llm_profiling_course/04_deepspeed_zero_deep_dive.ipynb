{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Chapter 4: DeepSpeed ZeRO Deep Dive\n",
    "\n",
    "## 📚 Theoretical Foundations of Parameter Partitioning\n",
    "\n",
    "### Understanding the Memory Challenge\n",
    "\n",
    "Modern Large Language Models face a fundamental memory scaling challenge. A 175B parameter model like GPT-3 requires:\n",
    "- **700GB** in FP32 (4 bytes per parameter)\n",
    "- **350GB** in FP16 (2 bytes per parameter)\n",
    "- **Additional memory** for gradients, optimizer states, and activations\n",
    "\n",
    "Even with the largest GPUs (A100 80GB, H100 80GB), training such models is impossible without sophisticated memory optimization strategies.\n",
    "\n",
    "### DeepSpeed ZeRO Architecture\n",
    "\n",
    "**Zero Redundancy Optimizer (ZeRO)** eliminates memory redundancies in data-parallel training through three progressive stages:\n",
    "\n",
    "#### **ZeRO Stage 1: Optimizer State Partitioning**\n",
    "- Partitions optimizer states (momentum, variance for Adam) across GPUs\n",
    "- **Memory Reduction**: 4x for Adam optimizer\n",
    "- **Communication**: All-gather during parameter updates\n",
    "\n",
    "#### **ZeRO Stage 2: Gradient Partitioning**\n",
    "- Partitions gradients in addition to optimizer states\n",
    "- **Memory Reduction**: 8x total reduction\n",
    "- **Communication**: Reduce-scatter for gradient aggregation\n",
    "\n",
    "#### **ZeRO Stage 3: Parameter Partitioning**\n",
    "- Partitions model parameters, gradients, and optimizer states\n",
    "- **Memory Reduction**: Linear with number of GPUs\n",
    "- **Communication**: All-gather before forward/backward, partition after\n",
    "\n",
    "### Mathematical Analysis of Memory Scaling\n",
    "\n",
    "For a model with **Ψ** parameters and **N** GPUs:\n",
    "\n",
    "**Standard Data Parallel:**\n",
    "```\n",
    "Memory_per_GPU = Ψ × (2 + 2 + 12) = 16Ψ bytes\n",
    "                  ↑   ↑    ↑\n",
    "               params grads optimizer_states\n",
    "```\n",
    "\n",
    "**ZeRO Stage 3:**\n",
    "```\n",
    "Memory_per_GPU = Ψ/N × (2 + 2 + 12) = 16Ψ/N bytes\n",
    "```\n",
    "\n",
    "**Communication Complexity:**\n",
    "- **Forward Pass**: All-gather parameters → O(Ψ)\n",
    "- **Backward Pass**: Reduce-scatter gradients → O(Ψ)\n",
    "- **Total Communication**: 2Ψ per training step\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 Hands-On Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies for DeepSpeed ZeRO implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 DeepSpeed ZeRO Deep Dive Environment Ready!\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 ZeRO Parameter Partitioning Simulator\n",
    "\n",
    "### Core Concept: Distributed Parameter Management\n",
    "\n",
    "ZeRO Stage 3 fundamentally changes how parameters are managed during training:\n",
    "\n",
    "1. **Partitioning**: Each GPU owns a subset of parameters\n",
    "2. **All-Gather**: Before computation, gather required parameters\n",
    "3. **Computation**: Perform forward/backward with full parameters\n",
    "4. **Partition**: Release non-owned parameters, keep gradients for owned subset\n",
    "5. **Optimization**: Update only owned parameters\n",
    "\n",
    "This implementation simulates the core mechanics of ZeRO parameter partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ZeROConfig:\n",
    "    \"\"\"Configuration for ZeRO parameter partitioning simulation.\"\"\"\n",
    "    stage: int = 3  # ZeRO stage (1, 2, or 3)\n",
    "    world_size: int = 4  # Number of simulated GPUs\n",
    "    overlap_comm: bool = True  # Overlap communication with computation\n",
    "    cpu_offload: bool = False  # Offload to CPU memory\n",
    "    nvme_offload: bool = False  # Offload to NVMe storage\n",
    "    gradient_clipping: float = 1.0  # Gradient clipping threshold\n",
    "    \n",
    "class ZeROParameterManager:\n",
    "    \"\"\"Simulates ZeRO parameter partitioning and communication patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ZeROConfig):\n",
    "        self.config = config\n",
    "        self.rank = 0  # Simulated rank (would be from torch.distributed)\n",
    "        self.world_size = config.world_size\n",
    "        \n",
    "        # Memory tracking\n",
    "        self.memory_stats = {\n",
    "            'parameters': defaultdict(float),\n",
    "            'gradients': defaultdict(float),\n",
    "            'optimizer_states': defaultdict(float),\n",
    "            'activations': defaultdict(float),\n",
    "            'communication_buffer': defaultdict(float)\n",
    "        }\n",
    "        \n",
    "        # Communication tracking\n",
    "        self.communication_log = []\n",
    "        \n",
    "    def partition_parameters(self, model_size: int) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate parameter partitioning across GPUs.\"\"\"\n",
    "        \n",
    "        # Calculate partition sizes\n",
    "        base_partition = model_size // self.world_size\n",
    "        remainder = model_size % self.world_size\n",
    "        \n",
    "        partitions = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for rank in range(self.world_size):\n",
    "            # Handle remainder distribution\n",
    "            partition_size = base_partition + (1 if rank < remainder else 0)\n",
    "            end_idx = start_idx + partition_size\n",
    "            \n",
    "            partitions.append({\n",
    "                'rank': rank,\n",
    "                'start': start_idx,\n",
    "                'end': end_idx,\n",
    "                'size': partition_size,\n",
    "                'owned_parameters': partition_size,\n",
    "                'memory_usage': self._calculate_memory_usage(partition_size)\n",
    "            })\n",
    "            \n",
    "            start_idx = end_idx\n",
    "        \n",
    "        return {\n",
    "            'partitions': partitions,\n",
    "            'total_parameters': model_size,\n",
    "            'max_partition_size': max(p['size'] for p in partitions),\n",
    "            'memory_reduction_factor': self.world_size if self.config.stage == 3 else 1\n",
    "        }\n",
    "    \n",
    "    def _calculate_memory_usage(self, param_count: int) -> Dict[str, float]:\n",
    "        \"\"\"Calculate memory usage for different ZeRO stages.\"\"\"\n",
    "        \n",
    "        # Memory per parameter (in bytes)\n",
    "        param_memory = 2  # FP16 parameters\n",
    "        grad_memory = 2   # FP16 gradients\n",
    "        \n",
    "        # Adam optimizer: momentum (FP32) + variance (FP32)\n",
    "        optimizer_memory = 8  # 4 + 4 bytes\n",
    "        \n",
    "        if self.config.stage == 1:\n",
    "            # Only optimizer states are partitioned\n",
    "            return {\n",
    "                'parameters': param_count * param_memory * self.world_size,  # Replicated\n",
    "                'gradients': param_count * grad_memory * self.world_size,    # Replicated\n",
    "                'optimizer_states': param_count * optimizer_memory,          # Partitioned\n",
    "                'total': param_count * (param_memory + grad_memory) * self.world_size + param_count * optimizer_memory\n",
    "            }\n",
    "        elif self.config.stage == 2:\n",
    "            # Optimizer states and gradients are partitioned\n",
    "            return {\n",
    "                'parameters': param_count * param_memory * self.world_size,  # Replicated\n",
    "                'gradients': param_count * grad_memory,                      # Partitioned\n",
    "                'optimizer_states': param_count * optimizer_memory,          # Partitioned\n",
    "                'total': param_count * param_memory * self.world_size + param_count * (grad_memory + optimizer_memory)\n",
    "            }\n",
    "        elif self.config.stage == 3:\n",
    "            # All components are partitioned\n",
    "            return {\n",
    "                'parameters': param_count * param_memory,      # Partitioned\n",
    "                'gradients': param_count * grad_memory,        # Partitioned\n",
    "                'optimizer_states': param_count * optimizer_memory,  # Partitioned\n",
    "                'total': param_count * (param_memory + grad_memory + optimizer_memory)\n",
    "            }\n",
    "    \n",
    "    def simulate_training_step(self, model_size: int, sequence_length: int = 2048) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate a complete training step with ZeRO communication pattern.\"\"\"\n",
    "        \n",
    "        partitioning_info = self.partition_parameters(model_size)\n",
    "        step_stats = {\n",
    "            'communication_volume': 0,\n",
    "            'communication_steps': [],\n",
    "            'memory_peak': 0,\n",
    "            'compute_time': 0,\n",
    "            'communication_time': 0\n",
    "        }\n",
    "        \n",
    "        # Simulate forward pass\n",
    "        if self.config.stage == 3:\n",
    "            # All-gather parameters before forward pass\n",
    "            comm_volume = model_size * 2  # FP16 parameters\n",
    "            step_stats['communication_volume'] += comm_volume\n",
    "            step_stats['communication_steps'].append({\n",
    "                'operation': 'all_gather_parameters',\n",
    "                'volume': comm_volume,\n",
    "                'phase': 'forward'\n",
    "            })\n",
    "        \n",
    "        # Simulate activation memory\n",
    "        activation_memory = sequence_length * model_size * 2 / (1024**3)  # GB\n",
    "        step_stats['memory_peak'] = max(step_stats['memory_peak'], activation_memory)\n",
    "        \n",
    "        # Simulate backward pass\n",
    "        if self.config.stage >= 2:\n",
    "            # Reduce-scatter gradients\n",
    "            comm_volume = model_size * 2  # FP16 gradients\n",
    "            step_stats['communication_volume'] += comm_volume\n",
    "            step_stats['communication_steps'].append({\n",
    "                'operation': 'reduce_scatter_gradients',\n",
    "                'volume': comm_volume,\n",
    "                'phase': 'backward'\n",
    "            })\n",
    "        \n",
    "        # Simulate optimizer step communication\n",
    "        if self.config.stage >= 1:\n",
    "            # Broadcast updated parameters (simplified)\n",
    "            owned_params = partitioning_info['partitions'][self.rank]['size']\n",
    "            comm_volume = owned_params * 2  # FP16 parameters\n",
    "            step_stats['communication_volume'] += comm_volume\n",
    "            step_stats['communication_steps'].append({\n",
    "                'operation': 'broadcast_parameters',\n",
    "                'volume': comm_volume,\n",
    "                'phase': 'optimizer'\n",
    "            })\n",
    "        \n",
    "        # Estimate communication time (simplified model)\n",
    "        # Assumes 100 GB/s interconnect bandwidth\n",
    "        bandwidth_gbps = 100\n",
    "        step_stats['communication_time'] = step_stats['communication_volume'] / (bandwidth_gbps * 1e9)\n",
    "        \n",
    "        # Estimate compute time (very simplified)\n",
    "        # Based on FLOPS for transformer forward/backward pass\n",
    "        flops_per_param = sequence_length * 6  # Approximation for transformer\n",
    "        total_flops = model_size * flops_per_param\n",
    "        \n",
    "        # Assume 150 TFLOPS for T4 (mixed precision)\n",
    "        compute_tflops = 65  # Conservative estimate for T4\n",
    "        step_stats['compute_time'] = total_flops / (compute_tflops * 1e12)\n",
    "        \n",
    "        return {\n",
    "            'partitioning': partitioning_info,\n",
    "            'step_statistics': step_stats,\n",
    "            'efficiency_metrics': self._calculate_efficiency_metrics(step_stats)\n",
    "        }\n",
    "    \n",
    "    def _calculate_efficiency_metrics(self, step_stats: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Calculate training efficiency metrics.\"\"\"\n",
    "        \n",
    "        total_time = step_stats['compute_time'] + step_stats['communication_time']\n",
    "        \n",
    "        return {\n",
    "            'compute_efficiency': step_stats['compute_time'] / total_time if total_time > 0 else 0,\n",
    "            'communication_overhead': step_stats['communication_time'] / total_time if total_time > 0 else 0,\n",
    "            'memory_reduction_factor': self.world_size if self.config.stage == 3 else 1,\n",
    "            'total_step_time': total_time,\n",
    "            'communication_to_compute_ratio': step_stats['communication_time'] / step_stats['compute_time'] if step_stats['compute_time'] > 0 else float('inf')\n",
    "        }\n",
    "\n",
    "# Test the ZeRO parameter manager\n",
    "print(\"🧠 Testing ZeRO Parameter Partitioning Simulator\")\n",
    "\n",
    "# Test different ZeRO stages\n",
    "model_sizes = [1e9, 7e9, 13e9, 30e9, 70e9]  # 1B, 7B, 13B, 30B, 70B parameters\n",
    "zero_stages = [1, 2, 3]\n",
    "world_sizes = [2, 4, 8, 16]\n",
    "\n",
    "print(f\"Testing model sizes: {[f'{size/1e9:.0f}B' for size in model_sizes]}\")\n",
    "print(f\"Testing ZeRO stages: {zero_stages}\")\n",
    "print(f\"Testing world sizes: {world_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Comprehensive ZeRO Performance Analysis\n",
    "\n",
    "### Memory Scaling Analysis\n",
    "\n",
    "This section analyzes how different ZeRO stages affect memory usage and communication overhead across various model sizes and GPU configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_zero_analysis():\n",
    "    \"\"\"Run comprehensive analysis of ZeRO performance across different configurations.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_size in model_sizes:\n",
    "        for stage in zero_stages:\n",
    "            for world_size in world_sizes:\n",
    "                config = ZeROConfig(stage=stage, world_size=world_size)\n",
    "                manager = ZeROParameterManager(config)\n",
    "                \n",
    "                # Run simulation\n",
    "                result = manager.simulate_training_step(int(model_size))\n",
    "                \n",
    "                # Extract key metrics\n",
    "                partition_info = result['partitioning']\n",
    "                step_stats = result['step_statistics']\n",
    "                efficiency = result['efficiency_metrics']\n",
    "                \n",
    "                # Memory per GPU (in GB)\n",
    "                memory_per_gpu = partition_info['partitions'][0]['memory_usage']['total'] / 1e9\n",
    "                \n",
    "                results.append({\n",
    "                    'model_size_b': model_size / 1e9,\n",
    "                    'zero_stage': stage,\n",
    "                    'world_size': world_size,\n",
    "                    'memory_per_gpu_gb': memory_per_gpu,\n",
    "                    'memory_reduction_factor': efficiency['memory_reduction_factor'],\n",
    "                    'communication_volume_gb': step_stats['communication_volume'] / 1e9,\n",
    "                    'compute_time_s': step_stats['compute_time'],\n",
    "                    'communication_time_s': step_stats['communication_time'],\n",
    "                    'total_time_s': efficiency['total_step_time'],\n",
    "                    'compute_efficiency': efficiency['compute_efficiency'],\n",
    "                    'communication_overhead': efficiency['communication_overhead'],\n",
    "                    'comm_compute_ratio': efficiency['communication_to_compute_ratio']\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive analysis\n",
    "print(\"🚀 Running Comprehensive ZeRO Analysis...\")\n",
    "print(\"This may take a moment to simulate all configurations...\")\n",
    "\n",
    "analysis_results = run_comprehensive_zero_analysis()\n",
    "\n",
    "print(f\"✅ Analysis Complete! Generated {len(analysis_results)} data points\")\n",
    "print(\"\\n📊 Sample Results:\")\n",
    "for i, result in enumerate(analysis_results[:5]):\n",
    "    print(f\"{i+1}. {result['model_size_b']:.0f}B params, ZeRO-{result['zero_stage']}, {result['world_size']} GPUs: \"\n",
    "          f\"{result['memory_per_gpu_gb']:.1f}GB/GPU, {result['compute_efficiency']:.1%} compute efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Advanced Visualization and Analysis\n",
    "\n",
    "### Memory Scaling Visualization\n",
    "\n",
    "The following visualizations demonstrate how ZeRO stages affect memory usage, communication patterns, and training efficiency across different model sizes and GPU configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_zero_visualizations(results: List[Dict]):\n",
    "    \"\"\"Create comprehensive visualizations for ZeRO analysis results.\"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('🚀 DeepSpeed ZeRO Comprehensive Performance Analysis', fontsize=16, y=0.98)\n",
    "    \n",
    "    # 1. Memory Usage by ZeRO Stage\n",
    "    ax1 = axes[0, 0]\n",
    "    for stage in zero_stages:\n",
    "        stage_data = df[(df['zero_stage'] == stage) & (df['world_size'] == 8)]\n",
    "        ax1.plot(stage_data['model_size_b'], stage_data['memory_per_gpu_gb'], \n",
    "                marker='o', linewidth=2, label=f'ZeRO-{stage}', markersize=6)\n",
    "    \n",
    "    ax1.set_xlabel('Model Size (Billion Parameters)')\n",
    "    ax1.set_ylabel('Memory per GPU (GB)')\n",
    "    ax1.set_title('Memory Usage vs Model Size\\n(8 GPUs)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Add T4 memory limit line\n",
    "    ax1.axhline(y=16, color='red', linestyle='--', alpha=0.7, label='T4 Memory Limit (16GB)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Memory Scaling with World Size\n",
    "    ax2 = axes[0, 1]\n",
    "    model_size_70b = df[df['model_size_b'] == 70.0]\n",
    "    for stage in zero_stages:\n",
    "        stage_data = model_size_70b[model_size_70b['zero_stage'] == stage]\n",
    "        ax2.plot(stage_data['world_size'], stage_data['memory_per_gpu_gb'], \n",
    "                marker='s', linewidth=2, label=f'ZeRO-{stage}', markersize=6)\n",
    "    \n",
    "    ax2.set_xlabel('Number of GPUs (World Size)')\n",
    "    ax2.set_ylabel('Memory per GPU (GB)')\n",
    "    ax2.set_title('Memory Scaling with GPU Count\\n(70B Parameter Model)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # 3. Communication Volume Analysis\n",
    "    ax3 = axes[0, 2]\n",
    "    for stage in zero_stages:\n",
    "        stage_data = df[(df['zero_stage'] == stage) & (df['world_size'] == 8)]\n",
    "        ax3.plot(stage_data['model_size_b'], stage_data['communication_volume_gb'], \n",
    "                marker='^', linewidth=2, label=f'ZeRO-{stage}', markersize=6)\n",
    "    \n",
    "    ax3.set_xlabel('Model Size (Billion Parameters)')\n",
    "    ax3.set_ylabel('Communication Volume per Step (GB)')\n",
    "    ax3.set_title('Communication Overhead\\n(8 GPUs)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Compute Efficiency Heatmap\n",
    "    ax4 = axes[1, 0]\n",
    "    pivot_efficiency = df[df['zero_stage'] == 3].pivot(index='world_size', \n",
    "                                                      columns='model_size_b', \n",
    "                                                      values='compute_efficiency')\n",
    "    sns.heatmap(pivot_efficiency, annot=True, fmt='.2f', cmap='RdYlBu_r', \n",
    "                ax=ax4, cbar_kws={'label': 'Compute Efficiency'})\n",
    "    ax4.set_title('Compute Efficiency Heatmap\\n(ZeRO-3)')\n",
    "    ax4.set_xlabel('Model Size (Billion Parameters)')\n",
    "    ax4.set_ylabel('Number of GPUs')\n",
    "    \n",
    "    # 5. Communication to Compute Ratio\n",
    "    ax5 = axes[1, 1]\n",
    "    for world_size in [4, 8, 16]:\n",
    "        ws_data = df[(df['zero_stage'] == 3) & (df['world_size'] == world_size)]\n",
    "        ax5.plot(ws_data['model_size_b'], ws_data['comm_compute_ratio'], \n",
    "                marker='d', linewidth=2, label=f'{world_size} GPUs', markersize=6)\n",
    "    \n",
    "    ax5.set_xlabel('Model Size (Billion Parameters)')\n",
    "    ax5.set_ylabel('Communication/Compute Ratio')\n",
    "    ax5.set_title('Communication to Compute Ratio\\n(ZeRO-3)')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.set_yscale('log')\n",
    "    \n",
    "    # 6. Training Efficiency Comparison\n",
    "    ax6 = axes[1, 2]\n",
    "    model_size_30b = df[df['model_size_b'] == 30.0]\n",
    "    \n",
    "    stages_data = []\n",
    "    world_sizes_plot = [4, 8, 16]\n",
    "    \n",
    "    for stage in zero_stages:\n",
    "        for ws in world_sizes_plot:\n",
    "            data_point = model_size_30b[(model_size_30b['zero_stage'] == stage) & \n",
    "                                      (model_size_30b['world_size'] == ws)]\n",
    "            if not data_point.empty:\n",
    "                stages_data.append({\n",
    "                    'stage': f'ZeRO-{stage}',\n",
    "                    'world_size': ws,\n",
    "                    'efficiency': data_point['compute_efficiency'].iloc[0]\n",
    "                })\n",
    "    \n",
    "    stages_df = pd.DataFrame(stages_data)\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    x = np.arange(len(world_sizes_plot))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, stage in enumerate([f'ZeRO-{s}' for s in zero_stages]):\n",
    "        stage_data = stages_df[stages_df['stage'] == stage]\n",
    "        ax6.bar(x + i*width, stage_data['efficiency'], width, \n",
    "               label=stage, alpha=0.8)\n",
    "    \n",
    "    ax6.set_xlabel('Number of GPUs')\n",
    "    ax6.set_ylabel('Compute Efficiency')\n",
    "    ax6.set_title('Training Efficiency Comparison\\n(30B Parameter Model)')\n",
    "    ax6.set_xticks(x + width)\n",
    "    ax6.set_xticklabels(world_sizes_plot)\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "print(\"📊 Creating Comprehensive ZeRO Visualizations...\")\n",
    "fig = create_comprehensive_zero_visualizations(analysis_results)\n",
    "print(\"✅ Visualizations Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Advanced ZeRO Optimization Techniques\n",
    "\n",
    "### Communication Optimization Strategies\n",
    "\n",
    "Beyond basic parameter partitioning, advanced ZeRO implementations employ several optimization techniques:\n",
    "\n",
    "1. **Communication Overlap**: Overlapping communication with computation\n",
    "2. **Hierarchical All-Reduce**: Optimizing communication topology\n",
    "3. **Gradient Compression**: Reducing communication volume\n",
    "4. **CPU/NVMe Offloading**: Managing memory hierarchy\n",
    "\n",
    "### Implementation of Advanced Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedZeROOptimizer:\n",
    "    \"\"\"Advanced ZeRO optimizer with communication optimizations and memory management.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ZeROConfig):\n",
    "        self.config = config\n",
    "        self.world_size = config.world_size\n",
    "        \n",
    "        # Communication optimization parameters\n",
    "        self.bucket_size_mb = 25  # DeepSpeed default bucket size\n",
    "        self.overlap_threshold = 0.1  # Minimum computation time for overlap\n",
    "        \n",
    "        # Memory management\n",
    "        self.cpu_memory_pool = {}  # Simulated CPU memory pool\n",
    "        self.nvme_storage = {}     # Simulated NVMe storage\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.communication_timings = []\n",
    "        self.memory_timeline = []\n",
    "        \n",
    "    def optimize_communication_pattern(self, layer_sizes: List[int]) -> Dict[str, Any]:\n",
    "        \"\"\"Optimize communication pattern using bucketing and overlap strategies.\"\"\"\n",
    "        \n",
    "        # Create communication buckets\n",
    "        buckets = self._create_communication_buckets(layer_sizes)\n",
    "        \n",
    "        # Optimize bucket scheduling\n",
    "        optimized_schedule = self._optimize_bucket_schedule(buckets)\n",
    "        \n",
    "        # Calculate overlap opportunities\n",
    "        overlap_analysis = self._analyze_overlap_opportunities(optimized_schedule)\n",
    "        \n",
    "        return {\n",
    "            'buckets': buckets,\n",
    "            'optimized_schedule': optimized_schedule,\n",
    "            'overlap_analysis': overlap_analysis,\n",
    "            'total_communication_time': sum(bucket['communication_time'] for bucket in buckets),\n",
    "            'overlapped_communication_time': overlap_analysis['overlapped_time'],\n",
    "            'communication_efficiency': overlap_analysis['efficiency']\n",
    "        }\n",
    "    \n",
    "    def _create_communication_buckets(self, layer_sizes: List[int]) -> List[Dict]:\n",
    "        \"\"\"Create communication buckets for gradient synchronization.\"\"\"\n",
    "        \n",
    "        buckets = []\n",
    "        current_bucket = []\n",
    "        current_bucket_size = 0\n",
    "        bucket_size_bytes = self.bucket_size_mb * 1024 * 1024\n",
    "        \n",
    "        for i, layer_size in enumerate(layer_sizes):\n",
    "            layer_size_bytes = layer_size * 2  # FP16 gradients\n",
    "            \n",
    "            if current_bucket_size + layer_size_bytes > bucket_size_bytes and current_bucket:\n",
    "                # Finalize current bucket\n",
    "                buckets.append(self._finalize_bucket(current_bucket, current_bucket_size))\n",
    "                current_bucket = []\n",
    "                current_bucket_size = 0\n",
    "            \n",
    "            current_bucket.append({\n",
    "                'layer_id': i,\n",
    "                'layer_size': layer_size,\n",
    "                'layer_size_bytes': layer_size_bytes\n",
    "            })\n",
    "            current_bucket_size += layer_size_bytes\n",
    "        \n",
    "        # Finalize last bucket\n",
    "        if current_bucket:\n",
    "            buckets.append(self._finalize_bucket(current_bucket, current_bucket_size))\n",
    "        \n",
    "        return buckets\n",
    "    \n",
    "    def _finalize_bucket(self, layers: List[Dict], total_size_bytes: int) -> Dict:\n",
    "        \"\"\"Finalize a communication bucket with timing estimates.\"\"\"\n",
    "        \n",
    "        # Estimate communication time based on bandwidth and latency\n",
    "        bandwidth_gbps = 100  # 100 GB/s interconnect\n",
    "        latency_us = 2  # 2 microsecond latency\n",
    "        \n",
    "        # All-reduce communication volume: 2 * (N-1)/N * data_size\n",
    "        all_reduce_factor = 2 * (self.world_size - 1) / self.world_size\n",
    "        communication_volume = total_size_bytes * all_reduce_factor\n",
    "        \n",
    "        communication_time = (communication_volume / (bandwidth_gbps * 1e9)) + (latency_us * 1e-6)\n",
    "        \n",
    "        return {\n",
    "            'layers': layers,\n",
    "            'total_size_bytes': total_size_bytes,\n",
    "            'communication_volume': communication_volume,\n",
    "            'communication_time': communication_time,\n",
    "            'layer_count': len(layers)\n",
    "        }\n",
    "    \n",
    "    def _optimize_bucket_schedule(self, buckets: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Optimize bucket communication scheduling for maximum overlap.\"\"\"\n",
    "        \n",
    "        # Sort buckets by communication time (largest first for better overlap)\n",
    "        optimized_buckets = sorted(buckets, key=lambda x: x['communication_time'], reverse=True)\n",
    "        \n",
    "        # Add scheduling information\n",
    "        for i, bucket in enumerate(optimized_buckets):\n",
    "            bucket['schedule_order'] = i\n",
    "            bucket['can_overlap'] = bucket['communication_time'] > self.overlap_threshold\n",
    "        \n",
    "        return optimized_buckets\n",
    "    \n",
    "    def _analyze_overlap_opportunities(self, scheduled_buckets: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze opportunities for communication-computation overlap.\"\"\"\n",
    "        \n",
    "        total_communication_time = sum(bucket['communication_time'] for bucket in scheduled_buckets)\n",
    "        overlappable_time = sum(bucket['communication_time'] for bucket in scheduled_buckets \n",
    "                              if bucket['can_overlap'])\n",
    "        \n",
    "        # Estimate overlap efficiency (simplified model)\n",
    "        # Assumes 70% of communication can be overlapped with computation\n",
    "        overlap_efficiency = 0.7\n",
    "        overlapped_time = overlappable_time * overlap_efficiency\n",
    "        \n",
    "        return {\n",
    "            'total_communication_time': total_communication_time,\n",
    "            'overlappable_time': overlappable_time,\n",
    "            'overlapped_time': overlapped_time,\n",
    "            'remaining_communication_time': total_communication_time - overlapped_time,\n",
    "            'efficiency': overlapped_time / total_communication_time if total_communication_time > 0 else 0,\n",
    "            'overlappable_buckets': sum(1 for bucket in scheduled_buckets if bucket['can_overlap']),\n",
    "            'total_buckets': len(scheduled_buckets)\n",
    "        }\n",
    "    \n",
    "    def simulate_memory_offloading(self, model_size: int, available_gpu_memory: float) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate CPU and NVMe offloading strategies.\"\"\"\n",
    "        \n",
    "        # Calculate memory requirements\n",
    "        param_memory = model_size * 2 / 1e9  # FP16 parameters in GB\n",
    "        grad_memory = model_size * 2 / 1e9   # FP16 gradients in GB\n",
    "        optimizer_memory = model_size * 8 / 1e9  # FP32 optimizer states in GB\n",
    "        \n",
    "        total_memory_required = param_memory + grad_memory + optimizer_memory\n",
    "        \n",
    "        # Determine offloading strategy\n",
    "        offloading_strategy = self._determine_offloading_strategy(\n",
    "            total_memory_required, available_gpu_memory\n",
    "        )\n",
    "        \n",
    "        # Calculate transfer times\n",
    "        transfer_analysis = self._analyze_transfer_times(offloading_strategy)\n",
    "        \n",
    "        return {\n",
    "            'memory_breakdown': {\n",
    "                'parameters_gb': param_memory,\n",
    "                'gradients_gb': grad_memory,\n",
    "                'optimizer_states_gb': optimizer_memory,\n",
    "                'total_required_gb': total_memory_required\n",
    "            },\n",
    "            'available_gpu_memory_gb': available_gpu_memory,\n",
    "            'offloading_strategy': offloading_strategy,\n",
    "            'transfer_analysis': transfer_analysis,\n",
    "            'memory_efficiency': offloading_strategy['gpu_utilization']\n",
    "        }\n",
    "    \n",
    "    def _determine_offloading_strategy(self, required_memory: float, available_memory: float) -> Dict:\n",
    "        \"\"\"Determine optimal offloading strategy based on memory constraints.\"\"\"\n",
    "        \n",
    "        if required_memory <= available_memory:\n",
    "            # No offloading needed\n",
    "            return {\n",
    "                'strategy': 'gpu_only',\n",
    "                'parameters_location': 'gpu',\n",
    "                'gradients_location': 'gpu',\n",
    "                'optimizer_location': 'gpu',\n",
    "                'gpu_memory_usage': required_memory,\n",
    "                'cpu_memory_usage': 0,\n",
    "                'nvme_usage': 0,\n",
    "                'gpu_utilization': required_memory / available_memory\n",
    "            }\n",
    "        elif self.config.cpu_offload and required_memory <= available_memory * 2:\n",
    "            # CPU offloading strategy\n",
    "            # Keep parameters and gradients on GPU, optimizer states on CPU\n",
    "            gpu_memory = required_memory * 0.5  # Parameters + gradients\n",
    "            cpu_memory = required_memory * 0.5  # Optimizer states\n",
    "            \n",
    "            return {\n",
    "                'strategy': 'cpu_offload',\n",
    "                'parameters_location': 'gpu',\n",
    "                'gradients_location': 'gpu',\n",
    "                'optimizer_location': 'cpu',\n",
    "                'gpu_memory_usage': gpu_memory,\n",
    "                'cpu_memory_usage': cpu_memory,\n",
    "                'nvme_usage': 0,\n",
    "                'gpu_utilization': gpu_memory / available_memory\n",
    "            }\n",
    "        elif self.config.nvme_offload:\n",
    "            # NVMe offloading strategy\n",
    "            # Keep only active parameters on GPU, everything else on NVMe\n",
    "            gpu_memory = available_memory * 0.8  # Use 80% of GPU memory\n",
    "            nvme_usage = required_memory - gpu_memory\n",
    "            \n",
    "            return {\n",
    "                'strategy': 'nvme_offload',\n",
    "                'parameters_location': 'nvme/gpu',  # Streamed as needed\n",
    "                'gradients_location': 'nvme',\n",
    "                'optimizer_location': 'nvme',\n",
    "                'gpu_memory_usage': gpu_memory,\n",
    "                'cpu_memory_usage': 0,\n",
    "                'nvme_usage': nvme_usage,\n",
    "                'gpu_utilization': 0.8\n",
    "            }\n",
    "        else:\n",
    "            # Model too large for available resources\n",
    "            return {\n",
    "                'strategy': 'insufficient_memory',\n",
    "                'error': 'Model too large for available resources',\n",
    "                'required_memory': required_memory,\n",
    "                'available_memory': available_memory,\n",
    "                'gpu_utilization': 1.0\n",
    "            }\n",
    "    \n",
    "    def _analyze_transfer_times(self, strategy: Dict) -> Dict:\n",
    "        \"\"\"Analyze data transfer times for offloading strategy.\"\"\"\n",
    "        \n",
    "        # Transfer bandwidth estimates (GB/s)\n",
    "        gpu_cpu_bandwidth = 50    # PCIe 4.0 x16\n",
    "        cpu_nvme_bandwidth = 7    # High-end NVMe SSD\n",
    "        \n",
    "        transfer_times = {}\n",
    "        \n",
    "        if strategy['strategy'] == 'cpu_offload':\n",
    "            # Calculate optimizer state transfer time\n",
    "            optimizer_transfer_time = strategy['cpu_memory_usage'] / gpu_cpu_bandwidth\n",
    "            transfer_times = {\n",
    "                'optimizer_cpu_transfer': optimizer_transfer_time,\n",
    "                'total_transfer_time': optimizer_transfer_time,\n",
    "                'transfer_overhead': optimizer_transfer_time * 0.1  # 10% overhead\n",
    "            }\n",
    "        elif strategy['strategy'] == 'nvme_offload':\n",
    "            # Calculate NVMe transfer times\n",
    "            nvme_transfer_time = strategy['nvme_usage'] / cpu_nvme_bandwidth\n",
    "            transfer_times = {\n",
    "                'nvme_transfer': nvme_transfer_time,\n",
    "                'total_transfer_time': nvme_transfer_time,\n",
    "                'transfer_overhead': nvme_transfer_time * 0.2  # 20% overhead\n",
    "            }\n",
    "        else:\n",
    "            transfer_times = {\n",
    "                'total_transfer_time': 0,\n",
    "                'transfer_overhead': 0\n",
    "            }\n",
    "        \n",
    "        return transfer_times\n",
    "\n",
    "# Test advanced ZeRO optimizer\n",
    "print(\"🔧 Testing Advanced ZeRO Optimization Techniques\")\n",
    "\n",
    "# Create advanced optimizer\n",
    "advanced_config = ZeROConfig(stage=3, world_size=8, overlap_comm=True, cpu_offload=True)\n",
    "advanced_optimizer = AdvancedZeROOptimizer(advanced_config)\n",
    "\n",
    "# Simulate a transformer model with realistic layer sizes\n",
    "# Based on 7B parameter model (similar to Llama-7B)\n",
    "layer_sizes = [\n",
    "    # Embedding layer\n",
    "    32000 * 4096,  # vocab_size * hidden_size\n",
    "    \n",
    "    # 32 transformer layers\n",
    "    *([4096 * 4096,    # query projection\n",
    "       4096 * 4096,    # key projection  \n",
    "       4096 * 4096,    # value projection\n",
    "       4096 * 4096,    # output projection\n",
    "       4096 * 11008,   # feed-forward up projection\n",
    "       11008 * 4096,   # feed-forward down projection\n",
    "       4096,           # layer norm 1\n",
    "       4096] * 32),    # layer norm 2\n",
    "    \n",
    "    # Final layer norm and output\n",
    "    4096,               # final layer norm\n",
    "    32000 * 4096        # output projection\n",
    "]\n",
    "\n",
    "print(f\"📊 Analyzing model with {len(layer_sizes)} layers\")\n",
    "print(f\"📊 Total parameters: {sum(layer_sizes) / 1e9:.1f}B\")\n",
    "\n",
    "# Optimize communication pattern\n",
    "comm_optimization = advanced_optimizer.optimize_communication_pattern(layer_sizes)\n",
    "\n",
    "print(f\"\\n🚀 Communication Optimization Results:\")\n",
    "print(f\"  • Created {len(comm_optimization['buckets'])} communication buckets\")\n",
    "print(f\"  • Total communication time: {comm_optimization['total_communication_time']*1000:.1f} ms\")\n",
    "print(f\"  • Overlapped communication time: {comm_optimization['overlapped_communication_time']*1000:.1f} ms\")\n",
    "print(f\"  • Communication efficiency: {comm_optimization['overlap_analysis']['efficiency']:.1%}\")\n",
    "\n",
    "# Analyze memory offloading for T4 GPU (16GB)\n",
    "offloading_analysis = advanced_optimizer.simulate_memory_offloading(\n",
    "    model_size=int(7e9), \n",
    "    available_gpu_memory=16.0\n",
    ")\n",
    "\n",
    "print(f\"\\n💾 Memory Offloading Analysis (T4 16GB):\")\n",
    "print(f\"  • Strategy: {offloading_analysis['offloading_strategy']['strategy']}\")\n",
    "print(f\"  • GPU memory usage: {offloading_analysis['offloading_strategy']['gpu_memory_usage']:.1f} GB\")\n",
    "print(f\"  • CPU memory usage: {offloading_analysis['offloading_strategy']['cpu_memory_usage']:.1f} GB\")\n",
    "print(f\"  • GPU utilization: {offloading_analysis['memory_efficiency']:.1%}\")\n",
    "\n",
    "if 'total_transfer_time' in offloading_analysis['transfer_analysis']:\n",
    "    print(f\"  • Transfer overhead: {offloading_analysis['transfer_analysis']['total_transfer_time']*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Production Implementation Recommendations\n",
    "\n",
    "### Key Insights from Analysis\n",
    "\n",
    "Based on our comprehensive analysis, here are the critical insights for production DeepSpeed ZeRO deployments:\n",
    "\n",
    "#### **Memory Scaling Recommendations:**\n",
    "1. **ZeRO Stage 3** provides linear memory scaling with GPU count\n",
    "2. **T4 GPUs (16GB)** can train up to **7B parameter models** with ZeRO-3 + CPU offloading\n",
    "3. **Communication overhead** becomes significant for models >30B parameters\n",
    "\n",
    "#### **Communication Optimization:**\n",
    "1. **Bucket size of 25MB** provides optimal balance between latency and bandwidth\n",
    "2. **Communication overlap** can improve efficiency by up to 70%\n",
    "3. **Hierarchical communication** patterns reduce bottlenecks in large deployments\n",
    "\n",
    "#### **Hardware-Specific Optimizations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_production_recommendations(analysis_results: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Generate production deployment recommendations based on analysis results.\"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(analysis_results)\n",
    "    \n",
    "    recommendations = {\n",
    "        'hardware_configurations': {},\n",
    "        'model_size_guidelines': {},\n",
    "        'optimization_strategies': {},\n",
    "        'cost_analysis': {}\n",
    "    }\n",
    "    \n",
    "    # Hardware configuration recommendations\n",
    "    gpu_types = {\n",
    "        'T4': {'memory_gb': 16, 'cost_per_hour': 0.35, 'compute_tflops': 65},\n",
    "        'A100': {'memory_gb': 80, 'cost_per_hour': 3.06, 'compute_tflops': 312},\n",
    "        'H100': {'memory_gb': 80, 'cost_per_hour': 4.90, 'compute_tflops': 989}\n",
    "    }\n",
    "    \n",
    "    for gpu_type, gpu_specs in gpu_types.items():\n",
    "        # Find maximum trainable model size for each GPU type\n",
    "        gpu_memory = gpu_specs['memory_gb']\n",
    "        \n",
    "        # Filter results for configurations that fit in GPU memory\n",
    "        feasible_configs = df[\n",
    "            (df['memory_per_gpu_gb'] <= gpu_memory * 0.9) &  # 90% memory utilization\n",
    "            (df['zero_stage'] == 3) &  # ZeRO-3 for maximum efficiency\n",
    "            (df['compute_efficiency'] >= 0.7)  # At least 70% compute efficiency\n",
    "        ]\n",
    "        \n",
    "        if not feasible_configs.empty:\n",
    "            max_model = feasible_configs.groupby('world_size')['model_size_b'].max().to_dict()\n",
    "            \n",
    "            recommendations['hardware_configurations'][gpu_type] = {\n",
    "                'memory_gb': gpu_memory,\n",
    "                'cost_per_hour': gpu_specs['cost_per_hour'],\n",
    "                'max_model_sizes': max_model,\n",
    "                'recommended_world_sizes': list(max_model.keys()),\n",
    "                'cost_efficiency': gpu_specs['compute_tflops'] / gpu_specs['cost_per_hour']\n",
    "            }\n",
    "    \n",
    "    # Model size guidelines\n",
    "    model_sizes = [1, 7, 13, 30, 70]\n",
    "    \n",
    "    for model_size in model_sizes:\n",
    "        model_data = df[df['model_size_b'] == model_size]\n",
    "        \n",
    "        if not model_data.empty:\n",
    "            # Find minimum world size for different memory constraints\n",
    "            memory_constraints = [16, 32, 80]  # T4, A100-40GB, A100-80GB\n",
    "            min_world_sizes = {}\n",
    "            \n",
    "            for mem_constraint in memory_constraints:\n",
    "                feasible = model_data[\n",
    "                    (model_data['memory_per_gpu_gb'] <= mem_constraint * 0.9) &\n",
    "                    (model_data['zero_stage'] == 3)\n",
    "                ]\n",
    "                \n",
    "                if not feasible.empty:\n",
    "                    min_world_sizes[f'{mem_constraint}GB'] = feasible['world_size'].min()\n",
    "            \n",
    "            recommendations['model_size_guidelines'][f'{model_size}B'] = {\n",
    "                'minimum_world_sizes': min_world_sizes,\n",
    "                'recommended_zero_stage': 3,\n",
    "                'estimated_training_time_hours': model_size * 100,  # Rough estimate\n",
    "                'requires_cpu_offloading': model_size > 7\n",
    "            }\n",
    "    \n",
    "    # Optimization strategy recommendations\n",
    "    recommendations['optimization_strategies'] = {\n",
    "        'communication': {\n",
    "            'bucket_size_mb': 25,\n",
    "            'overlap_communication': True,\n",
    "            'gradient_compression': 'fp16',\n",
    "            'hierarchical_allreduce': True\n",
    "        },\n",
    "        'memory': {\n",
    "            'cpu_offloading': 'for_models_over_7B',\n",
    "            'nvme_offloading': 'for_models_over_30B',\n",
    "            'activation_checkpointing': True,\n",
    "            'parameter_offloading': 'stage_3_only'\n",
    "        },\n",
    "        'compute': {\n",
    "            'mixed_precision': 'bf16',\n",
    "            'gradient_accumulation': 'auto_based_on_memory',\n",
    "            'micro_batch_size': 'optimize_for_hardware'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Cost analysis\n",
    "    recommendations['cost_analysis'] = {\n",
    "        'cost_per_billion_parameters': {\n",
    "            'T4_cluster': 0.35 * 8 * 24,    # 8 T4s for 24 hours\n",
    "            'A100_cluster': 3.06 * 4 * 12,  # 4 A100s for 12 hours \n",
    "            'H100_cluster': 4.90 * 2 * 6    # 2 H100s for 6 hours\n",
    "        },\n",
    "        'training_time_estimates': {\n",
    "            '7B_model': {'T4': 168, 'A100': 48, 'H100': 24},    # hours\n",
    "            '30B_model': {'T4': 720, 'A100': 168, 'H100': 72},  # hours\n",
    "            '70B_model': {'A100': 336, 'H100': 120}             # hours\n",
    "        },\n",
    "        'total_cost_estimates': {\n",
    "            '7B_model': {'T4': 1176, 'A100': 588, 'H100': 588},  # USD\n",
    "            '30B_model': {'T4': 5040, 'A100': 2058, 'H100': 1411}, # USD\n",
    "            '70B_model': {'A100': 4120, 'H100': 2352}           # USD\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def print_production_recommendations(recommendations: Dict):\n",
    "    \"\"\"Print formatted production recommendations.\"\"\"\n",
    "    \n",
    "    print(\"\\n🎯 PRODUCTION DEPLOYMENT RECOMMENDATIONS\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Hardware configurations\n",
    "    print(\"\\n📱 HARDWARE CONFIGURATION GUIDELINES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for gpu_type, config in recommendations['hardware_configurations'].items():\n",
    "        print(f\"\\n{gpu_type} GPU ({config['memory_gb']}GB):\")\n",
    "        print(f\"  • Cost: ${config['cost_per_hour']:.2f}/hour\")\n",
    "        print(f\"  • Cost efficiency: {config['cost_efficiency']:.0f} TFLOPS/$\")\n",
    "        print(f\"  • Maximum trainable models:\")\n",
    "        \n",
    "        for world_size, max_model in config['max_model_sizes'].items():\n",
    "            print(f\"    - {world_size} GPUs: {max_model:.0f}B parameters\")\n",
    "    \n",
    "    # Model size guidelines\n",
    "    print(\"\\n🧠 MODEL SIZE DEPLOYMENT GUIDELINES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_size, guidelines in recommendations['model_size_guidelines'].items():\n",
    "        print(f\"\\n{model_size} Parameter Model:\")\n",
    "        print(f\"  • Minimum GPU requirements:\")\n",
    "        \n",
    "        for memory, world_size in guidelines['minimum_world_sizes'].items():\n",
    "            print(f\"    - {memory} GPUs: {world_size} nodes minimum\")\n",
    "        \n",
    "        print(f\"  • Requires CPU offloading: {guidelines['requires_cpu_offloading']}\")\n",
    "        print(f\"  • Estimated training time: {guidelines['estimated_training_time_hours']} hours\")\n",
    "    \n",
    "    # Cost analysis\n",
    "    print(\"\\n💰 COST ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\nTraining Cost Estimates:\")\n",
    "    for model, costs in recommendations['cost_analysis']['total_cost_estimates'].items():\n",
    "        print(f\"\\n{model.replace('_', ' ').title()}:\")\n",
    "        for gpu_type, cost in costs.items():\n",
    "            time_estimate = recommendations['cost_analysis']['training_time_estimates'][model][gpu_type]\n",
    "            print(f\"  • {gpu_type}: ${cost:,} ({time_estimate} hours)\")\n",
    "    \n",
    "    # Optimization strategies\n",
    "    print(\"\\n⚙️ OPTIMIZATION STRATEGY RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    strategies = recommendations['optimization_strategies']\n",
    "    \n",
    "    print(\"\\nCommunication Optimizations:\")\n",
    "    for key, value in strategies['communication'].items():\n",
    "        print(f\"  • {key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    print(\"\\nMemory Optimizations:\")\n",
    "    for key, value in strategies['memory'].items():\n",
    "        print(f\"  • {key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    print(\"\\nCompute Optimizations:\")\n",
    "    for key, value in strategies['compute'].items():\n",
    "        print(f\"  • {key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🚀 Ready for production DeepSpeed ZeRO deployment!\")\n",
    "\n",
    "# Generate and display production recommendations\n",
    "print(\"🎯 Generating Production Deployment Recommendations...\")\n",
    "production_recommendations = generate_production_recommendations(analysis_results)\n",
    "print_production_recommendations(production_recommendations)\n",
    "\n",
    "print(\"\\n✅ Chapter 4: DeepSpeed ZeRO Deep Dive Complete!\")\n",
    "print(\"\\n📚 Key Learning Outcomes:\")\n",
    "print(\"  • Deep understanding of ZeRO parameter partitioning\")\n",
    "print(\"  • Advanced communication optimization techniques\")\n",
    "print(\"  • Memory offloading strategies for different hardware\")\n",
    "print(\"  • Production deployment guidelines and cost analysis\")\n",
    "print(\"  • Hands-on experience with optimization implementations\")\n",
    "\n",
    "print(\"\\n🎓 Next Chapter: Mixed Precision Training Mastery\")\n",
    "print(\"Continue to Chapter 5 to dive deep into FP16/BF16/FP8 optimization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}