{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Advanced Inference Optimization\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will:\n",
    "- **Master vLLM architecture** and PagedAttention algorithm\n",
    "- **Understand continuous batching** vs static batching tradeoffs\n",
    "- **Implement KV cache optimization** strategies\n",
    "- **Build production inference systems** with advanced optimizations\n",
    "- **Analyze inference bottlenecks** and optimization opportunities\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 The Inference Challenge in Production LLMs\n",
    "\n",
    "### **Why Inference Optimization Matters**\n",
    "\n",
    "LLM inference in production faces unique challenges:\n",
    "\n",
    "#### **Scale Requirements**\n",
    "- **ChatGPT**: Serves 100M+ requests daily\n",
    "- **Claude**: Handles millions of conversations\n",
    "- **GitHub Copilot**: Processes billions of code completions\n",
    "\n",
    "#### **Performance Demands**\n",
    "- **Latency**: < 100ms time-to-first-token for interactive use\n",
    "- **Throughput**: 1000+ requests/second per GPU\n",
    "- **Cost**: $0.001 per 1K tokens or lower\n",
    "- **Availability**: 99.9% uptime requirements\n",
    "\n",
    "#### **Technical Constraints**\n",
    "- **Memory bound**: KV cache grows with sequence length\n",
    "- **Variable length**: Requests have diverse output lengths\n",
    "- **Batching complexity**: How to group requests efficiently?\n",
    "- **Hardware utilization**: Keep expensive GPUs busy\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Traditional Inference Limitations\n",
    "\n",
    "### **Static Batching Problems**\n",
    "\n",
    "Traditional inference uses **static batching** with severe limitations:\n",
    "\n",
    "#### **The Batch Straggler Problem**\n",
    "```\n",
    "Request A: \"Hello\" → \"Hello world!\" (3 tokens, 50ms)\n",
    "Request B: \"Explain\" → \"Explain quantum physics...\" (200 tokens, 2000ms)\n",
    "Request C: \"Hi\" → \"Hi there!\" (2 tokens, 30ms)\n",
    "\n",
    "Static Batch Processing:\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│ All requests wait for slowest (Request B: 2000ms)  │\n",
    "│ GPU utilization drops as requests complete early   │\n",
    "│ Memory allocated for max length across entire batch│\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "#### **Memory Waste in Static Batching**\n",
    "```python\n",
    "# Static batching allocates for worst case\n",
    "batch_size = 8\n",
    "max_seq_len = 2048  # Must handle longest possible sequence\n",
    "hidden_dim = 4096\n",
    "\n",
    "# KV cache allocation (even for short sequences!)\n",
    "kv_cache_memory = batch_size * max_seq_len * hidden_dim * 2 * num_layers * 2  # K + V\n",
    "# = 8 * 2048 * 4096 * 2 * 32 * 2 = 8.6 GB per batch!\n",
    "```\n",
    "\n",
    "#### **GPU Underutilization**\n",
    "- **Padding overhead**: Short sequences padded to max length\n",
    "- **Idle compute**: GPU cores sit idle waiting for slowest sequence\n",
    "- **Memory fragmentation**: Allocated but unused memory\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 vLLM: The Inference Revolution\n",
    "\n",
    "### **Core Innovations**\n",
    "\n",
    "**vLLM** (\"Very Large Language Model\" inference) introduces three breakthrough concepts:\n",
    "\n",
    "#### **1. Continuous Batching**\n",
    "- **Dynamic requests**: Add/remove requests during generation\n",
    "- **No batch stragglers**: Completed requests immediately replaced\n",
    "- **High utilization**: GPU stays busy throughout generation\n",
    "\n",
    "#### **2. PagedAttention**\n",
    "- **Virtual memory**: Borrows concepts from OS memory management\n",
    "- **Block allocation**: KV cache stored in fixed-size blocks\n",
    "- **Zero fragmentation**: No memory waste from padding\n",
    "\n",
    "#### **3. Optimized CUDA Kernels**\n",
    "- **Fused operations**: Combine multiple GPU operations\n",
    "- **Memory coalescing**: Optimize memory access patterns\n",
    "- **Kernel specialization**: Different kernels for different scenarios\n",
    "\n",
    "### **Performance Impact**\n",
    "- **2-24x higher throughput** than static batching\n",
    "- **55% lower latency** for interactive workloads\n",
    "- **90% GPU utilization** vs 30-40% with static batching\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 PagedAttention Deep Dive\n",
    "\n",
    "### **The Memory Management Revolution**\n",
    "\n",
    "PagedAttention treats **attention computation like virtual memory**:\n",
    "\n",
    "#### **Traditional Attention Memory Layout**\n",
    "```\n",
    "Sequence 1: [████████████████████████████████] (allocated for max)\n",
    "Sequence 2: [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] (partially used)\n",
    "Sequence 3: [███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] (mostly wasted)\n",
    "\n",
    "Problem: Fixed allocation leads to fragmentation\n",
    "```\n",
    "\n",
    "#### **PagedAttention Memory Layout**\n",
    "```\n",
    "Physical Memory Blocks:\n",
    "Block 0: [████] Block 1: [████] Block 2: [████] Block 3: [████]\n",
    "\n",
    "Sequence 1 mapping: Block 0 → Block 1 → Block 2\n",
    "Sequence 2 mapping: Block 3 → Block 7\n",
    "Sequence 3 mapping: Block 4\n",
    "\n",
    "Benefit: Memory allocated exactly as needed\n",
    "```\n",
    "\n",
    "### **Block Management Algorithm**\n",
    "\n",
    "```python\n",
    "class PagedAttention:\n",
    "    def __init__(self, block_size=16):\n",
    "        self.block_size = block_size  # tokens per block\n",
    "        self.physical_blocks = []     # actual memory blocks\n",
    "        self.free_blocks = set()      # available blocks\n",
    "        self.sequence_tables = {}     # virtual → physical mapping\n",
    "    \n",
    "    def allocate_sequence(self, seq_id, initial_length):\n",
    "        blocks_needed = (initial_length + self.block_size - 1) // self.block_size\n",
    "        allocated_blocks = []\n",
    "        \n",
    "        for _ in range(blocks_needed):\n",
    "            if not self.free_blocks:\n",
    "                return None  # Out of memory\n",
    "            \n",
    "            block_id = self.free_blocks.pop()\n",
    "            allocated_blocks.append(block_id)\n",
    "        \n",
    "        self.sequence_tables[seq_id] = allocated_blocks\n",
    "        return allocated_blocks\n",
    "    \n",
    "    def extend_sequence(self, seq_id, new_tokens):\n",
    "        # Allocate additional blocks as sequence grows\n",
    "        current_blocks = self.sequence_tables[seq_id]\n",
    "        # ... implementation details\n",
    "```\n",
    "\n",
    "Let's implement a comprehensive vLLM-style inference system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import queue\n",
    "import uuid\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class InferenceRequest:\n",
    "    \"\"\"Represents a single inference request with comprehensive metadata\"\"\"\n",
    "    request_id: str\n",
    "    prompt: str\n",
    "    max_tokens: int\n",
    "    temperature: float = 1.0\n",
    "    \n",
    "    # Timestamps\n",
    "    arrival_time: float = field(default_factory=time.time)\n",
    "    start_time: Optional[float] = None\n",
    "    first_token_time: Optional[float] = None\n",
    "    completion_time: Optional[float] = None\n",
    "    \n",
    "    # Generation state\n",
    "    generated_tokens: List[int] = field(default_factory=list)\n",
    "    is_complete: bool = False\n",
    "    \n",
    "    # Performance metrics\n",
    "    tokens_generated: int = 0\n",
    "    \n",
    "    def get_latency(self) -> Optional[float]:\n",
    "        \"\"\"End-to-end latency\"\"\"\n",
    "        if self.completion_time and self.arrival_time:\n",
    "            return self.completion_time - self.arrival_time\n",
    "        return None\n",
    "    \n",
    "    def get_time_to_first_token(self) -> Optional[float]:\n",
    "        \"\"\"Time to first token (TTFT)\"\"\"\n",
    "        if self.first_token_time and self.arrival_time:\n",
    "            return self.first_token_time - self.arrival_time\n",
    "        return None\n",
    "    \n",
    "    def get_inter_token_latency(self) -> Optional[float]:\n",
    "        \"\"\"Average time between tokens\"\"\"\n",
    "        if self.completion_time and self.first_token_time and self.tokens_generated > 1:\n",
    "            return (self.completion_time - self.first_token_time) / (self.tokens_generated - 1)\n",
    "        return None\n",
    "\n",
    "class KVCacheBlock:\n",
    "    \"\"\"Fixed-size block for storing Key-Value cache\"\"\"\n",
    "    \n",
    "    def __init__(self, block_id: int, block_size: int = 16):\n",
    "        self.block_id = block_id\n",
    "        self.block_size = block_size  # number of tokens\n",
    "        self.allocated_tokens = 0\n",
    "        self.sequence_id: Optional[str] = None\n",
    "        self.is_free = True\n",
    "        \n",
    "        # Simulated KV data (in practice, this would be actual tensors)\n",
    "        self.k_cache = None  # [block_size, num_heads, head_dim]\n",
    "        self.v_cache = None  # [block_size, num_heads, head_dim]\n",
    "    \n",
    "    def allocate(self, sequence_id: str, num_tokens: int) -> bool:\n",
    "        \"\"\"Allocate tokens in this block\"\"\"\n",
    "        if self.allocated_tokens + num_tokens <= self.block_size:\n",
    "            self.allocated_tokens += num_tokens\n",
    "            self.sequence_id = sequence_id\n",
    "            self.is_free = False\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def deallocate(self):\n",
    "        \"\"\"Free this block\"\"\"\n",
    "        self.allocated_tokens = 0\n",
    "        self.sequence_id = None\n",
    "        self.is_free = True\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "    \n",
    "    def get_utilization(self) -> float:\n",
    "        \"\"\"Get block utilization percentage\"\"\"\n",
    "        return self.allocated_tokens / self.block_size\n",
    "\n",
    "class PagedAttentionManager:\n",
    "    \"\"\"\n",
    "    Implementation of PagedAttention memory management\n",
    "    \n",
    "    Educational Focus:\n",
    "    This class demonstrates the core innovation of vLLM:\n",
    "    treating attention computation like virtual memory management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, total_blocks: int = 1000, block_size: int = 16):\n",
    "        self.total_blocks = total_blocks\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Initialize physical blocks\n",
    "        self.physical_blocks = [\n",
    "            KVCacheBlock(block_id, block_size) \n",
    "            for block_id in range(total_blocks)\n",
    "        ]\n",
    "        \n",
    "        # Free block management\n",
    "        self.free_blocks = deque(range(total_blocks))\n",
    "        \n",
    "        # Sequence to blocks mapping (page table)\n",
    "        self.sequence_page_tables: Dict[str, List[int]] = {}\n",
    "        \n",
    "        # Thread safety\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def allocate_sequence(self, sequence_id: str, initial_tokens: int) -> bool:\n",
    "        \"\"\"Allocate blocks for a new sequence\"\"\"\n",
    "        with self.lock:\n",
    "            blocks_needed = (initial_tokens + self.block_size - 1) // self.block_size\n",
    "            \n",
    "            if len(self.free_blocks) < blocks_needed:\n",
    "                return False  # Out of memory\n",
    "            \n",
    "            # Allocate blocks\n",
    "            allocated_blocks = []\n",
    "            remaining_tokens = initial_tokens\n",
    "            \n",
    "            for _ in range(blocks_needed):\n",
    "                block_id = self.free_blocks.popleft()\n",
    "                block = self.physical_blocks[block_id]\n",
    "                \n",
    "                tokens_to_allocate = min(self.block_size, remaining_tokens)\n",
    "                block.allocate(sequence_id, tokens_to_allocate)\n",
    "                \n",
    "                allocated_blocks.append(block_id)\n",
    "                remaining_tokens -= tokens_to_allocate\n",
    "            \n",
    "            self.sequence_page_tables[sequence_id] = allocated_blocks\n",
    "            return True\n",
    "    \n",
    "    def extend_sequence(self, sequence_id: str, additional_tokens: int) -> bool:\n",
    "        \"\"\"Extend sequence with additional tokens (during generation)\"\"\"\n",
    "        with self.lock:\n",
    "            if sequence_id not in self.sequence_page_tables:\n",
    "                return False\n",
    "            \n",
    "            current_blocks = self.sequence_page_tables[sequence_id]\n",
    "            remaining_tokens = additional_tokens\n",
    "            \n",
    "            # Try to fill existing blocks first\n",
    "            for block_id in current_blocks:\n",
    "                block = self.physical_blocks[block_id]\n",
    "                available_space = block.block_size - block.allocated_tokens\n",
    "                \n",
    "                if available_space > 0:\n",
    "                    tokens_to_add = min(available_space, remaining_tokens)\n",
    "                    block.allocated_tokens += tokens_to_add\n",
    "                    remaining_tokens -= tokens_to_add\n",
    "                    \n",
    "                    if remaining_tokens == 0:\n",
    "                        return True\n",
    "            \n",
    "            # Need new blocks\n",
    "            blocks_needed = (remaining_tokens + self.block_size - 1) // self.block_size\n",
    "            \n",
    "            if len(self.free_blocks) < blocks_needed:\n",
    "                return False  # Out of memory\n",
    "            \n",
    "            # Allocate new blocks\n",
    "            for _ in range(blocks_needed):\n",
    "                block_id = self.free_blocks.popleft()\n",
    "                block = self.physical_blocks[block_id]\n",
    "                \n",
    "                tokens_to_allocate = min(self.block_size, remaining_tokens)\n",
    "                block.allocate(sequence_id, tokens_to_allocate)\n",
    "                \n",
    "                current_blocks.append(block_id)\n",
    "                remaining_tokens -= tokens_to_allocate\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    def deallocate_sequence(self, sequence_id: str):\n",
    "        \"\"\"Free all blocks for a completed sequence\"\"\"\n",
    "        with self.lock:\n",
    "            if sequence_id in self.sequence_page_tables:\n",
    "                block_ids = self.sequence_page_tables[sequence_id]\n",
    "                \n",
    "                for block_id in block_ids:\n",
    "                    self.physical_blocks[block_id].deallocate()\n",
    "                    self.free_blocks.append(block_id)\n",
    "                \n",
    "                del self.sequence_page_tables[sequence_id]\n",
    "    \n",
    "    def get_memory_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive memory utilization statistics\"\"\"\n",
    "        with self.lock:\n",
    "            free_blocks = len(self.free_blocks)\n",
    "            used_blocks = self.total_blocks - free_blocks\n",
    "            \n",
    "            # Calculate fragmentation\n",
    "            total_allocated_tokens = 0\n",
    "            total_capacity_tokens = 0\n",
    "            partially_filled_blocks = 0\n",
    "            \n",
    "            for block in self.physical_blocks:\n",
    "                if not block.is_free:\n",
    "                    total_allocated_tokens += block.allocated_tokens\n",
    "                    total_capacity_tokens += block.block_size\n",
    "                    \n",
    "                    if block.allocated_tokens < block.block_size:\n",
    "                        partially_filled_blocks += 1\n",
    "            \n",
    "            fragmentation_ratio = 0\n",
    "            if total_capacity_tokens > 0:\n",
    "                wasted_tokens = total_capacity_tokens - total_allocated_tokens\n",
    "                fragmentation_ratio = wasted_tokens / total_capacity_tokens\n",
    "            \n",
    "            return {\n",
    "                'total_blocks': self.total_blocks,\n",
    "                'free_blocks': free_blocks,\n",
    "                'used_blocks': used_blocks,\n",
    "                'memory_utilization': used_blocks / self.total_blocks,\n",
    "                'fragmentation_ratio': fragmentation_ratio,\n",
    "                'partially_filled_blocks': partially_filled_blocks,\n",
    "                'active_sequences': len(self.sequence_page_tables),\n",
    "                'avg_blocks_per_sequence': used_blocks / len(self.sequence_page_tables) if self.sequence_page_tables else 0\n",
    "            }\n",
    "    \n",
    "    def print_memory_status(self):\n",
    "        \"\"\"Print detailed memory status\"\"\"\n",
    "        stats = self.get_memory_stats()\n",
    "        \n",
    "        print(f\"\\n💾 PagedAttention Memory Status:\")\n",
    "        print(f\"   Total blocks: {stats['total_blocks']}\")\n",
    "        print(f\"   Used blocks: {stats['used_blocks']} ({stats['memory_utilization']*100:.1f}%)\")\n",
    "        print(f\"   Free blocks: {stats['free_blocks']}\")\n",
    "        print(f\"   Active sequences: {stats['active_sequences']}\")\n",
    "        print(f\"   Fragmentation: {stats['fragmentation_ratio']*100:.1f}%\")\n",
    "        print(f\"   Avg blocks/sequence: {stats['avg_blocks_per_sequence']:.1f}\")\n",
    "\n",
    "print(\"✅ PagedAttention Implementation Complete!\")\n",
    "print(\"🧠 Ready to build advanced inference systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Continuous Batching Engine Implementation\n",
    "\n",
    "Now let's implement the complete continuous batching system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousBatchingEngine:\n",
    "    \"\"\"\n",
    "    Advanced continuous batching engine implementing vLLM-style optimizations\n",
    "    \n",
    "    Educational Focus:\n",
    "    This implementation demonstrates the key algorithms behind modern\n",
    "    high-throughput LLM inference systems used in production.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_batch_size: int = 32,\n",
    "                 max_sequence_length: int = 2048,\n",
    "                 block_size: int = 16,\n",
    "                 total_blocks: int = 1000):\n",
    "        \n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        \n",
    "        # Memory management\n",
    "        self.memory_manager = PagedAttentionManager(total_blocks, block_size)\n",
    "        \n",
    "        # Request management\n",
    "        self.request_queue = queue.Queue()\n",
    "        self.active_requests: Dict[str, InferenceRequest] = {}\n",
    "        self.completed_requests: List[InferenceRequest] = []\n",
    "        \n",
    "        # Processing control\n",
    "        self.is_running = False\n",
    "        self.processing_thread: Optional[threading.Thread] = None\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.stats = {\n",
    "            'total_requests': 0,\n",
    "            'completed_requests': 0,\n",
    "            'total_tokens_generated': 0,\n",
    "            'processing_steps': 0,\n",
    "            'avg_batch_size': 0,\n",
    "            'throughput_history': [],\n",
    "            'latency_history': [],\n",
    "            'gpu_utilization_history': []\n",
    "        }\n",
    "        \n",
    "        # Thread safety\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def add_request(self, request: InferenceRequest) -> bool:\n",
    "        \"\"\"Add a new inference request to the queue\"\"\"\n",
    "        try:\n",
    "            self.request_queue.put_nowait(request)\n",
    "            with self.lock:\n",
    "                self.stats['total_requests'] += 1\n",
    "            return True\n",
    "        except queue.Full:\n",
    "            return False\n",
    "    \n",
    "    def start_processing(self):\n",
    "        \"\"\"Start the continuous processing loop\"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "        \n",
    "        self.is_running = True\n",
    "        self.processing_thread = threading.Thread(target=self._processing_loop, daemon=True)\n",
    "        self.processing_thread.start()\n",
    "        \n",
    "        print(f\"🚀 Continuous batching engine started\")\n",
    "        print(f\"   Max batch size: {self.max_batch_size}\")\n",
    "        print(f\"   Total memory blocks: {self.memory_manager.total_blocks}\")\n",
    "    \n",
    "    def stop_processing(self):\n",
    "        \"\"\"Stop the processing loop\"\"\"\n",
    "        self.is_running = False\n",
    "        if self.processing_thread:\n",
    "            self.processing_thread.join(timeout=5.0)\n",
    "        print(\"⏹️ Continuous batching engine stopped\")\n",
    "    \n",
    "    def _processing_loop(self):\n",
    "        \"\"\"Main continuous processing loop\"\"\"\n",
    "        \n",
    "        print(\"🔄 Starting continuous processing loop...\")\n",
    "        \n",
    "        while self.is_running:\n",
    "            step_start_time = time.time()\n",
    "            \n",
    "            # 1. Add new requests to active batch\n",
    "            self._admit_new_requests()\n",
    "            \n",
    "            # 2. Process current batch (if any active requests)\n",
    "            if self.active_requests:\n",
    "                self._process_active_batch()\n",
    "            \n",
    "            # 3. Remove completed requests\n",
    "            self._remove_completed_requests()\n",
    "            \n",
    "            # 4. Update statistics\n",
    "            self._update_statistics(step_start_time)\n",
    "            \n",
    "            # Small delay to prevent busy waiting\n",
    "            time.sleep(0.001)  # 1ms\n",
    "        \n",
    "        print(\"🏁 Processing loop completed\")\n",
    "    \n",
    "    def _admit_new_requests(self):\n",
    "        \"\"\"Admit new requests up to batch capacity\"\"\"\n",
    "        \n",
    "        while (len(self.active_requests) < self.max_batch_size and \n",
    "               not self.request_queue.empty()):\n",
    "            \n",
    "            try:\n",
    "                request = self.request_queue.get_nowait()\n",
    "            except queue.Empty:\n",
    "                break\n",
    "            \n",
    "            # Estimate initial tokens (prompt processing)\n",
    "            initial_tokens = len(request.prompt.split())  # Simplified tokenization\n",
    "            \n",
    "            # Try to allocate memory\n",
    "            if self.memory_manager.allocate_sequence(request.request_id, \n",
    "                                                   initial_tokens + request.max_tokens):\n",
    "                # Successfully allocated, add to active batch\n",
    "                request.start_time = time.time()\n",
    "                self.active_requests[request.request_id] = request\n",
    "            else:\n",
    "                # Out of memory, put back in queue\n",
    "                self.request_queue.put(request)\n",
    "                break  # No point trying more requests\n",
    "    \n",
    "    def _process_active_batch(self):\n",
    "        \"\"\"Process one step of the active batch\"\"\"\n",
    "        \n",
    "        # Simulate model forward pass for active requests\n",
    "        # In practice, this would be actual transformer computation\n",
    "        \n",
    "        processing_start = time.time()\n",
    "        \n",
    "        # Simulate different processing times based on batch size\n",
    "        batch_size = len(self.active_requests)\n",
    "        base_time = 0.005  # 5ms base time\n",
    "        batch_overhead = batch_size * 0.0005  # 0.5ms per request\n",
    "        processing_time = base_time + batch_overhead\n",
    "        \n",
    "        time.sleep(processing_time)\n",
    "        \n",
    "        # Update each request in the batch\n",
    "        for request in list(self.active_requests.values()):\n",
    "            # Generate one token (simplified)\n",
    "            new_token = random.randint(0, 1000)  # Dummy token\n",
    "            request.generated_tokens.append(new_token)\n",
    "            request.tokens_generated += 1\n",
    "            \n",
    "            # Record first token time\n",
    "            if request.tokens_generated == 1 and request.first_token_time is None:\n",
    "                request.first_token_time = time.time()\n",
    "            \n",
    "            # Extend KV cache for this token\n",
    "            self.memory_manager.extend_sequence(request.request_id, 1)\n",
    "            \n",
    "            # Check completion conditions\n",
    "            if (request.tokens_generated >= request.max_tokens or\n",
    "                random.random() < 0.02):  # 2% chance to end naturally\n",
    "                \n",
    "                request.is_complete = True\n",
    "                request.completion_time = time.time()\n",
    "    \n",
    "    def _remove_completed_requests(self):\n",
    "        \"\"\"Remove completed requests and free their memory\"\"\"\n",
    "        \n",
    "        completed_ids = []\n",
    "        \n",
    "        for request_id, request in self.active_requests.items():\n",
    "            if request.is_complete:\n",
    "                completed_ids.append(request_id)\n",
    "                self.completed_requests.append(request)\n",
    "                \n",
    "                # Free memory\n",
    "                self.memory_manager.deallocate_sequence(request_id)\n",
    "                \n",
    "                # Update stats\n",
    "                with self.lock:\n",
    "                    self.stats['completed_requests'] += 1\n",
    "                    self.stats['total_tokens_generated'] += request.tokens_generated\n",
    "        \n",
    "        # Remove from active requests\n",
    "        for request_id in completed_ids:\n",
    "            del self.active_requests[request_id]\n",
    "    \n",
    "    def _update_statistics(self, step_start_time: float):\n",
    "        \"\"\"Update performance statistics\"\"\"\n",
    "        \n",
    "        step_duration = time.time() - step_start_time\n",
    "        \n",
    "        with self.lock:\n",
    "            self.stats['processing_steps'] += 1\n",
    "            \n",
    "            # Track batch size over time\n",
    "            current_batch_size = len(self.active_requests)\n",
    "            total_batches = self.stats['processing_steps']\n",
    "            self.stats['avg_batch_size'] = ((self.stats['avg_batch_size'] * (total_batches - 1) + \n",
    "                                           current_batch_size) / total_batches)\n",
    "            \n",
    "            # Calculate current throughput (tokens/second)\n",
    "            if step_duration > 0:\n",
    "                tokens_this_step = current_batch_size  # One token per request per step\n",
    "                throughput = tokens_this_step / step_duration\n",
    "                self.stats['throughput_history'].append(throughput)\n",
    "                \n",
    "                # Keep history manageable\n",
    "                if len(self.stats['throughput_history']) > 1000:\n",
    "                    self.stats['throughput_history'] = self.stats['throughput_history'][-500:]\n",
    "            \n",
    "            # GPU utilization estimate (simplified)\n",
    "            gpu_util = min(100, current_batch_size / self.max_batch_size * 100)\n",
    "            self.stats['gpu_utilization_history'].append(gpu_util)\n",
    "            \n",
    "            if len(self.stats['gpu_utilization_history']) > 1000:\n",
    "                self.stats['gpu_utilization_history'] = self.stats['gpu_utilization_history'][-500:]\n",
    "    \n",
    "    def get_performance_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive performance statistics\"\"\"\n",
    "        \n",
    "        with self.lock:\n",
    "            stats = self.stats.copy()\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        if self.completed_requests:\n",
    "            latencies = [r.get_latency() for r in self.completed_requests if r.get_latency()]\n",
    "            ttfts = [r.get_time_to_first_token() for r in self.completed_requests if r.get_time_to_first_token()]\n",
    "            itls = [r.get_inter_token_latency() for r in self.completed_requests if r.get_inter_token_latency()]\n",
    "            \n",
    "            if latencies:\n",
    "                stats['avg_latency_ms'] = np.mean(latencies) * 1000\n",
    "                stats['p95_latency_ms'] = np.percentile(latencies, 95) * 1000\n",
    "                stats['p99_latency_ms'] = np.percentile(latencies, 99) * 1000\n",
    "            \n",
    "            if ttfts:\n",
    "                stats['avg_ttft_ms'] = np.mean(ttfts) * 1000\n",
    "                stats['p95_ttft_ms'] = np.percentile(ttfts, 95) * 1000\n",
    "            \n",
    "            if itls:\n",
    "                stats['avg_inter_token_latency_ms'] = np.mean(itls) * 1000\n",
    "        \n",
    "        # Current throughput\n",
    "        if stats['throughput_history']:\n",
    "            recent_throughput = stats['throughput_history'][-100:]  # Last 100 steps\n",
    "            stats['current_throughput_tps'] = np.mean(recent_throughput)\n",
    "        \n",
    "        # GPU utilization\n",
    "        if stats['gpu_utilization_history']:\n",
    "            recent_gpu_util = stats['gpu_utilization_history'][-100:]\n",
    "            stats['current_gpu_utilization'] = np.mean(recent_gpu_util)\n",
    "        \n",
    "        # Memory stats\n",
    "        stats['memory_stats'] = self.memory_manager.get_memory_stats()\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_status(self):\n",
    "        \"\"\"Print current engine status\"\"\"\n",
    "        \n",
    "        stats = self.get_performance_stats()\n",
    "        \n",
    "        print(f\"\\n🔄 Continuous Batching Engine Status:\")\n",
    "        print(f\"   Active requests: {len(self.active_requests)}\")\n",
    "        print(f\"   Queue depth: {self.request_queue.qsize()}\")\n",
    "        print(f\"   Completed requests: {stats['completed_requests']}\")\n",
    "        print(f\"   Average batch size: {stats['avg_batch_size']:.1f}\")\n",
    "        \n",
    "        if 'current_throughput_tps' in stats:\n",
    "            print(f\"   Current throughput: {stats['current_throughput_tps']:.1f} tokens/sec\")\n",
    "        \n",
    "        if 'current_gpu_utilization' in stats:\n",
    "            print(f\"   GPU utilization: {stats['current_gpu_utilization']:.1f}%\")\n",
    "        \n",
    "        if 'avg_latency_ms' in stats:\n",
    "            print(f\"   Avg latency: {stats['avg_latency_ms']:.1f} ms\")\n",
    "            print(f\"   P95 latency: {stats['p95_latency_ms']:.1f} ms\")\n",
    "        \n",
    "        if 'avg_ttft_ms' in stats:\n",
    "            print(f\"   Avg TTFT: {stats['avg_ttft_ms']:.1f} ms\")\n",
    "        \n",
    "        # Memory status\n",
    "        mem_stats = stats['memory_stats']\n",
    "        print(f\"   Memory usage: {mem_stats['memory_utilization']*100:.1f}% \"\n",
    "              f\"({mem_stats['used_blocks']}/{mem_stats['total_blocks']} blocks)\")\n",
    "\n",
    "print(\"✅ Continuous Batching Engine Implementation Complete!\")\n",
    "print(\"🚀 Ready for high-performance inference testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Comprehensive Inference Benchmark\n",
    "\n",
    "Let's run a comprehensive experiment comparing static vs continuous batching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_comparison_experiment():\n",
    "    \"\"\"\n",
    "    Comprehensive experiment comparing different inference strategies\n",
    "    \n",
    "    Educational Focus:\n",
    "    This experiment demonstrates the practical benefits of advanced\n",
    "    inference optimizations in realistic workload scenarios.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🧪 Starting Comprehensive Inference Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate realistic inference workload\n",
    "    def generate_realistic_requests(num_requests: int = 50) -> List[InferenceRequest]:\n",
    "        \"\"\"Generate requests with realistic length distribution\"\"\"\n",
    "        \n",
    "        request_templates = [\n",
    "            (\"Write a short story about\", (50, 150)),    # Short creative\n",
    "            (\"Explain the concept of\", (100, 300)),      # Medium explanatory \n",
    "            (\"Generate Python code for\", (20, 100)),     # Short code\n",
    "            (\"Summarize this article\", (50, 200)),       # Medium summary\n",
    "            (\"Translate to French:\", (10, 50)),          # Short translation\n",
    "            (\"Write a detailed analysis\", (200, 500)),   # Long analysis\n",
    "            (\"What is\", (10, 30)),                       # Very short QA\n",
    "            (\"Create a comprehensive guide\", (300, 800)) # Very long guide\n",
    "        ]\n",
    "        \n",
    "        requests = []\n",
    "        \n",
    "        for i in range(num_requests):\n",
    "            template, (min_tokens, max_tokens) = random.choice(request_templates)\n",
    "            \n",
    "            request = InferenceRequest(\n",
    "                request_id=f\"req_{i:03d}\",\n",
    "                prompt=f\"{template} topic {i}\",\n",
    "                max_tokens=random.randint(min_tokens, max_tokens),\n",
    "                temperature=random.uniform(0.7, 1.3)\n",
    "            )\n",
    "            \n",
    "            requests.append(request)\n",
    "        \n",
    "        return requests\n",
    "    \n",
    "    # Generate workload\n",
    "    test_requests = generate_realistic_requests(100)\n",
    "    \n",
    "    print(f\"📊 Generated {len(test_requests)} realistic requests\")\n",
    "    token_distribution = [r.max_tokens for r in test_requests]\n",
    "    print(f\"   Token range: {min(token_distribution)} - {max(token_distribution)}\")\n",
    "    print(f\"   Average tokens: {np.mean(token_distribution):.1f}\")\n",
    "    print(f\"   Median tokens: {np.median(token_distribution):.1f}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test 1: Continuous Batching\n",
    "    print(f\"\\n🔄 Testing Continuous Batching Engine...\")\n",
    "    \n",
    "    continuous_engine = ContinuousBatchingEngine(\n",
    "        max_batch_size=16,\n",
    "        max_sequence_length=2048,\n",
    "        block_size=16,\n",
    "        total_blocks=2000\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Start processing\n",
    "        continuous_start = time.time()\n",
    "        continuous_engine.start_processing()\n",
    "        \n",
    "        # Add requests with realistic timing\n",
    "        for i, request in enumerate(test_requests):\n",
    "            # Create fresh request to avoid state issues\n",
    "            fresh_request = InferenceRequest(\n",
    "                request_id=f\"continuous_{request.request_id}\",\n",
    "                prompt=request.prompt,\n",
    "                max_tokens=request.max_tokens,\n",
    "                temperature=request.temperature\n",
    "            )\n",
    "            \n",
    "            continuous_engine.add_request(fresh_request)\n",
    "            \n",
    "            # Realistic request arrival pattern (Poisson-like)\n",
    "            if i % 10 == 0:  # Print progress\n",
    "                print(f\"   Added {i+1}/{len(test_requests)} requests\")\n",
    "            \n",
    "            # Small random delay between requests\n",
    "            time.sleep(random.uniform(0.01, 0.05))\n",
    "        \n",
    "        print(\"   All requests submitted, waiting for completion...\")\n",
    "        \n",
    "        # Wait for completion with status updates\n",
    "        wait_start = time.time()\n",
    "        max_wait_time = 30.0  # 30 seconds max wait\n",
    "        \n",
    "        while (len(continuous_engine.completed_requests) < len(test_requests) and \n",
    "               (time.time() - wait_start) < max_wait_time):\n",
    "            \n",
    "            time.sleep(1.0)  # Check every second\n",
    "            \n",
    "            # Status update every 5 seconds\n",
    "            if int(time.time() - wait_start) % 5 == 0:\n",
    "                completed = len(continuous_engine.completed_requests)\n",
    "                active = len(continuous_engine.active_requests)\n",
    "                print(f\"   Progress: {completed}/{len(test_requests)} completed, {active} active\")\n",
    "        \n",
    "        continuous_duration = time.time() - continuous_start\n",
    "        continuous_engine.stop_processing()\n",
    "        \n",
    "        # Collect results\n",
    "        continuous_stats = continuous_engine.get_performance_stats()\n",
    "        results['continuous_batching'] = {\n",
    "            'engine': continuous_engine,\n",
    "            'duration': continuous_duration,\n",
    "            'stats': continuous_stats,\n",
    "            'completed_requests': len(continuous_engine.completed_requests)\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✅ Continuous batching completed in {continuous_duration:.1f}s\")\n",
    "        print(f\"   📊 Completed {len(continuous_engine.completed_requests)}/{len(test_requests)} requests\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Continuous batching failed: {e}\")\n",
    "        results['continuous_batching'] = {'error': str(e)}\n",
    "    \n",
    "    # Test 2: Simulated Static Batching\n",
    "    print(f\"\\n📦 Testing Static Batching (Simulated)...\")\n",
    "    \n",
    "    try:\n",
    "        static_start = time.time()\n",
    "        \n",
    "        # Simulate static batching behavior\n",
    "        batch_size = 8\n",
    "        static_completed = 0\n",
    "        static_total_tokens = 0\n",
    "        \n",
    "        # Process in static batches\n",
    "        for batch_start in range(0, len(test_requests), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(test_requests))\n",
    "            batch = test_requests[batch_start:batch_end]\n",
    "            \n",
    "            # Find maximum tokens in batch (padding requirement)\n",
    "            max_tokens_in_batch = max(r.max_tokens for r in batch)\n",
    "            \n",
    "            # Simulate processing time (slower due to padding overhead)\n",
    "            # Base time + overhead for padding + stragglers\n",
    "            base_time = len(batch) * 0.01  # 10ms per request\n",
    "            padding_overhead = len(batch) * 0.005  # 5ms padding overhead per request\n",
    "            straggler_penalty = max_tokens_in_batch * 0.0001  # Penalty for waiting for longest\n",
    "            \n",
    "            total_batch_time = base_time + padding_overhead + straggler_penalty\n",
    "            time.sleep(total_batch_time)\n",
    "            \n",
    "            # All requests in batch complete together\n",
    "            static_completed += len(batch)\n",
    "            static_total_tokens += sum(r.max_tokens for r in batch)\n",
    "            \n",
    "            print(f\"   Batch {batch_start//batch_size + 1}: {len(batch)} requests, \"\n",
    "                  f\"max tokens: {max_tokens_in_batch}, time: {total_batch_time:.2f}s\")\n",
    "        \n",
    "        static_duration = time.time() - static_start\n",
    "        \n",
    "        results['static_batching'] = {\n",
    "            'duration': static_duration,\n",
    "            'completed_requests': static_completed,\n",
    "            'total_tokens': static_total_tokens,\n",
    "            'throughput_tps': static_total_tokens / static_duration,\n",
    "            'avg_latency_ms': static_duration * 1000 / static_completed  # Simplified\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✅ Static batching completed in {static_duration:.1f}s\")\n",
    "        print(f\"   📊 Processed {static_completed} requests\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Static batching failed: {e}\")\n",
    "        results['static_batching'] = {'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_inference_results(results: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of inference comparison results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n📊 INFERENCE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'continuous_batching' not in results or 'static_batching' not in results:\n",
    "        print(\"❌ Incomplete results - cannot perform comparison\")\n",
    "        return\n",
    "    \n",
    "    continuous = results['continuous_batching']\n",
    "    static = results['static_batching']\n",
    "    \n",
    "    if 'error' in continuous or 'error' in static:\n",
    "        print(\"❌ One or more tests failed\")\n",
    "        if 'error' in continuous:\n",
    "            print(f\"   Continuous batching error: {continuous['error']}\")\n",
    "        if 'error' in static:\n",
    "            print(f\"   Static batching error: {static['error']}\")\n",
    "        return\n",
    "    \n",
    "    # Extract metrics\n",
    "    continuous_stats = continuous['stats']\n",
    "    \n",
    "    print(f\"\\n🔄 Continuous Batching Results:\")\n",
    "    print(f\"   Duration: {continuous['duration']:.1f}s\")\n",
    "    print(f\"   Completed: {continuous['completed_requests']} requests\")\n",
    "    print(f\"   Tokens generated: {continuous_stats['total_tokens_generated']}\")\n",
    "    \n",
    "    if 'current_throughput_tps' in continuous_stats:\n",
    "        print(f\"   Throughput: {continuous_stats['current_throughput_tps']:.1f} tokens/sec\")\n",
    "    \n",
    "    if 'avg_latency_ms' in continuous_stats:\n",
    "        print(f\"   Avg latency: {continuous_stats['avg_latency_ms']:.1f} ms\")\n",
    "        print(f\"   P95 latency: {continuous_stats['p95_latency_ms']:.1f} ms\")\n",
    "    \n",
    "    if 'avg_ttft_ms' in continuous_stats:\n",
    "        print(f\"   Avg TTFT: {continuous_stats['avg_ttft_ms']:.1f} ms\")\n",
    "    \n",
    "    print(f\"   Avg batch size: {continuous_stats['avg_batch_size']:.1f}\")\n",
    "    \n",
    "    print(f\"\\n📦 Static Batching Results:\")\n",
    "    print(f\"   Duration: {static['duration']:.1f}s\")\n",
    "    print(f\"   Completed: {static['completed_requests']} requests\")\n",
    "    print(f\"   Tokens generated: {static['total_tokens']}\")\n",
    "    print(f\"   Throughput: {static['throughput_tps']:.1f} tokens/sec\")\n",
    "    print(f\"   Avg latency: {static['avg_latency_ms']:.1f} ms\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(f\"\\n📈 Performance Comparison:\")\n",
    "    \n",
    "    # Throughput improvement\n",
    "    if 'current_throughput_tps' in continuous_stats:\n",
    "        throughput_improvement = (continuous_stats['current_throughput_tps'] / \n",
    "                                static['throughput_tps'] - 1) * 100\n",
    "        print(f\"   Throughput improvement: {throughput_improvement:+.1f}%\")\n",
    "    \n",
    "    # Latency improvement\n",
    "    if 'avg_latency_ms' in continuous_stats:\n",
    "        latency_improvement = (1 - continuous_stats['avg_latency_ms'] / \n",
    "                             static['avg_latency_ms']) * 100\n",
    "        print(f\"   Latency improvement: {latency_improvement:+.1f}%\")\n",
    "    \n",
    "    # Total time comparison\n",
    "    time_improvement = (1 - continuous['duration'] / static['duration']) * 100\n",
    "    print(f\"   Total time improvement: {time_improvement:+.1f}%\")\n",
    "    \n",
    "    # Memory efficiency\n",
    "    if 'memory_stats' in continuous_stats:\n",
    "        mem_stats = continuous_stats['memory_stats']\n",
    "        print(f\"\\n💾 Memory Efficiency (Continuous):\")\n",
    "        print(f\"   Memory utilization: {mem_stats['memory_utilization']*100:.1f}%\")\n",
    "        print(f\"   Fragmentation: {mem_stats['fragmentation_ratio']*100:.1f}%\")\n",
    "        print(f\"   Active sequences: {mem_stats['active_sequences']}\")\n",
    "        \n",
    "        if mem_stats['fragmentation_ratio'] < 0.1:\n",
    "            print(f\"   ✅ Excellent memory efficiency!\")\n",
    "        elif mem_stats['fragmentation_ratio'] < 0.2:\n",
    "            print(f\"   🟡 Good memory efficiency\")\n",
    "        else:\n",
    "            print(f\"   🔴 High fragmentation - consider optimization\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    print(f\"\\n🎯 Overall Assessment:\")\n",
    "    \n",
    "    if (continuous['completed_requests'] > static['completed_requests'] * 0.9 and \n",
    "        continuous['duration'] < static['duration']):\n",
    "        print(f\"   ✅ Continuous batching shows clear advantages\")\n",
    "        print(f\"   📊 Recommendation: Use continuous batching for production\")\n",
    "    else:\n",
    "        print(f\"   🟡 Results are mixed - further analysis needed\")\n",
    "        print(f\"   🔍 Consider workload characteristics and hardware constraints\")\n",
    "\n",
    "# Run the comprehensive inference comparison\n",
    "print(\"🚀 Starting Comprehensive Inference Comparison Experiment\")\n",
    "inference_results = run_inference_comparison_experiment()\n",
    "analyze_inference_results(inference_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways from Advanced Inference Optimization\n",
    "\n",
    "### **Continuous Batching is Game-Changing**\n",
    "- **2-24x throughput improvement** over static batching\n",
    "- **50%+ latency reduction** for interactive workloads\n",
    "- **90%+ GPU utilization** vs 30-40% with static batching\n",
    "- **Essential for production** LLM serving\n",
    "\n",
    "### **PagedAttention Eliminates Memory Waste**\n",
    "- **Near-zero fragmentation** vs 50%+ in traditional systems\n",
    "- **Dynamic allocation** based on actual sequence lengths\n",
    "- **Block-based management** enables flexible memory reuse\n",
    "- **Virtual memory concepts** applied to attention computation\n",
    "\n",
    "### **Production Implications**\n",
    "- **Cost reduction**: Serve more users per GPU\n",
    "- **Better user experience**: Lower latency, higher throughput\n",
    "- **Resource efficiency**: Maximum hardware utilization\n",
    "- **Scalability**: Handle variable workload patterns\n",
    "\n",
    "### **Implementation Considerations**\n",
    "- **Complexity**: Requires sophisticated request management\n",
    "- **Memory management**: Need efficient block allocation algorithms\n",
    "- **Load balancing**: Dynamic request scheduling\n",
    "- **Monitoring**: More metrics to track and optimize\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Advanced Inference Techniques\n",
    "\n",
    "### **Speculative Decoding**\n",
    "```python\n",
    "# Use smaller model to predict multiple tokens ahead\n",
    "def speculative_decode(large_model, small_model, input_ids):\n",
    "    # Small model generates candidate tokens quickly\n",
    "    candidates = small_model.generate_candidates(input_ids, num_tokens=4)\n",
    "    \n",
    "    # Large model verifies in parallel\n",
    "    verified_tokens = large_model.verify_parallel(input_ids, candidates)\n",
    "    \n",
    "    return verified_tokens\n",
    "```\n",
    "\n",
    "### **Parallel Sampling**\n",
    "```python\n",
    "# Generate multiple sequences in parallel\n",
    "def parallel_sampling(model, prompt, num_samples=4):\n",
    "    # Use same KV cache for shared prefix\n",
    "    shared_prefix = model.encode(prompt)\n",
    "    \n",
    "    # Branch at sampling points\n",
    "    branches = model.parallel_decode(shared_prefix, num_samples)\n",
    "    \n",
    "    return branches\n",
    "```\n",
    "\n",
    "### **Quantized Inference**\n",
    "```python\n",
    "# INT8 inference with calibration\n",
    "def quantized_inference(model, calibration_data):\n",
    "    # Calibrate quantization parameters\n",
    "    quantized_model = quantize_model(model, calibration_data)\n",
    "    \n",
    "    # Use INT8 kernels for inference\n",
    "    return quantized_model.inference_optimized()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 Production Deployment Patterns\n",
    "\n",
    "### **Multi-Tier Architecture**\n",
    "```\n",
    "Load Balancer\n",
    "     ↓\n",
    "Request Router (determines model size needed)\n",
    "     ↓\n",
    "┌─────────────┬─────────────┬─────────────┐\n",
    "│ Small Model │ Medium Model│ Large Model │\n",
    "│ (Fast)      │ (Balanced)  │ (Accurate)  │\n",
    "│ vLLM Engine │ vLLM Engine │ vLLM Engine │\n",
    "└─────────────┴─────────────┴─────────────┘\n",
    "```\n",
    "\n",
    "### **Auto-scaling Strategy**\n",
    "```python\n",
    "# Scale based on queue depth and latency\n",
    "def should_scale_up(metrics):\n",
    "    return (\n",
    "        metrics.queue_depth > 50 or\n",
    "        metrics.p95_latency > 2000 or  # 2s\n",
    "        metrics.gpu_utilization < 60\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next: Chapter 7 - Distributed Training Strategies** 🌐\n",
    "\n",
    "*In the next chapter, we'll explore how to scale training across multiple GPUs and nodes, covering data parallelism, model parallelism, and pipeline parallelism strategies.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}