{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Scientific Profiling Methodology\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will:\n",
    "- **Master scientific profiling principles** for reproducible performance analysis\n",
    "- **Build production-grade monitoring systems** with statistical rigor\n",
    "- **Identify performance bottlenecks** systematically using data-driven methods\n",
    "- **Implement automated performance regression detection**\n",
    "- **Create comprehensive performance dashboards** for LLM operations\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why Scientific Profiling Matters\n",
    "\n",
    "### **The Problem with Ad-Hoc Profiling**\n",
    "\n",
    "Most performance optimization efforts fail because they lack **scientific rigor**:\n",
    "\n",
    "‚ùå **Anecdotal Evidence**: \"This model seems faster\"\n",
    "‚ùå **Cherry-picked Results**: Showing only the best runs\n",
    "‚ùå **Inconsistent Methodology**: Different conditions for each test\n",
    "‚ùå **No Statistical Validation**: Single measurements without confidence intervals\n",
    "‚ùå **Missing Context**: No baseline or environment documentation\n",
    "\n",
    "### **The Scientific Approach**\n",
    "\n",
    "‚úÖ **Reproducible Experiments**: Controlled conditions, documented setup\n",
    "‚úÖ **Statistical Analysis**: Multiple runs, confidence intervals, significance testing\n",
    "‚úÖ **Systematic Methodology**: Consistent measurement procedures\n",
    "‚úÖ **Comprehensive Metrics**: Latency, throughput, resource utilization\n",
    "‚úÖ **Automated Detection**: Regression alerts and performance tracking\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Performance Metrics Taxonomy\n",
    "\n",
    "### **Latency Metrics**\n",
    "\n",
    "#### **Time to First Token (TTFT)**\n",
    "- **Definition**: Time from request start to first generated token\n",
    "- **Importance**: User-perceived responsiveness\n",
    "- **Target**: < 200ms for interactive applications\n",
    "- **Factors**: Model loading, prompt processing, scheduling delays\n",
    "\n",
    "#### **Inter-Token Latency (ITL)**\n",
    "- **Definition**: Time between consecutive tokens\n",
    "- **Importance**: Streaming user experience\n",
    "- **Target**: < 50ms for smooth streaming\n",
    "- **Factors**: Inference speed, batching efficiency\n",
    "\n",
    "#### **End-to-End Latency**\n",
    "- **Definition**: Total time from request to complete response\n",
    "- **Importance**: Overall system performance\n",
    "- **Components**: Network, queuing, processing, post-processing\n",
    "\n",
    "### **Throughput Metrics**\n",
    "\n",
    "#### **Tokens per Second (TPS)**\n",
    "- **Per Model**: Tokens/second for a single model instance\n",
    "- **Per GPU**: Tokens/second per GPU device\n",
    "- **Per Dollar**: Cost efficiency metric\n",
    "\n",
    "#### **Requests per Second (RPS)**\n",
    "- **Concurrent Requests**: Number of simultaneous requests handled\n",
    "- **Queue Saturation**: Point where latency increases rapidly\n",
    "\n",
    "### **Resource Utilization Metrics**\n",
    "\n",
    "#### **GPU Metrics**\n",
    "- **Compute Utilization**: % time GPU cores are active\n",
    "- **Memory Utilization**: % of GPU memory used\n",
    "- **Memory Bandwidth**: % of theoretical peak bandwidth\n",
    "- **Tensor Core Utilization**: % time tensor cores are active\n",
    "\n",
    "#### **System Metrics**\n",
    "- **CPU Utilization**: Host CPU usage\n",
    "- **Network I/O**: Data transfer rates\n",
    "- **Disk I/O**: Storage access patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Scientific Measurement Framework\n",
    "\n",
    "### **Experimental Design Principles**\n",
    "\n",
    "#### 1. **Controlled Variables**\n",
    "- **Fixed**: Model, hardware, software versions\n",
    "- **Varied**: Only the parameter being tested\n",
    "- **Documented**: All environmental conditions\n",
    "\n",
    "#### 2. **Statistical Power**\n",
    "- **Sample Size**: Sufficient measurements for significance\n",
    "- **Multiple Runs**: Account for measurement variance\n",
    "- **Confidence Intervals**: Quantify measurement uncertainty\n",
    "\n",
    "#### 3. **Systematic Bias Elimination**\n",
    "- **Warm-up Periods**: Allow system to reach steady state\n",
    "- **Randomization**: Prevent ordering effects\n",
    "- **Baseline Measurements**: Reference point for comparisons\n",
    "\n",
    "Let's implement a professional benchmarking framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Callable, Any\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import psutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up professional plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    \"\"\"Configuration for scientific benchmarking experiments\"\"\"\n",
    "    \n",
    "    # Experiment parameters\n",
    "    name: str\n",
    "    description: str\n",
    "    num_warmup_runs: int = 5\n",
    "    num_measurement_runs: int = 20\n",
    "    confidence_level: float = 0.95\n",
    "    \n",
    "    # Environment\n",
    "    device: str = 'cuda'\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    \n",
    "    # Data collection\n",
    "    collect_gpu_stats: bool = True\n",
    "    collect_memory_stats: bool = True\n",
    "    collect_system_stats: bool = True\n",
    "    \n",
    "    # Output\n",
    "    save_results: bool = True\n",
    "    output_dir: str = \"benchmark_results\"\n",
    "    \n",
    "@dataclass\n",
    "class MeasurementResult:\n",
    "    \"\"\"Single measurement result with comprehensive metrics\"\"\"\n",
    "    \n",
    "    timestamp: float\n",
    "    duration_seconds: float\n",
    "    gpu_utilization: Optional[float] = None\n",
    "    gpu_memory_used: Optional[float] = None\n",
    "    gpu_memory_total: Optional[float] = None\n",
    "    gpu_temperature: Optional[float] = None\n",
    "    gpu_power_draw: Optional[float] = None\n",
    "    cpu_utilization: Optional[float] = None\n",
    "    system_memory_used: Optional[float] = None\n",
    "    custom_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "class ScientificProfiler:\n",
    "    \"\"\"\n",
    "    Production-grade scientific profiling framework\n",
    "    \n",
    "    This class implements rigorous statistical methodology for\n",
    "    performance measurement and analysis in LLM systems.\n",
    "    \n",
    "    Educational Focus:\n",
    "    - Statistical significance testing\n",
    "    - Measurement uncertainty quantification\n",
    "    - Systematic bias elimination\n",
    "    - Reproducible experimental design\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BenchmarkConfig):\n",
    "        self.config = config\n",
    "        self.results: List[MeasurementResult] = []\n",
    "        self.metadata = self._collect_system_metadata()\n",
    "        \n",
    "        # Create output directory\n",
    "        if config.save_results:\n",
    "            Path(config.output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    def _collect_system_metadata(self) -> Dict[str, Any]:\n",
    "        \"\"\"Collect comprehensive system information for reproducibility\"\"\"\n",
    "        \n",
    "        metadata = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"pytorch_version\": torch.__version__,\n",
    "            \"cuda_available\": torch.cuda.is_available(),\n",
    "            \"python_version\": f\"{psutil.sys.version_info.major}.{psutil.sys.version_info.minor}\"\n",
    "        }\n",
    "        \n",
    "        # CUDA information\n",
    "        if torch.cuda.is_available():\n",
    "            props = torch.cuda.get_device_properties(0)\n",
    "            metadata.update({\n",
    "                \"gpu_name\": props.name,\n",
    "                \"gpu_memory_gb\": props.total_memory / (1024**3),\n",
    "                \"compute_capability\": f\"{props.major}.{props.minor}\",\n",
    "                \"multiprocessor_count\": props.multiprocessor_count,\n",
    "                \"cuda_version\": torch.version.cuda\n",
    "            })\n",
    "        \n",
    "        # System information\n",
    "        metadata.update({\n",
    "            \"cpu_count\": psutil.cpu_count(),\n",
    "            \"cpu_freq_mhz\": psutil.cpu_freq().current if psutil.cpu_freq() else \"unknown\",\n",
    "            \"system_memory_gb\": psutil.virtual_memory().total / (1024**3),\n",
    "            \"platform\": psutil.sys.platform\n",
    "        })\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _collect_runtime_stats(self) -> Dict[str, Optional[float]]:\n",
    "        \"\"\"Collect runtime performance statistics\"\"\"\n",
    "        \n",
    "        stats = {}\n",
    "        \n",
    "        # GPU stats\n",
    "        if self.config.collect_gpu_stats and torch.cuda.is_available():\n",
    "            try:\n",
    "                # Try nvidia-smi first\n",
    "                result = subprocess.run([\n",
    "                    'nvidia-smi', \n",
    "                    '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw',\n",
    "                    '--format=csv,noheader,nounits'\n",
    "                ], capture_output=True, text=True, timeout=2)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    values = result.stdout.strip().split(', ')\n",
    "                    stats.update({\n",
    "                        'gpu_utilization': float(values[0]),\n",
    "                        'gpu_memory_used': float(values[1]) * 1e6,  # MB to bytes\n",
    "                        'gpu_memory_total': float(values[2]) * 1e6,\n",
    "                        'gpu_temperature': float(values[3]),\n",
    "                        'gpu_power_draw': float(values[4])\n",
    "                    })\n",
    "                else:\n",
    "                    # Fallback to PyTorch memory info\n",
    "                    stats.update({\n",
    "                        'gpu_memory_used': torch.cuda.memory_allocated(0),\n",
    "                        'gpu_memory_total': torch.cuda.get_device_properties(0).total_memory\n",
    "                    })\n",
    "            except:\n",
    "                # Minimal fallback\n",
    "                stats.update({\n",
    "                    'gpu_memory_used': torch.cuda.memory_allocated(0) if torch.cuda.is_available() else None,\n",
    "                    'gpu_memory_total': torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else None\n",
    "                })\n",
    "        \n",
    "        # System stats\n",
    "        if self.config.collect_system_stats:\n",
    "            stats.update({\n",
    "                'cpu_utilization': psutil.cpu_percent(interval=0.1),\n",
    "                'system_memory_used': psutil.virtual_memory().used\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def benchmark_function(self, \n",
    "                          func: Callable, \n",
    "                          *args, \n",
    "                          custom_metrics_func: Optional[Callable] = None,\n",
    "                          **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Benchmark a function with scientific rigor\n",
    "        \n",
    "        Args:\n",
    "            func: Function to benchmark\n",
    "            *args, **kwargs: Arguments to pass to the function\n",
    "            custom_metrics_func: Optional function to compute custom metrics\n",
    "            \n",
    "        Returns:\n",
    "            Comprehensive statistical analysis of performance\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"üî¨ Starting scientific benchmark: {self.config.name}\")\n",
    "        print(f\"üìä Configuration: {self.config.num_warmup_runs} warmup + {self.config.num_measurement_runs} measurement runs\")\n",
    "        \n",
    "        # Clear any cached results\n",
    "        self.results.clear()\n",
    "        \n",
    "        # GPU synchronization if available\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Warmup phase\n",
    "        print(\"üî• Warmup phase...\")\n",
    "        for i in range(self.config.num_warmup_runs):\n",
    "            try:\n",
    "                _ = func(*args, **kwargs)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Warmup run {i+1} failed: {e}\")\n",
    "        \n",
    "        # Measurement phase\n",
    "        print(\"üìè Measurement phase...\")\n",
    "        for i in range(self.config.num_measurement_runs):\n",
    "            # Pre-measurement stats\n",
    "            pre_stats = self._collect_runtime_stats()\n",
    "            \n",
    "            # Clear GPU state\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Measure execution time\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                end_time = time.perf_counter()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                # Post-measurement stats\n",
    "                post_stats = self._collect_runtime_stats()\n",
    "                \n",
    "                # Compute custom metrics if provided\n",
    "                custom_metrics = {}\n",
    "                if custom_metrics_func:\n",
    "                    try:\n",
    "                        custom_metrics = custom_metrics_func(result) or {}\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Custom metrics computation failed: {e}\")\n",
    "                \n",
    "                # Store measurement\n",
    "                measurement = MeasurementResult(\n",
    "                    timestamp=end_time,\n",
    "                    duration_seconds=duration,\n",
    "                    gpu_utilization=post_stats.get('gpu_utilization'),\n",
    "                    gpu_memory_used=post_stats.get('gpu_memory_used'),\n",
    "                    gpu_memory_total=post_stats.get('gpu_memory_total'),\n",
    "                    gpu_temperature=post_stats.get('gpu_temperature'),\n",
    "                    gpu_power_draw=post_stats.get('gpu_power_draw'),\n",
    "                    cpu_utilization=post_stats.get('cpu_utilization'),\n",
    "                    system_memory_used=post_stats.get('system_memory_used'),\n",
    "                    custom_metrics=custom_metrics\n",
    "                )\n",
    "                \n",
    "                self.results.append(measurement)\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 5 == 0:\n",
    "                    print(f\"   Completed {i + 1}/{self.config.num_measurement_runs} measurements\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Measurement {i+1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Analyze results\n",
    "        analysis = self._analyze_results()\n",
    "        \n",
    "        # Save results if requested\n",
    "        if self.config.save_results:\n",
    "            self._save_results(analysis)\n",
    "        \n",
    "        print(f\"‚úÖ Benchmark completed: {len(self.results)} successful measurements\")\n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_results(self) -> Dict[str, Any]:\n",
    "        \"\"\"Perform comprehensive statistical analysis of benchmark results\"\"\"\n",
    "        \n",
    "        if not self.results:\n",
    "            return {\"error\": \"No successful measurements\"}\n",
    "        \n",
    "        # Extract duration data\n",
    "        durations = [r.duration_seconds for r in self.results]\n",
    "        durations_ms = [d * 1000 for d in durations]  # Convert to milliseconds\n",
    "        \n",
    "        # Basic statistics\n",
    "        mean_duration = np.mean(durations)\n",
    "        median_duration = np.median(durations)\n",
    "        std_duration = np.std(durations, ddof=1)  # Sample standard deviation\n",
    "        \n",
    "        # Percentiles\n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        duration_percentiles = {f\"p{p}\": np.percentile(durations_ms, p) for p in percentiles}\n",
    "        \n",
    "        # Confidence interval for mean\n",
    "        from scipy import stats\n",
    "        confidence_level = self.config.confidence_level\n",
    "        t_critical = stats.t.ppf((1 + confidence_level) / 2, len(durations) - 1)\n",
    "        margin_of_error = t_critical * (std_duration / np.sqrt(len(durations)))\n",
    "        ci_lower = (mean_duration - margin_of_error) * 1000  # ms\n",
    "        ci_upper = (mean_duration + margin_of_error) * 1000  # ms\n",
    "        \n",
    "        # Throughput calculations\n",
    "        mean_throughput_per_sec = 1.0 / mean_duration if mean_duration > 0 else 0\n",
    "        \n",
    "        analysis = {\n",
    "            \"experiment\": {\n",
    "                \"name\": self.config.name,\n",
    "                \"description\": self.config.description,\n",
    "                \"num_measurements\": len(self.results),\n",
    "                \"confidence_level\": confidence_level\n",
    "            },\n",
    "            \"timing\": {\n",
    "                \"mean_ms\": mean_duration * 1000,\n",
    "                \"median_ms\": median_duration * 1000,\n",
    "                \"std_ms\": std_duration * 1000,\n",
    "                \"min_ms\": min(durations_ms),\n",
    "                \"max_ms\": max(durations_ms),\n",
    "                **duration_percentiles,\n",
    "                \"confidence_interval_ms\": [ci_lower, ci_upper],\n",
    "                \"margin_of_error_ms\": margin_of_error * 1000,\n",
    "                \"coefficient_of_variation\": (std_duration / mean_duration) * 100 if mean_duration > 0 else float('inf')\n",
    "            },\n",
    "            \"throughput\": {\n",
    "                \"operations_per_second\": mean_throughput_per_sec,\n",
    "                \"operations_per_minute\": mean_throughput_per_sec * 60\n",
    "            },\n",
    "            \"system_metadata\": self.metadata\n",
    "        }\n",
    "        \n",
    "        # GPU statistics if available\n",
    "        gpu_utils = [r.gpu_utilization for r in self.results if r.gpu_utilization is not None]\n",
    "        if gpu_utils:\n",
    "            analysis[\"gpu_utilization\"] = {\n",
    "                \"mean_percent\": np.mean(gpu_utils),\n",
    "                \"std_percent\": np.std(gpu_utils, ddof=1),\n",
    "                \"min_percent\": min(gpu_utils),\n",
    "                \"max_percent\": max(gpu_utils)\n",
    "            }\n",
    "        \n",
    "        # Memory statistics\n",
    "        gpu_memory_used = [r.gpu_memory_used for r in self.results if r.gpu_memory_used is not None]\n",
    "        if gpu_memory_used:\n",
    "            analysis[\"gpu_memory\"] = {\n",
    "                \"mean_gb\": np.mean(gpu_memory_used) / (1024**3),\n",
    "                \"peak_gb\": max(gpu_memory_used) / (1024**3),\n",
    "                \"total_gb\": self.results[0].gpu_memory_total / (1024**3) if self.results[0].gpu_memory_total else None\n",
    "            }\n",
    "        \n",
    "        # Custom metrics analysis\n",
    "        custom_metrics_analysis = {}\n",
    "        if self.results[0].custom_metrics:\n",
    "            for metric_name in self.results[0].custom_metrics.keys():\n",
    "                values = [r.custom_metrics.get(metric_name) for r in self.results if metric_name in r.custom_metrics]\n",
    "                if values and all(v is not None for v in values):\n",
    "                    custom_metrics_analysis[metric_name] = {\n",
    "                        \"mean\": np.mean(values),\n",
    "                        \"std\": np.std(values, ddof=1),\n",
    "                        \"min\": min(values),\n",
    "                        \"max\": max(values)\n",
    "                    }\n",
    "        \n",
    "        if custom_metrics_analysis:\n",
    "            analysis[\"custom_metrics\"] = custom_metrics_analysis\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _save_results(self, analysis: Dict[str, Any]):\n",
    "        \"\"\"Save benchmark results to files\"\"\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_filename = f\"{self.config.name}_{timestamp}\"\n",
    "        \n",
    "        # Save analysis as JSON\n",
    "        analysis_file = Path(self.config.output_dir) / f\"{base_filename}_analysis.json\"\n",
    "        with open(analysis_file, 'w') as f:\n",
    "            json.dump(analysis, f, indent=2, default=str)\n",
    "        \n",
    "        # Save raw results as CSV\n",
    "        raw_data = []\n",
    "        for i, result in enumerate(self.results):\n",
    "            row = {\n",
    "                'measurement_id': i,\n",
    "                'timestamp': result.timestamp,\n",
    "                'duration_seconds': result.duration_seconds,\n",
    "                'duration_ms': result.duration_seconds * 1000,\n",
    "                'gpu_utilization': result.gpu_utilization,\n",
    "                'gpu_memory_used_gb': result.gpu_memory_used / (1024**3) if result.gpu_memory_used else None,\n",
    "                'gpu_temperature': result.gpu_temperature,\n",
    "                'cpu_utilization': result.cpu_utilization\n",
    "            }\n",
    "            # Add custom metrics\n",
    "            if result.custom_metrics:\n",
    "                row.update(result.custom_metrics)\n",
    "            raw_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(raw_data)\n",
    "        csv_file = Path(self.config.output_dir) / f\"{base_filename}_raw.csv\"\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        print(f\"üìÅ Results saved:\")\n",
    "        print(f\"   Analysis: {analysis_file}\")\n",
    "        print(f\"   Raw data: {csv_file}\")\n",
    "\n",
    "print(\"‚úÖ Scientific Profiling Framework Initialized!\")\n",
    "print(\"üî¨ Ready for rigorous performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Practical Example: Matrix Multiplication Benchmark\n",
    "\n",
    "Let's demonstrate the scientific profiling framework with a comprehensive matrix multiplication benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matrix_operations():\n",
    "    \"\"\"\n",
    "    Comprehensive matrix multiplication benchmark demonstrating\n",
    "    scientific profiling methodology for LLM-relevant operations.\n",
    "    \n",
    "    This benchmark tests the core operation in transformer models:\n",
    "    dense matrix multiplication (GEMM) which dominates LLM compute.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define test configurations\n",
    "    test_configs = [\n",
    "        {\n",
    "            \"name\": \"small_gemm_fp32\",\n",
    "            \"description\": \"Small GEMM operation in FP32 (baseline)\",\n",
    "            \"size\": 1024,\n",
    "            \"dtype\": torch.float32\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"large_gemm_fp32\", \n",
    "            \"description\": \"Large GEMM operation in FP32 (LLM-scale)\",\n",
    "            \"size\": 4096,\n",
    "            \"dtype\": torch.float32\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add FP16 tests if GPU supports it\n",
    "    if torch.cuda.is_available():\n",
    "        test_configs.extend([\n",
    "            {\n",
    "                \"name\": \"small_gemm_fp16\",\n",
    "                \"description\": \"Small GEMM operation in FP16 (mixed precision)\",\n",
    "                \"size\": 1024,\n",
    "                \"dtype\": torch.float16\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"large_gemm_fp16\",\n",
    "                \"description\": \"Large GEMM operation in FP16 (mixed precision)\", \n",
    "                \"size\": 4096,\n",
    "                \"dtype\": torch.float16\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in test_configs:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üî¨ Testing: {config['name']}\")\n",
    "        print(f\"üìù Description: {config['description']}\")\n",
    "        print(f\"üìä Matrix size: {config['size']}x{config['size']}\")\n",
    "        print(f\"üî¢ Data type: {config['dtype']}\")\n",
    "        \n",
    "        # Create benchmark configuration\n",
    "        benchmark_config = BenchmarkConfig(\n",
    "            name=config['name'],\n",
    "            description=config['description'],\n",
    "            num_warmup_runs=3,\n",
    "            num_measurement_runs=15,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            dtype=config['dtype']\n",
    "        )\n",
    "        \n",
    "        # Initialize profiler\n",
    "        profiler = ScientificProfiler(benchmark_config)\n",
    "        \n",
    "        # Define the operation to benchmark\n",
    "        def matrix_multiply_operation(size, dtype, device):\n",
    "            \"\"\"The operation we're benchmarking\"\"\"\n",
    "            A = torch.randn(size, size, dtype=dtype, device=device)\n",
    "            B = torch.randn(size, size, dtype=dtype, device=device)\n",
    "            C = torch.mm(A, B)  # Matrix multiplication\n",
    "            return C\n",
    "        \n",
    "        # Custom metrics function\n",
    "        def compute_custom_metrics(result):\n",
    "            \"\"\"Compute LLM-relevant performance metrics\"\"\"\n",
    "            size = config['size']\n",
    "            \n",
    "            # Calculate theoretical FLOPS (2*N^3 for matrix multiplication)\n",
    "            theoretical_flops = 2 * (size ** 3)\n",
    "            \n",
    "            # Estimate memory bandwidth (read A, read B, write C)\n",
    "            element_size = 4 if config['dtype'] == torch.float32 else 2  # bytes\n",
    "            memory_transferred = (2 * size * size + size * size) * element_size  # A + B + C\n",
    "            \n",
    "            return {\n",
    "                'theoretical_flops': theoretical_flops,\n",
    "                'memory_transferred_mb': memory_transferred / (1024**2),\n",
    "                'matrix_size': size\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Run benchmark\n",
    "            analysis = profiler.benchmark_function(\n",
    "                matrix_multiply_operation,\n",
    "                config['size'],\n",
    "                config['dtype'], \n",
    "                benchmark_config.device,\n",
    "                custom_metrics_func=compute_custom_metrics\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results[config['name']] = analysis\n",
    "            \n",
    "            # Print key results\n",
    "            timing = analysis['timing']\n",
    "            print(f\"\\nüìä Results Summary:\")\n",
    "            print(f\"   ‚è±Ô∏è  Mean time: {timing['mean_ms']:.2f} ¬± {timing['margin_of_error_ms']:.2f} ms\")\n",
    "            print(f\"   üìà Throughput: {analysis['throughput']['operations_per_second']:.1f} ops/sec\")\n",
    "            print(f\"   üìè P95 latency: {timing['p95']:.2f} ms\")\n",
    "            \n",
    "            # Calculate derived performance metrics\n",
    "            if 'custom_metrics' in analysis:\n",
    "                custom = analysis['custom_metrics']\n",
    "                if 'theoretical_flops' in custom:\n",
    "                    flops_per_sec = custom['theoretical_flops']['mean'] / (timing['mean_ms'] / 1000)\n",
    "                    gflops_per_sec = flops_per_sec / 1e9\n",
    "                    print(f\"   üöÄ Performance: {gflops_per_sec:.1f} GFLOPS\")\n",
    "                    \n",
    "                if 'memory_transferred_mb' in custom:\n",
    "                    bandwidth_gbs = (custom['memory_transferred_mb']['mean'] / 1024) / (timing['mean_ms'] / 1000)\n",
    "                    print(f\"   üíæ Bandwidth: {bandwidth_gbs:.1f} GB/s\")\n",
    "            \n",
    "            if 'gpu_utilization' in analysis:\n",
    "                gpu_util = analysis['gpu_utilization']\n",
    "                print(f\"   üéÆ GPU utilization: {gpu_util['mean_percent']:.1f}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Benchmark failed: {e}\")\n",
    "            results[config['name']] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comprehensive benchmark\n",
    "print(\"üöÄ Starting Comprehensive Matrix Multiplication Benchmark\")\n",
    "print(\"This will demonstrate scientific profiling methodology...\")\n",
    "\n",
    "benchmark_results = benchmark_matrix_operations()\n",
    "\n",
    "print(f\"\\n‚úÖ Benchmark suite completed!\")\n",
    "print(f\"üìä {len([r for r in benchmark_results.values() if 'error' not in r])} successful benchmarks\")\n",
    "print(f\"‚ùå {len([r for r in benchmark_results.values() if 'error' in r])} failed benchmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Statistical Analysis and Visualization\n",
    "\n",
    "Let's create comprehensive visualizations of our benchmark results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_analysis_dashboard(results: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Create a comprehensive performance analysis dashboard\n",
    "    \n",
    "    Educational Focus:\n",
    "    This function demonstrates how to create publication-quality\n",
    "    performance analysis visualizations with statistical rigor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter successful results\n",
    "    successful_results = {k: v for k, v in results.items() if 'error' not in v}\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"‚ùå No successful results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üî¨ Scientific Performance Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Latency Comparison with Error Bars\n",
    "    ax1 = axes[0, 0]\n",
    "    names = []\n",
    "    means = []\n",
    "    errors = []\n",
    "    colors = []\n",
    "    \n",
    "    color_map = {'fp32': '#FF6B6B', 'fp16': '#4ECDC4', 'small': '#45B7D1', 'large': '#96CEB4'}\n",
    "    \n",
    "    for name, result in successful_results.items():\n",
    "        if 'timing' in result:\n",
    "            timing = result['timing']\n",
    "            names.append(name.replace('_', '\\n'))\n",
    "            means.append(timing['mean_ms'])\n",
    "            errors.append(timing['margin_of_error_ms'])\n",
    "            \n",
    "            # Assign colors based on configuration\n",
    "            if 'fp16' in name:\n",
    "                colors.append(color_map['fp16'])\n",
    "            else:\n",
    "                colors.append(color_map['fp32'])\n",
    "    \n",
    "    bars = ax1.bar(names, means, yerr=errors, capsize=5, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax1.set_ylabel('Latency (ms)')\n",
    "    ax1.set_title('üïê Latency Comparison with Confidence Intervals')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean, error in zip(bars, means, errors):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + error + max(means)*0.01,\n",
    "                f'{mean:.1f}¬±{error:.1f}ms', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Performance (GFLOPS) Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    gflops_data = []\n",
    "    gflops_names = []\n",
    "    \n",
    "    for name, result in successful_results.items():\n",
    "        if 'custom_metrics' in result and 'theoretical_flops' in result['custom_metrics']:\n",
    "            timing = result['timing']\n",
    "            flops = result['custom_metrics']['theoretical_flops']['mean']\n",
    "            gflops = flops / (timing['mean_ms'] / 1000) / 1e9\n",
    "            gflops_data.append(gflops)\n",
    "            gflops_names.append(name.replace('_', '\\n'))\n",
    "    \n",
    "    if gflops_data:\n",
    "        bars2 = ax2.bar(gflops_names, gflops_data, color=colors[:len(gflops_data)], alpha=0.8, edgecolor='black')\n",
    "        ax2.set_ylabel('Performance (GFLOPS)')\n",
    "        ax2.set_title('üöÄ Computational Performance')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, gflops in zip(bars2, gflops_data):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(gflops_data)*0.01,\n",
    "                    f'{gflops:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 3. Memory Bandwidth Utilization\n",
    "    ax3 = axes[1, 0]\n",
    "    bandwidth_data = []\n",
    "    bandwidth_names = []\n",
    "    \n",
    "    for name, result in successful_results.items():\n",
    "        if 'custom_metrics' in result and 'memory_transferred_mb' in result['custom_metrics']:\n",
    "            timing = result['timing']\n",
    "            memory_mb = result['custom_metrics']['memory_transferred_mb']['mean']\n",
    "            bandwidth_gbs = (memory_mb / 1024) / (timing['mean_ms'] / 1000)\n",
    "            bandwidth_data.append(bandwidth_gbs)\n",
    "            bandwidth_names.append(name.replace('_', '\\n'))\n",
    "    \n",
    "    if bandwidth_data:\n",
    "        bars3 = ax3.bar(bandwidth_names, bandwidth_data, color=colors[:len(bandwidth_data)], alpha=0.8, edgecolor='black')\n",
    "        ax3.set_ylabel('Memory Bandwidth (GB/s)')\n",
    "        ax3.set_title('üíæ Memory Bandwidth Utilization')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add theoretical bandwidth line if T4\n",
    "        if torch.cuda.is_available():\n",
    "            props = torch.cuda.get_device_properties(0)\n",
    "            if \"T4\" in props.name:\n",
    "                ax3.axhline(y=320, color='red', linestyle='--', alpha=0.7, label='T4 Peak (320 GB/s)')\n",
    "                ax3.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, bw in zip(bars3, bandwidth_data):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + max(bandwidth_data)*0.01,\n",
    "                    f'{bw:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 4. Statistical Distribution (Box Plot)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # We'll create a simplified visualization since we don't have raw measurement data here\n",
    "    # In practice, you'd plot the actual distribution of measurements\n",
    "    cv_data = []\n",
    "    cv_names = []\n",
    "    \n",
    "    for name, result in successful_results.items():\n",
    "        if 'timing' in result:\n",
    "            cv = result['timing']['coefficient_of_variation']\n",
    "            if cv != float('inf'):\n",
    "                cv_data.append(cv)\n",
    "                cv_names.append(name.replace('_', '\\n'))\n",
    "    \n",
    "    if cv_data:\n",
    "        bars4 = ax4.bar(cv_names, cv_data, color=colors[:len(cv_data)], alpha=0.8, edgecolor='black')\n",
    "        ax4.set_ylabel('Coefficient of Variation (%)')\n",
    "        ax4.set_title('üìä Measurement Stability')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add reference line for acceptable variability (5%)\n",
    "        ax4.axhline(y=5, color='green', linestyle='--', alpha=0.7, label='Target (<5%)')\n",
    "        ax4.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, cv in zip(bars4, cv_data):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + max(cv_data)*0.01,\n",
    "                    f'{cv:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print performance analysis summary\n",
    "    print(\"\\nüìã Performance Analysis Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find best performing configurations\n",
    "    if gflops_data and gflops_names:\n",
    "        best_perf_idx = np.argmax(gflops_data)\n",
    "        best_config = gflops_names[best_perf_idx].replace('\\n', '_')\n",
    "        best_gflops = gflops_data[best_perf_idx]\n",
    "        print(f\"üèÜ Best Performance: {best_config} - {best_gflops:.1f} GFLOPS\")\n",
    "    \n",
    "    if means and names:\n",
    "        fastest_idx = np.argmin(means)\n",
    "        fastest_config = names[fastest_idx].replace('\\n', '_')\n",
    "        fastest_time = means[fastest_idx]\n",
    "        print(f\"‚ö° Lowest Latency: {fastest_config} - {fastest_time:.2f} ms\")\n",
    "    \n",
    "    # Performance insights\n",
    "    print(\"\\nüîç Key Insights:\")\n",
    "    \n",
    "    # Compare FP32 vs FP16 if both available\n",
    "    fp32_results = {k: v for k, v in successful_results.items() if 'fp32' in k}\n",
    "    fp16_results = {k: v for k, v in successful_results.items() if 'fp16' in k}\n",
    "    \n",
    "    if fp32_results and fp16_results:\n",
    "        fp32_mean = np.mean([r['timing']['mean_ms'] for r in fp32_results.values()])\n",
    "        fp16_mean = np.mean([r['timing']['mean_ms'] for r in fp16_results.values()])\n",
    "        speedup = fp32_mean / fp16_mean if fp16_mean > 0 else 0\n",
    "        print(f\"   üìà FP16 vs FP32 speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        if speedup > 1.5:\n",
    "            print(f\"   ‚úÖ Mixed precision shows significant benefit\")\n",
    "        elif speedup > 1.1:\n",
    "            print(f\"   üü° Mixed precision shows moderate benefit\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Mixed precision benefit limited - check Tensor Core utilization\")\n",
    "    \n",
    "    # Measurement quality assessment\n",
    "    if cv_data:\n",
    "        avg_cv = np.mean(cv_data)\n",
    "        if avg_cv < 2:\n",
    "            print(f\"   ‚úÖ Excellent measurement stability (CV: {avg_cv:.1f}%)\")\n",
    "        elif avg_cv < 5:\n",
    "            print(f\"   üü° Good measurement stability (CV: {avg_cv:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   üî¥ High measurement variability (CV: {avg_cv:.1f}%) - investigate\")\n",
    "\n",
    "# Create the performance analysis dashboard\n",
    "if benchmark_results:\n",
    "    print(\"\\nüìä Creating Performance Analysis Dashboard...\")\n",
    "    create_performance_analysis_dashboard(benchmark_results)\n",
    "else:\n",
    "    print(\"‚ùå No benchmark results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways from Scientific Profiling\n",
    "\n",
    "### **Statistical Rigor is Essential**\n",
    "- **Single measurements** are meaningless - use statistical analysis\n",
    "- **Confidence intervals** quantify measurement uncertainty\n",
    "- **Multiple runs** account for system variability\n",
    "- **Coefficient of variation** indicates measurement quality\n",
    "\n",
    "### **Comprehensive Metrics Matter**\n",
    "- **Latency percentiles** (P95, P99) reveal tail behavior\n",
    "- **Resource utilization** identifies bottlenecks\n",
    "- **Memory bandwidth** often limits LLM performance\n",
    "- **Custom metrics** provide domain-specific insights\n",
    "\n",
    "### **Systematic Methodology**\n",
    "- **Controlled experiments** with documented conditions\n",
    "- **Warmup phases** eliminate cold-start effects\n",
    "- **Environmental documentation** enables reproducibility\n",
    "- **Automated analysis** prevents human bias\n",
    "\n",
    "### **Production Implications**\n",
    "- **Performance regression detection** prevents degradation\n",
    "- **Resource planning** based on empirical data\n",
    "- **SLA definition** grounded in measurement reality\n",
    "- **Optimization prioritization** guided by bottleneck analysis\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Advanced Profiling Techniques\n",
    "\n",
    "### **PyTorch Profiler Integration**\n",
    "```python\n",
    "# Example of detailed kernel-level profiling\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    # Your LLM operations here\n",
    "    pass\n",
    "\n",
    "# Export for analysis\n",
    "prof.export_chrome_trace(\"trace.json\")\n",
    "```\n",
    "\n",
    "### **NVIDIA Nsight Systems**\n",
    "```bash\n",
    "# Profile CUDA kernels and memory transfers\n",
    "nsys profile -o profile_output python your_llm_script.py\n",
    "```\n",
    "\n",
    "### **Memory Profiling**\n",
    "```python\n",
    "# Track memory allocation patterns\n",
    "torch.cuda.memory._record_memory_history()\n",
    "# ... run your code ...\n",
    "snapshot = torch.cuda.memory._snapshot()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Exercises\n",
    "\n",
    "### **Exercise 1: Custom Benchmark**\n",
    "Create a benchmark for a specific LLM operation (attention, layer norm, etc.) using the scientific profiling framework.\n",
    "\n",
    "### **Exercise 2: Performance Regression Detection**\n",
    "Implement an automated system that detects when performance degrades beyond acceptable thresholds.\n",
    "\n",
    "### **Exercise 3: Multi-GPU Profiling**\n",
    "Extend the framework to profile multi-GPU operations and analyze communication overhead.\n",
    "\n",
    "---\n",
    "\n",
    "**Next: Chapter 3 - Memory Optimization Techniques** üíæ\n",
    "\n",
    "*In the next chapter, we'll dive deep into memory optimization techniques including gradient checkpointing, activation recomputation, and dynamic memory management.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}