{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: GPU Architecture & Fundamentals\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will:\n",
    "- **Understand modern GPU architecture** from the ground up\n",
    "- **Master memory hierarchy** and its impact on ML performance\n",
    "- **Analyze CUDA execution models** and their implications\n",
    "- **Optimize memory access patterns** for maximum bandwidth utilization\n",
    "- **Leverage Tensor Cores** for mixed-precision acceleration\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why GPU Architecture Matters for LLMs\n",
    "\n",
    "### **The Performance Reality**\n",
    "Modern LLMs like GPT-4 and Claude require:\n",
    "- **Massive parallelism**: Billions of parameters processed simultaneously\n",
    "- **High memory bandwidth**: Moving TB/s of data between compute units\n",
    "- **Specialized compute**: Tensor operations optimized for AI workloads\n",
    "- **Efficient communication**: Multi-GPU coordination for large models\n",
    "\n",
    "**Understanding GPU architecture isn't optional—it's the foundation of all LLM optimization.**\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Modern GPU Architecture Deep Dive\n",
    "\n",
    "### **NVIDIA GPU Hierarchy (2024)**\n",
    "\n",
    "```\n",
    "Consumer/Research: RTX 4090, RTX 4080\n",
    "                   ↓\n",
    "Professional:      A6000, RTX 6000 Ada\n",
    "                   ↓\n",
    "Data Center:       T4, V100, A100, H100\n",
    "                   ↓\n",
    "Next-Gen:          B100, GB200 (2024-2025)\n",
    "```\n",
    "\n",
    "### **Key Architectural Components**\n",
    "\n",
    "#### 1. **Streaming Multiprocessors (SMs)**\n",
    "- **Definition**: The basic computational unit containing multiple cores\n",
    "- **T4**: 40 SMs × 64 CUDA cores = 2,560 total cores\n",
    "- **A100**: 108 SMs × 64 CUDA cores = 6,912 total cores\n",
    "- **H100**: 114 SMs × 128 CUDA cores = 14,592 total cores\n",
    "\n",
    "#### 2. **CUDA Cores**\n",
    "- **Purpose**: Execute single-precision floating-point operations\n",
    "- **Capability**: One FP32 operation per clock cycle\n",
    "- **Limitation**: Not optimized for AI workloads (hence Tensor Cores)\n",
    "\n",
    "#### 3. **Tensor Cores**\n",
    "- **Revolution**: Specialized AI acceleration units\n",
    "- **T4**: 320 Tensor Cores (1st gen) - 65 TFLOPS FP16\n",
    "- **A100**: 432 Tensor Cores (3rd gen) - 312 TFLOPS FP16\n",
    "- **H100**: 456 Tensor Cores (4th gen) - 1,979 TFLOPS FP8\n",
    "\n",
    "#### 4. **RT Cores** (Graphics-focused, less relevant for LLMs)\n",
    "- Ray tracing acceleration\n",
    "- Some applications in 3D AI and NeRF training\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Memory Hierarchy: The Performance Bottleneck\n",
    "\n",
    "### **The Memory Wall Problem**\n",
    "\n",
    "**Compute performance** has grown exponentially:\n",
    "- 2016: P100 - 21 TFLOPS FP16\n",
    "- 2024: H100 - 1,979 TFLOPS FP8 (94x improvement)\n",
    "\n",
    "**Memory bandwidth** has grown linearly:\n",
    "- 2016: P100 - 732 GB/s\n",
    "- 2024: H100 - 3,350 GB/s (4.6x improvement)\n",
    "\n",
    "**Result**: Most LLM operations are memory-bound, not compute-bound!\n",
    "\n",
    "### **GPU Memory Hierarchy (Fastest to Slowest)**\n",
    "\n",
    "#### 1. **Registers**\n",
    "- **Location**: Inside each CUDA core\n",
    "- **Size**: 256KB per SM (T4), 512KB per SM (A100)\n",
    "- **Bandwidth**: ~20 TB/s\n",
    "- **Latency**: 0 cycles\n",
    "- **Use Case**: Thread-local variables, loop counters\n",
    "\n",
    "#### 2. **Shared Memory**\n",
    "- **Location**: Shared within each SM\n",
    "- **Size**: 64KB per SM (configurable with L1 cache)\n",
    "- **Bandwidth**: ~15 TB/s\n",
    "- **Latency**: 1-2 cycles\n",
    "- **Use Case**: Block-level data sharing, manual caching\n",
    "\n",
    "#### 3. **L1 Cache**\n",
    "- **Location**: Per SM (shared with shared memory)\n",
    "- **Size**: 128KB per SM (configurable split)\n",
    "- **Bandwidth**: ~1 TB/s\n",
    "- **Latency**: 20-40 cycles\n",
    "- **Use Case**: Automatic caching of global memory accesses\n",
    "\n",
    "#### 4. **L2 Cache**\n",
    "- **Location**: Shared across all SMs\n",
    "- **Size**: 5MB (T4), 40MB (A100), 50MB (H100)\n",
    "- **Bandwidth**: ~500 GB/s\n",
    "- **Latency**: 200-400 cycles\n",
    "- **Use Case**: Cross-SM data sharing, texture caching\n",
    "\n",
    "#### 5. **Global Memory (HBM)**\n",
    "- **Location**: High Bandwidth Memory stacks\n",
    "- **Size**: 16GB (T4), 80GB (A100), 80GB (H100)\n",
    "- **Bandwidth**: 320GB/s (T4), 1,935GB/s (A100), 3,350GB/s (H100)\n",
    "- **Latency**: 400-800 cycles\n",
    "- **Use Case**: Model parameters, activations, gradients\n",
    "\n",
    "#### 6. **System Memory**\n",
    "- **Location**: CPU DRAM via PCIe\n",
    "- **Size**: System dependent (32GB-1TB+)\n",
    "- **Bandwidth**: ~50 GB/s (PCIe 4.0 x16)\n",
    "- **Latency**: 1000+ cycles\n",
    "- **Use Case**: Data staging, checkpointing\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 CUDA Execution Model\n",
    "\n",
    "### **Thread Hierarchy**\n",
    "\n",
    "```\n",
    "Grid (entire kernel launch)\n",
    "├── Block 0 (executed on one SM)\n",
    "│   ├── Warp 0 (32 threads, SIMD execution)\n",
    "│   │   ├── Thread 0\n",
    "│   │   ├── Thread 1\n",
    "│   │   └── ... Thread 31\n",
    "│   ├── Warp 1 (32 threads)\n",
    "│   └── ...\n",
    "├── Block 1\n",
    "└── ...\n",
    "```\n",
    "\n",
    "### **Key Concepts**\n",
    "\n",
    "#### **Warp Execution**\n",
    "- **32 threads** execute the same instruction simultaneously (SIMD)\n",
    "- **Divergence penalty**: If threads take different branches, performance drops\n",
    "- **Memory coalescing**: Adjacent threads should access adjacent memory\n",
    "\n",
    "#### **Occupancy**\n",
    "- **Definition**: Ratio of active warps to maximum possible warps per SM\n",
    "- **Factors**: Register usage, shared memory usage, block size\n",
    "- **Target**: 75-100% occupancy for memory-bound kernels\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 Tensor Cores: The AI Revolution\n",
    "\n",
    "### **Evolution of Tensor Cores**\n",
    "\n",
    "#### **1st Generation (V100)**\n",
    "- **Mixed Precision**: FP16 inputs, FP32 accumulate\n",
    "- **Matrix Size**: 4×4×4 (A×B+C)\n",
    "- **Performance**: 125 TFLOPS peak\n",
    "\n",
    "#### **2nd Generation (T4, RTX series)**\n",
    "- **Added INT8/INT4** support\n",
    "- **Improved sparsity** handling\n",
    "- **Better compiler** integration\n",
    "\n",
    "#### **3rd Generation (A100)**\n",
    "- **BF16 support** (better numerical range than FP16)\n",
    "- **TF32 mode** (FP32 range, FP16 precision) - 156 TFLOPS\n",
    "- **Sparsity optimization** (2:4 structured sparsity)\n",
    "\n",
    "#### **4th Generation (H100)**\n",
    "- **FP8 support** (E4M3 and E5M2 formats)\n",
    "- **Transformer Engine** integration\n",
    "- **1,979 TFLOPS FP8** peak performance\n",
    "\n",
    "### **Why Tensor Cores Matter for LLMs**\n",
    "\n",
    "#### **Traditional GEMM (General Matrix Multiply)**\n",
    "```python\n",
    "# CUDA Cores: Element-wise operations\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        C[i][j] = 0\n",
    "        for k in range(K):\n",
    "            C[i][j] += A[i][k] * B[k][j]  # One FP32 op per cycle\n",
    "```\n",
    "\n",
    "#### **Tensor Core GEMM**\n",
    "```python\n",
    "# Tensor Cores: Fused matrix operations\n",
    "# Single instruction computes 4x4 matrix multiply-accumulate\n",
    "C_4x4 = A_4x4 @ B_4x4 + C_4x4  # 64 FP16 ops in one cycle!\n",
    "```\n",
    "\n",
    "**Performance Gain**: 16-32x speedup for mixed-precision workloads\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 LLM-Specific Performance Considerations\n",
    "\n",
    "### **Transformer Architecture Impact**\n",
    "\n",
    "#### **Attention Mechanism**\n",
    "- **GEMM operations**: Q×K^T, Softmax×V\n",
    "- **Memory pattern**: Non-sequential access for attention weights\n",
    "- **Optimization**: FlashAttention, PagedAttention\n",
    "\n",
    "#### **Feed-Forward Networks**\n",
    "- **Large GEMMs**: Hidden → 4×Hidden → Hidden\n",
    "- **Memory pattern**: Sequential, tensor-core friendly\n",
    "- **Optimization**: Activation checkpointing, mixed precision\n",
    "\n",
    "#### **Layer Normalization**\n",
    "- **Reduction operations**: Mean, variance across hidden dimension\n",
    "- **Memory bound**: Low arithmetic intensity\n",
    "- **Optimization**: Kernel fusion, FP16 computation\n",
    "\n",
    "### **Sequence Length Scaling**\n",
    "\n",
    "#### **Memory Complexity**\n",
    "- **Attention**: O(n²) for sequence length n\n",
    "- **Feed-forward**: O(n) linear scaling\n",
    "- **Total model**: Dominated by attention for long sequences\n",
    "\n",
    "#### **Compute Complexity**\n",
    "- **Training**: O(n²) attention + O(n) feed-forward\n",
    "- **Inference**: O(n) per token (with KV caching)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Practical Architecture Analysis\n",
    "\n",
    "Let's analyze your current GPU and understand its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "def analyze_gpu_architecture() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive GPU architecture analysis\n",
    "    \n",
    "    This function demonstrates how to programmatically analyze\n",
    "    GPU capabilities for LLM optimization decisions.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"error\": \"No CUDA GPU detected\"}\n",
    "    \n",
    "    # Basic PyTorch GPU info\n",
    "    device_id = 0\n",
    "    props = torch.cuda.get_device_properties(device_id)\n",
    "    \n",
    "    analysis = {\n",
    "        \"basic_info\": {\n",
    "            \"name\": props.name,\n",
    "            \"compute_capability\": f\"{props.major}.{props.minor}\",\n",
    "            \"total_memory_gb\": props.total_memory / (1024**3),\n",
    "            \"multiprocessor_count\": props.multiprocessor_count,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Derive architecture details from compute capability\n",
    "    major, minor = props.major, props.minor\n",
    "    \n",
    "    if major == 7 and minor == 5:  # T4\n",
    "        analysis[\"architecture\"] = {\n",
    "            \"generation\": \"Turing\",\n",
    "            \"year\": \"2018\",\n",
    "            \"cuda_cores_per_sm\": 64,\n",
    "            \"total_cuda_cores\": props.multiprocessor_count * 64,\n",
    "            \"tensor_cores\": \"2nd Gen (320 cores)\",\n",
    "            \"tensor_core_performance\": \"65 TFLOPS FP16\",\n",
    "            \"memory_type\": \"GDDR6\",\n",
    "            \"memory_bandwidth_gbs\": 320,\n",
    "            \"l2_cache_mb\": 5.0,\n",
    "            \"shared_memory_per_sm_kb\": 64,\n",
    "        }\n",
    "    elif major == 8 and minor == 0:  # A100\n",
    "        analysis[\"architecture\"] = {\n",
    "            \"generation\": \"Ampere\",\n",
    "            \"year\": \"2020\",\n",
    "            \"cuda_cores_per_sm\": 64,\n",
    "            \"total_cuda_cores\": props.multiprocessor_count * 64,\n",
    "            \"tensor_cores\": \"3rd Gen (432 cores)\",\n",
    "            \"tensor_core_performance\": \"312 TFLOPS FP16, 156 TFLOPS TF32\",\n",
    "            \"memory_type\": \"HBM2e\",\n",
    "            \"memory_bandwidth_gbs\": 1935,\n",
    "            \"l2_cache_mb\": 40.0,\n",
    "            \"shared_memory_per_sm_kb\": 164,\n",
    "        }\n",
    "    elif major == 9 and minor == 0:  # H100\n",
    "        analysis[\"architecture\"] = {\n",
    "            \"generation\": \"Hopper\",\n",
    "            \"year\": \"2022\",\n",
    "            \"cuda_cores_per_sm\": 128,\n",
    "            \"total_cuda_cores\": props.multiprocessor_count * 128,\n",
    "            \"tensor_cores\": \"4th Gen (456 cores)\",\n",
    "            \"tensor_core_performance\": \"1979 TFLOPS FP8, 989 TFLOPS FP16\",\n",
    "            \"memory_type\": \"HBM3\",\n",
    "            \"memory_bandwidth_gbs\": 3350,\n",
    "            \"l2_cache_mb\": 50.0,\n",
    "            \"shared_memory_per_sm_kb\": 228,\n",
    "        }\n",
    "    else:\n",
    "        analysis[\"architecture\"] = {\n",
    "            \"generation\": \"Unknown\",\n",
    "            \"note\": f\"Compute capability {major}.{minor} not in database\"\n",
    "        }\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    if \"architecture\" in analysis and \"memory_bandwidth_gbs\" in analysis[\"architecture\"]:\n",
    "        bandwidth = analysis[\"architecture\"][\"memory_bandwidth_gbs\"]\n",
    "        memory_gb = analysis[\"basic_info\"][\"total_memory_gb\"]\n",
    "        \n",
    "        analysis[\"performance_analysis\"] = {\n",
    "            \"memory_bandwidth_per_gb\": bandwidth / memory_gb,\n",
    "            \"bandwidth_utilization_threshold\": \"80%+ for memory-bound kernels\",\n",
    "            \"recommended_batch_size_fp16\": f\"~{int(memory_gb * 0.8 / 2)} for inference\",\n",
    "            \"tensor_core_recommendation\": \"Use mixed precision (FP16) for maximum performance\"\n",
    "        }\n",
    "    \n",
    "    # Try to get additional info from nvidia-smi\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'nvidia-smi', '--query-gpu=driver_version,cuda_version,pci.bus_id,pcie.link.gen.current,pcie.link.width.current',\n",
    "            '--format=csv,noheader,nounits'\n",
    "        ], capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            values = result.stdout.strip().split(', ')\n",
    "            analysis[\"system_info\"] = {\n",
    "                \"driver_version\": values[0],\n",
    "                \"cuda_version\": values[1],\n",
    "                \"pci_bus_id\": values[2],\n",
    "                \"pcie_generation\": values[3],\n",
    "                \"pcie_width\": values[4] + \"x\"\n",
    "            }\n",
    "    except:\n",
    "        analysis[\"system_info\"] = {\"note\": \"nvidia-smi not available\"}\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze current GPU\n",
    "print(\"🔍 Analyzing GPU Architecture...\")\n",
    "gpu_analysis = analyze_gpu_architecture()\n",
    "\n",
    "# Pretty print results\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, width=80)\n",
    "pp.pprint(gpu_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Memory Bandwidth Utilization Test\n",
    "\n",
    "Let's measure actual memory bandwidth and compare it to theoretical limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def benchmark_memory_bandwidth(device='cuda'):\n",
    "    \"\"\"\n",
    "    Measure actual memory bandwidth using various access patterns\n",
    "    \n",
    "    Educational Purpose:\n",
    "    This benchmark demonstrates how different memory access patterns\n",
    "    affect bandwidth utilization - a key concept for LLM optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test different data sizes (MB)\n",
    "    sizes_mb = [1, 4, 16, 64, 256, 1024]\n",
    "    \n",
    "    for size_mb in sizes_mb:\n",
    "        if device == 'cuda':\n",
    "            # Check if size fits in GPU memory\n",
    "            available_mb = torch.cuda.get_device_properties(0).total_memory / (1024**2)\n",
    "            if size_mb * 3 > available_mb * 0.8:  # Need 3x size for safety\n",
    "                continue\n",
    "        \n",
    "        size_elements = (size_mb * 1024 * 1024) // 4  # FP32 elements\n",
    "        \n",
    "        # Create test tensors\n",
    "        a = torch.randn(size_elements, device=device, dtype=torch.float32)\n",
    "        b = torch.randn(size_elements, device=device, dtype=torch.float32)\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Warm up\n",
    "        for _ in range(5):\n",
    "            c = a + b\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark vector addition (memory bound operation)\n",
    "        num_iterations = max(10, 1000 // size_mb)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for _ in range(num_iterations):\n",
    "            c = a + b  # Read A, Read B, Write C = 3x data movement\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate bandwidth\n",
    "        total_time = end_time - start_time\n",
    "        bytes_transferred = size_mb * 1024 * 1024 * 3 * num_iterations  # 3x for read A, read B, write C\n",
    "        bandwidth_gbs = (bytes_transferred / (1024**3)) / total_time\n",
    "        \n",
    "        results[size_mb] = {\n",
    "            'bandwidth_gbs': bandwidth_gbs,\n",
    "            'iterations': num_iterations,\n",
    "            'time_seconds': total_time\n",
    "        }\n",
    "        \n",
    "        print(f\"Size: {size_mb:4d} MB, Bandwidth: {bandwidth_gbs:6.1f} GB/s\")\n",
    "        \n",
    "        # Clean up\n",
    "        del a, b, c\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"📊 Memory Bandwidth Benchmark\")\n",
    "print(\"Testing vector addition (memory-bound operation)...\\n\")\n",
    "\n",
    "bandwidth_results = benchmark_memory_bandwidth()\n",
    "\n",
    "if bandwidth_results:\n",
    "    # Plot results\n",
    "    sizes = list(bandwidth_results.keys())\n",
    "    bandwidths = [bandwidth_results[s]['bandwidth_gbs'] for s in sizes]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogx(sizes, bandwidths, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Data Size (MB)')\n",
    "    plt.ylabel('Bandwidth (GB/s)')\n",
    "    plt.title('Memory Bandwidth vs Data Size')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add theoretical bandwidth line if we know the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        if \"T4\" in props.name:\n",
    "            plt.axhline(y=320, color='r', linestyle='--', alpha=0.7, label='T4 Theoretical (320 GB/s)')\n",
    "            plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    max_bandwidth = max(bandwidths)\n",
    "    print(f\"\\n🎯 Peak measured bandwidth: {max_bandwidth:.1f} GB/s\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        if \"T4\" in props.name:\n",
    "            efficiency = (max_bandwidth / 320) * 100\n",
    "            print(f\"📈 Bandwidth efficiency: {efficiency:.1f}% of T4 theoretical peak\")\n",
    "            \n",
    "            if efficiency > 80:\n",
    "                print(\"✅ Excellent bandwidth utilization!\")\n",
    "            elif efficiency > 60:\n",
    "                print(\"🟡 Good bandwidth utilization\")\n",
    "            else:\n",
    "                print(\"🔴 Low bandwidth utilization - check for bottlenecks\")\n",
    "else:\n",
    "    print(\"❌ Benchmark failed - insufficient memory or other issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "### **Memory is King**\n",
    "- Modern LLMs are **memory-bound**, not compute-bound\n",
    "- **Bandwidth utilization** is more important than peak FLOPS\n",
    "- **Memory access patterns** determine performance\n",
    "\n",
    "### **Tensor Cores are Essential**\n",
    "- **16-32x speedup** for mixed-precision operations\n",
    "- **Mixed precision training** is not optional for production\n",
    "- **Proper data layout** is required for tensor core utilization\n",
    "\n",
    "### **Architecture Matters**\n",
    "- **Different GPUs** have different optimization strategies\n",
    "- **Understanding your hardware** guides optimization decisions\n",
    "- **Theoretical limits** set expectations for achievable performance\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Additional Resources\n",
    "\n",
    "### **NVIDIA Documentation**\n",
    "- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)\n",
    "- [Tensor Core Programming](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/)\n",
    "- [GPU Architecture Whitepapers](https://www.nvidia.com/en-us/data-center/resources/)\n",
    "\n",
    "### **Academic Papers**\n",
    "- \"Analyzing Deep Neural Networks with Tensor Cores\" (ISCA 2020)\n",
    "- \"Understanding the Memory and Compute Requirements of LLMs\" (MLSys 2023)\n",
    "- \"FlashAttention: Fast and Memory-Efficient Exact Attention\" (NeurIPS 2022)\n",
    "\n",
    "### **Industry Benchmarks**\n",
    "- [MLPerf Training and Inference](https://mlperf.org/)\n",
    "- [NVIDIA Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples)\n",
    "\n",
    "---\n",
    "\n",
    "**Next: Chapter 2 - Scientific Profiling Methodology** 🔬\n",
    "\n",
    "*In the next chapter, we'll learn how to systematically identify and measure performance bottlenecks using scientific methodology and production-grade tools.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}