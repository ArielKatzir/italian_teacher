{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Memory Optimization Techniques\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will:\n",
    "- **Master gradient checkpointing** theory and implementation\n",
    "- **Understand memory-compute tradeoffs** in transformer training\n",
    "- **Implement activation recomputation** strategies\n",
    "- **Optimize memory allocation patterns** for maximum efficiency\n",
    "- **Build dynamic memory management** systems for variable-length sequences\n",
    "\n",
    "---\n",
    "\n",
    "## üß† The Memory Crisis in LLM Training\n",
    "\n",
    "### **Why Memory Optimization Matters**\n",
    "\n",
    "Modern LLMs face a **memory wall** that fundamentally limits training:\n",
    "\n",
    "#### **Memory Requirements Scale Exponentially**\n",
    "- **Model Parameters**: 7B model = ~28GB (FP32) or ~14GB (FP16)\n",
    "- **Optimizer States**: Adam requires 2x parameter memory = ~28GB additional\n",
    "- **Gradients**: Same size as parameters = ~14GB additional  \n",
    "- **Activations**: Grows with sequence length and batch size = 10-100GB+\n",
    "- **Total**: 70-150GB for 7B model training!\n",
    "\n",
    "#### **Hardware Constraints**\n",
    "- **Consumer GPUs**: RTX 4090 = 24GB VRAM\n",
    "- **Professional GPUs**: A100 = 80GB VRAM\n",
    "- **High-end**: H100 = 80GB VRAM\n",
    "\n",
    "**Result**: Even small LLMs don't fit on single GPUs without optimization!\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Memory Breakdown in Transformer Training\n",
    "\n",
    "### **Memory Components (7B Parameter Model)**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                Model Memory                     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Parameters      ‚îÇ 14GB (FP16)                  ‚îÇ\n",
    "‚îÇ Gradients       ‚îÇ 14GB (FP16)                  ‚îÇ\n",
    "‚îÇ Optimizer States‚îÇ 28GB (Adam: momentum + var)  ‚îÇ\n",
    "‚îÇ Activations     ‚îÇ 10-100GB (depends on batch)  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ TOTAL          ‚îÇ 66-156GB                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### **Activation Memory Deep Dive**\n",
    "\n",
    "**Activations are the memory killer** in transformer training:\n",
    "\n",
    "#### **Per-Layer Activations (Typical)**\n",
    "- **Input embeddings**: `[batch, seq_len, hidden_dim]`\n",
    "- **Attention QKV**: `3 √ó [batch, seq_len, hidden_dim]`\n",
    "- **Attention scores**: `[batch, num_heads, seq_len, seq_len]` ‚Üê **Quadratic!**\n",
    "- **Attention output**: `[batch, seq_len, hidden_dim]`\n",
    "- **FFN intermediate**: `[batch, seq_len, 4 √ó hidden_dim]` ‚Üê **4x expansion!**\n",
    "- **Layer output**: `[batch, seq_len, hidden_dim]`\n",
    "\n",
    "#### **Memory Scaling**\n",
    "```python\n",
    "# Attention memory scales quadratically with sequence length\n",
    "attention_memory = batch_size * num_heads * seq_len¬≤ * bytes_per_element\n",
    "\n",
    "# Example: GPT-3 scale\n",
    "# batch=8, heads=96, seq_len=2048, FP16=2 bytes\n",
    "# = 8 √ó 96 √ó 2048¬≤ √ó 2 = 6.4 GB per layer!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Gradient Checkpointing: Theory and Practice\n",
    "\n",
    "### **The Core Insight**\n",
    "\n",
    "**Trade computation for memory** by recomputing activations during backward pass:\n",
    "\n",
    "#### **Standard Training (Memory Expensive)**\n",
    "```\n",
    "Forward Pass:  Store all activations\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ L1  ‚îÇ ‚îÇ L2  ‚îÇ ‚îÇ L3  ‚îÇ ‚îÇ L4  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò\n",
    "   ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ\n",
    "   ‚ñº      ‚ñº      ‚ñº      ‚ñº\n",
    "  üíæ     üíæ     üíæ     üíæ   ‚Üê All stored\n",
    "\n",
    "Backward Pass: Use stored activations\n",
    "   ‚ñ≤      ‚ñ≤      ‚ñ≤      ‚ñ≤\n",
    "   ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ\n",
    "   üíæ     üíæ     üíæ     üíæ   ‚Üê Retrieved from memory\n",
    "```\n",
    "\n",
    "#### **Gradient Checkpointing (Compute Intensive)**\n",
    "```\n",
    "Forward Pass:  Store only checkpoints\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ L1  ‚îÇ ‚îÇ L2  ‚îÇ ‚îÇ L3  ‚îÇ ‚îÇ L4  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò\n",
    "   ‚îÇ              ‚îÇ      \n",
    "   ‚ñº              ‚ñº      \n",
    "  üíæ              üíæ       ‚Üê Only some stored\n",
    "\n",
    "Backward Pass: Recompute missing activations\n",
    "   ‚ñ≤      üîÑ      ‚ñ≤      üîÑ\n",
    "   ‚îÇ    (recomp)  ‚îÇ    (recomp)\n",
    "   üíæ             üíæ          ‚Üê Compute on demand\n",
    "```\n",
    "\n",
    "### **Mathematical Framework**\n",
    "\n",
    "#### **Memory-Compute Tradeoff**\n",
    "- **Memory Reduction**: `O(‚àön)` instead of `O(n)` for n layers\n",
    "- **Compute Overhead**: ~33% additional FLOPs\n",
    "- **Net Benefit**: Enable much larger models/batches\n",
    "\n",
    "#### **Optimal Checkpointing Strategy**\n",
    "For `n` layers, optimal checkpointing uses `‚àön` checkpoints:\n",
    "```\n",
    "checkpoint_interval = sqrt(num_layers)\n",
    "memory_saved = num_layers / sqrt(num_layers) = sqrt(num_layers)\n",
    "```\n",
    "\n",
    "Let's implement a comprehensive gradient checkpointing system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Callable, Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class MemorySnapshot:\n",
    "    \"\"\"Capture memory usage at a specific point\"\"\"\n",
    "    timestamp: float\n",
    "    allocated_mb: float\n",
    "    reserved_mb: float\n",
    "    max_allocated_mb: float\n",
    "    description: str = \"\"\n",
    "\n",
    "class MemoryProfiler:\n",
    "    \"\"\"\n",
    "    Professional memory profiling for gradient checkpointing analysis\n",
    "    \n",
    "    Educational Focus:\n",
    "    This class demonstrates how to systematically measure memory usage\n",
    "    and analyze the effectiveness of memory optimization techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, enabled: bool = True):\n",
    "        self.enabled = enabled and torch.cuda.is_available()\n",
    "        self.snapshots: List[MemorySnapshot] = []\n",
    "        \n",
    "        if self.enabled:\n",
    "            # Reset memory stats\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    def snapshot(self, description: str = \"\") -> MemorySnapshot:\n",
    "        \"\"\"Take a memory snapshot\"\"\"\n",
    "        \n",
    "        if not self.enabled:\n",
    "            return MemorySnapshot(time.time(), 0, 0, 0, description)\n",
    "        \n",
    "        snapshot = MemorySnapshot(\n",
    "            timestamp=time.time(),\n",
    "            allocated_mb=torch.cuda.memory_allocated() / (1024**2),\n",
    "            reserved_mb=torch.cuda.memory_reserved() / (1024**2),\n",
    "            max_allocated_mb=torch.cuda.max_memory_allocated() / (1024**2),\n",
    "            description=description\n",
    "        )\n",
    "        \n",
    "        self.snapshots.append(snapshot)\n",
    "        return snapshot\n",
    "    \n",
    "    @contextmanager\n",
    "    def profile_block(self, description: str):\n",
    "        \"\"\"Context manager for profiling a code block\"\"\"\n",
    "        self.snapshot(f\"{description} - start\")\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.snapshot(f\"{description} - end\")\n",
    "    \n",
    "    def get_peak_memory_mb(self) -> float:\n",
    "        \"\"\"Get peak memory usage across all snapshots\"\"\"\n",
    "        if not self.snapshots:\n",
    "            return 0.0\n",
    "        return max(s.max_allocated_mb for s in self.snapshots)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print memory usage summary\"\"\"\n",
    "        if not self.enabled:\n",
    "            print(\"‚ùå Memory profiling not available (no CUDA GPU)\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìä Memory Usage Summary:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, snapshot in enumerate(self.snapshots):\n",
    "            print(f\"{i+1:2d}. {snapshot.description:30s} \"\n",
    "                  f\"Allocated: {snapshot.allocated_mb:6.1f} MB, \"\n",
    "                  f\"Peak: {snapshot.max_allocated_mb:6.1f} MB\")\n",
    "        \n",
    "        if self.snapshots:\n",
    "            peak = self.get_peak_memory_mb()\n",
    "            print(f\"\\nüèîÔ∏è  Overall Peak Memory: {peak:.1f} MB\")\n",
    "\n",
    "class SimpleTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified transformer layer for memory optimization demonstrations\n",
    "    \n",
    "    Educational Purpose:\n",
    "    This implementation focuses on memory patterns rather than\n",
    "    performance, making it ideal for learning optimization techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int, ff_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.qkv_proj = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff1 = nn.Linear(hidden_dim, ff_dim, bias=False)\n",
    "        self.ff2 = nn.Linear(ff_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, use_checkpointing: bool = False) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with optional gradient checkpointing\"\"\"\n",
    "        \n",
    "        if use_checkpointing:\n",
    "            # Use gradient checkpointing for sub-components\n",
    "            attn_out = checkpoint.checkpoint(self._attention_block, x)\n",
    "            output = checkpoint.checkpoint(self._ffn_block, attn_out)\n",
    "        else:\n",
    "            # Standard forward pass\n",
    "            attn_out = self._attention_block(x)\n",
    "            output = self._ffn_block(attn_out)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _attention_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Multi-head attention block\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Layer norm + QKV projection\n",
    "        normed = self.ln1(x)\n",
    "        qkv = self.qkv_proj(normed)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention computation (memory intensive!)\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_probs, v)\n",
    "        \n",
    "        # Reshape and project\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, seq_len, self.hidden_dim)\n",
    "        output = self.out_proj(attention_output)\n",
    "        \n",
    "        # Residual connection\n",
    "        return x + output\n",
    "    \n",
    "    def _ffn_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Feed-forward network block\"\"\"\n",
    "        # Layer norm + FFN\n",
    "        normed = self.ln2(x)\n",
    "        ff_intermediate = self.activation(self.ff1(normed))  # 4x expansion!\n",
    "        ff_output = self.ff2(ff_intermediate)\n",
    "        \n",
    "        # Residual connection\n",
    "        return x + ff_output\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple transformer model for memory optimization experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, hidden_dim: int, num_layers: int, \n",
    "                 num_heads: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, hidden_dim)\n",
    "        \n",
    "        # Transformer layers\n",
    "        ff_dim = 4 * hidden_dim  # Standard 4x expansion\n",
    "        self.layers = nn.ModuleList([\n",
    "            SimpleTransformerLayer(hidden_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.ln_final = nn.LayerNorm(hidden_dim)\n",
    "        self.output_proj = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, use_checkpointing: bool = False) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with optional gradient checkpointing\"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        \n",
    "        # Transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, use_checkpointing=use_checkpointing)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ Memory Optimization Framework Initialized!\")\n",
    "print(\"üß† Ready to explore gradient checkpointing and memory optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Gradient Checkpointing Experiment\n",
    "\n",
    "Let's conduct a comprehensive experiment comparing standard training vs gradient checkpointing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_comparison_experiment():\n",
    "    \"\"\"\n",
    "    Comprehensive experiment comparing memory usage patterns\n",
    "    between standard training and gradient checkpointing.\n",
    "    \n",
    "    Educational Focus:\n",
    "    This experiment quantifies the memory-compute tradeoff\n",
    "    and demonstrates practical optimization techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß™ Starting Memory Optimization Experiment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Experimental configuration\n",
    "    configs = [\n",
    "        {\n",
    "            \"name\": \"Small Model\",\n",
    "            \"vocab_size\": 10000,\n",
    "            \"hidden_dim\": 512,\n",
    "            \"num_layers\": 6,\n",
    "            \"num_heads\": 8,\n",
    "            \"seq_len\": 256,\n",
    "            \"batch_size\": 4\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Medium Model\", \n",
    "            \"vocab_size\": 20000,\n",
    "            \"hidden_dim\": 1024,\n",
    "            \"num_layers\": 12,\n",
    "            \"num_heads\": 16,\n",
    "            \"seq_len\": 512,\n",
    "            \"batch_size\": 2\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\nüî¨ Testing Configuration: {config['name']}\")\n",
    "        print(f\"   Hidden Dim: {config['hidden_dim']}, Layers: {config['num_layers']}\")\n",
    "        print(f\"   Sequence Length: {config['seq_len']}, Batch Size: {config['batch_size']}\")\n",
    "        \n",
    "        config_results = {}\n",
    "        \n",
    "        # Test both standard and checkpointed training\n",
    "        for use_checkpointing in [False, True]:\n",
    "            mode = \"Gradient Checkpointing\" if use_checkpointing else \"Standard Training\"\n",
    "            print(f\"\\n  üìä Testing: {mode}\")\n",
    "            \n",
    "            try:\n",
    "                # Initialize memory profiler\n",
    "                profiler = MemoryProfiler()\n",
    "                profiler.snapshot(\"Initial state\")\n",
    "                \n",
    "                # Create model\n",
    "                with profiler.profile_block(\"Model creation\"):\n",
    "                    model = SimpleTransformer(\n",
    "                        vocab_size=config['vocab_size'],\n",
    "                        hidden_dim=config['hidden_dim'],\n",
    "                        num_layers=config['num_layers'],\n",
    "                        num_heads=config['num_heads'],\n",
    "                        max_seq_len=config['seq_len']\n",
    "                    )\n",
    "                    \n",
    "                    if torch.cuda.is_available():\n",
    "                        model = model.cuda()\n",
    "                    \n",
    "                    # Create optimizer\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "                \n",
    "                # Create sample data\n",
    "                device = next(model.parameters()).device\n",
    "                input_ids = torch.randint(\n",
    "                    0, config['vocab_size'], \n",
    "                    (config['batch_size'], config['seq_len']),\n",
    "                    device=device\n",
    "                )\n",
    "                labels = torch.randint(\n",
    "                    0, config['vocab_size'],\n",
    "                    (config['batch_size'], config['seq_len']),\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                profiler.snapshot(\"Data prepared\")\n",
    "                \n",
    "                # Measure forward pass\n",
    "                with profiler.profile_block(\"Forward pass\"):\n",
    "                    start_time = time.time()\n",
    "                    logits = model(input_ids, use_checkpointing=use_checkpointing)\n",
    "                    forward_time = time.time() - start_time\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = nn.functional.cross_entropy(\n",
    "                        logits.view(-1, config['vocab_size']),\n",
    "                        labels.view(-1)\n",
    "                    )\n",
    "                \n",
    "                # Measure backward pass\n",
    "                with profiler.profile_block(\"Backward pass\"):\n",
    "                    start_time = time.time()\n",
    "                    loss.backward()\n",
    "                    backward_time = time.time() - start_time\n",
    "                \n",
    "                profiler.snapshot(\"Training complete\")\n",
    "                \n",
    "                # Store results\n",
    "                peak_memory = profiler.get_peak_memory_mb()\n",
    "                config_results[mode] = {\n",
    "                    \"peak_memory_mb\": peak_memory,\n",
    "                    \"forward_time_ms\": forward_time * 1000,\n",
    "                    \"backward_time_ms\": backward_time * 1000,\n",
    "                    \"total_time_ms\": (forward_time + backward_time) * 1000,\n",
    "                    \"loss_value\": loss.item(),\n",
    "                    \"profiler\": profiler\n",
    "                }\n",
    "                \n",
    "                print(f\"     ‚úÖ Peak Memory: {peak_memory:.1f} MB\")\n",
    "                print(f\"     ‚è±Ô∏è  Forward: {forward_time*1000:.1f} ms, Backward: {backward_time*1000:.1f} ms\")\n",
    "                print(f\"     üìâ Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del model, optimizer, logits, loss\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Failed: {e}\")\n",
    "                config_results[mode] = {\"error\": str(e)}\n",
    "                \n",
    "                # Emergency cleanup\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        results[config['name']] = config_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_memory_results(results: Dict[str, Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Analyze and visualize memory optimization results\n",
    "    \n",
    "    Educational Focus:\n",
    "    This analysis demonstrates how to quantify optimization benefits\n",
    "    and make data-driven decisions about memory techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Memory Optimization Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    config_names = []\n",
    "    standard_memory = []\n",
    "    checkpointed_memory = []\n",
    "    memory_savings = []\n",
    "    \n",
    "    standard_time = []\n",
    "    checkpointed_time = []\n",
    "    time_overhead = []\n",
    "    \n",
    "    for config_name, config_results in results.items():\n",
    "        if (\"Standard Training\" in config_results and \n",
    "            \"Gradient Checkpointing\" in config_results and\n",
    "            \"error\" not in config_results[\"Standard Training\"] and\n",
    "            \"error\" not in config_results[\"Gradient Checkpointing\"]):\n",
    "            \n",
    "            std_result = config_results[\"Standard Training\"]\n",
    "            chk_result = config_results[\"Gradient Checkpointing\"]\n",
    "            \n",
    "            config_names.append(config_name)\n",
    "            \n",
    "            # Memory analysis\n",
    "            std_mem = std_result[\"peak_memory_mb\"]\n",
    "            chk_mem = chk_result[\"peak_memory_mb\"]\n",
    "            \n",
    "            standard_memory.append(std_mem)\n",
    "            checkpointed_memory.append(chk_mem)\n",
    "            \n",
    "            savings = ((std_mem - chk_mem) / std_mem) * 100 if std_mem > 0 else 0\n",
    "            memory_savings.append(savings)\n",
    "            \n",
    "            # Time analysis\n",
    "            std_time = std_result[\"total_time_ms\"]\n",
    "            chk_time = chk_result[\"total_time_ms\"]\n",
    "            \n",
    "            standard_time.append(std_time)\n",
    "            checkpointed_time.append(chk_time)\n",
    "            \n",
    "            overhead = ((chk_time - std_time) / std_time) * 100 if std_time > 0 else 0\n",
    "            time_overhead.append(overhead)\n",
    "            \n",
    "            print(f\"\\nüîç {config_name}:\")\n",
    "            print(f\"   Memory Reduction: {savings:.1f}% ({std_mem:.1f} ‚Üí {chk_mem:.1f} MB)\")\n",
    "            print(f\"   Time Overhead: {overhead:.1f}% ({std_time:.1f} ‚Üí {chk_time:.1f} ms)\")\n",
    "    \n",
    "    # Create visualization\n",
    "    if config_names:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('üß† Memory Optimization Analysis: Standard vs Gradient Checkpointing', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Memory Usage Comparison\n",
    "        x = np.arange(len(config_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax1.bar(x - width/2, standard_memory, width, label='Standard Training', \n",
    "                       color='#FF6B6B', alpha=0.8)\n",
    "        bars2 = ax1.bar(x + width/2, checkpointed_memory, width, label='Gradient Checkpointing',\n",
    "                       color='#4ECDC4', alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Model Configuration')\n",
    "        ax1.set_ylabel('Peak Memory (MB)')\n",
    "        ax1.set_title('üíæ Memory Usage Comparison')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(config_names)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2., height + max(standard_memory + checkpointed_memory)*0.01,\n",
    "                        f'{height:.0f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 2. Memory Savings\n",
    "        bars3 = ax2.bar(config_names, memory_savings, color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "        ax2.set_ylabel('Memory Savings (%)')\n",
    "        ax2.set_title('üìà Memory Reduction Achieved')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, savings in zip(bars3, memory_savings):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(memory_savings)*0.01,\n",
    "                    f'{savings:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 3. Time Comparison\n",
    "        bars4 = ax3.bar(x - width/2, standard_time, width, label='Standard Training',\n",
    "                       color='#FF6B6B', alpha=0.8)\n",
    "        bars5 = ax3.bar(x + width/2, checkpointed_time, width, label='Gradient Checkpointing',\n",
    "                       color='#4ECDC4', alpha=0.8)\n",
    "        \n",
    "        ax3.set_xlabel('Model Configuration')\n",
    "        ax3.set_ylabel('Total Time (ms)')\n",
    "        ax3.set_title('‚è±Ô∏è Training Time Comparison')\n",
    "        ax3.set_xticks(x)\n",
    "        ax3.set_xticklabels(config_names)\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Time Overhead\n",
    "        bars6 = ax4.bar(config_names, time_overhead, color='#F8C471', alpha=0.8, edgecolor='black')\n",
    "        ax4.set_ylabel('Time Overhead (%)')\n",
    "        ax4.set_title('‚ö° Computational Overhead')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, overhead in zip(bars6, time_overhead):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + max(time_overhead)*0.01,\n",
    "                    f'{overhead:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        if memory_savings and time_overhead:\n",
    "            avg_memory_savings = np.mean(memory_savings)\n",
    "            avg_time_overhead = np.mean(time_overhead)\n",
    "            \n",
    "            print(f\"\\nüéØ Summary Statistics:\")\n",
    "            print(f\"   Average Memory Savings: {avg_memory_savings:.1f}%\")\n",
    "            print(f\"   Average Time Overhead: {avg_time_overhead:.1f}%\")\n",
    "            \n",
    "            # Efficiency analysis\n",
    "            efficiency_ratio = avg_memory_savings / avg_time_overhead if avg_time_overhead > 0 else float('inf')\n",
    "            print(f\"   Efficiency Ratio: {efficiency_ratio:.2f} (memory saved per % time cost)\")\n",
    "            \n",
    "            if avg_memory_savings > 20 and avg_time_overhead < 50:\n",
    "                print(\"   ‚úÖ Gradient checkpointing is highly beneficial!\")\n",
    "            elif avg_memory_savings > 10:\n",
    "                print(\"   üü° Gradient checkpointing provides moderate benefits\")\n",
    "            else:\n",
    "                print(\"   üî¥ Gradient checkpointing benefits are limited for this configuration\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No successful results to analyze\")\n",
    "\n",
    "# Run the comprehensive memory experiment\n",
    "print(\"üöÄ Starting Comprehensive Memory Optimization Experiment\")\n",
    "print(\"This will compare standard training vs gradient checkpointing...\")\n",
    "\n",
    "memory_results = memory_comparison_experiment()\n",
    "analyze_memory_results(memory_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Advanced Memory Optimization Techniques\n",
    "\n",
    "### **Selective Activation Checkpointing**\n",
    "\n",
    "Not all layers benefit equally from checkpointing. Let's implement intelligent checkpointing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveCheckpointingStrategy:\n",
    "    \"\"\"\n",
    "    Intelligent checkpointing that selects optimal layers for recomputation\n",
    "    \n",
    "    Educational Focus:\n",
    "    This demonstrates advanced optimization where we analyze each layer's\n",
    "    memory vs compute characteristics to make optimal decisions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, memory_budget_mb: float = 8000):\n",
    "        self.model = model\n",
    "        self.memory_budget_mb = memory_budget_mb\n",
    "        self.layer_profiles = {}\n",
    "        self.optimal_strategy = None\n",
    "    \n",
    "    def profile_layers(self, sample_input: torch.Tensor) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Profile each layer to understand memory vs compute characteristics\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping layer names to their resource profiles\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîç Profiling individual layers for optimization strategy...\")\n",
    "        \n",
    "        self.layer_profiles = {}\n",
    "        \n",
    "        # Hook to capture activations and measure memory\n",
    "        activation_sizes = {}\n",
    "        computation_times = {}\n",
    "        \n",
    "        def memory_hook(name):\n",
    "            def hook_fn(module, input, output):\n",
    "                if torch.cuda.is_available():\n",
    "                    # Estimate activation memory size\n",
    "                    if isinstance(output, torch.Tensor):\n",
    "                        size_mb = output.numel() * output.element_size() / (1024**2)\n",
    "                        activation_sizes[name] = size_mb\n",
    "                    elif isinstance(output, (tuple, list)):\n",
    "                        total_size = sum(o.numel() * o.element_size() for o in output if isinstance(o, torch.Tensor))\n",
    "                        activation_sizes[name] = total_size / (1024**2)\n",
    "            return hook_fn\n",
    "        \n",
    "        # Register hooks for all layers\n",
    "        hooks = []\n",
    "        layer_names = []\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LayerNorm, SimpleTransformerLayer)):\n",
    "                hook = module.register_forward_hook(memory_hook(name))\n",
    "                hooks.append(hook)\n",
    "                layer_names.append(name)\n",
    "        \n",
    "        try:\n",
    "            # Profile forward pass\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                output = self.model(sample_input)\n",
    "                total_time = time.time() - start_time\n",
    "            \n",
    "            # Estimate computation time per layer (simplified)\n",
    "            estimated_time_per_layer = total_time / len([n for n in layer_names if 'layers.' in n])\n",
    "            \n",
    "            # Build layer profiles\n",
    "            for name in layer_names:\n",
    "                if name in activation_sizes:\n",
    "                    # Calculate memory-to-compute ratio (higher = better candidate for checkpointing)\n",
    "                    memory_mb = activation_sizes[name]\n",
    "                    compute_cost = estimated_time_per_layer  # Simplified estimate\n",
    "                    \n",
    "                    memory_to_compute_ratio = memory_mb / (compute_cost * 1000) if compute_cost > 0 else 0\n",
    "                    \n",
    "                    self.layer_profiles[name] = {\n",
    "                        'activation_memory_mb': memory_mb,\n",
    "                        'estimated_compute_ms': compute_cost * 1000,\n",
    "                        'memory_compute_ratio': memory_to_compute_ratio,\n",
    "                        'checkpoint_priority': memory_to_compute_ratio  # Higher = should checkpoint\n",
    "                    }\n",
    "        \n",
    "        finally:\n",
    "            # Clean up hooks\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "        \n",
    "        return self.layer_profiles\n",
    "    \n",
    "    def compute_optimal_strategy(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Compute optimal checkpointing strategy based on memory budget\n",
    "        \n",
    "        Returns:\n",
    "            List of layer names that should use checkpointing\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.layer_profiles:\n",
    "            print(\"‚ö†Ô∏è No layer profiles available. Run profile_layers() first.\")\n",
    "            return []\n",
    "        \n",
    "        # Sort layers by checkpoint priority (memory-to-compute ratio)\n",
    "        sorted_layers = sorted(\n",
    "            self.layer_profiles.items(),\n",
    "            key=lambda x: x[1]['checkpoint_priority'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Greedy selection based on memory budget\n",
    "        total_memory_saved = 0\n",
    "        selected_layers = []\n",
    "        \n",
    "        for layer_name, profile in sorted_layers:\n",
    "            memory_saving = profile['activation_memory_mb']\n",
    "            \n",
    "            if total_memory_saved + memory_saving <= self.memory_budget_mb:\n",
    "                selected_layers.append(layer_name)\n",
    "                total_memory_saved += memory_saving\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        self.optimal_strategy = selected_layers\n",
    "        \n",
    "        print(f\"\\nüéØ Optimal Checkpointing Strategy:\")\n",
    "        print(f\"   Layers to checkpoint: {len(selected_layers)}\")\n",
    "        print(f\"   Estimated memory saved: {total_memory_saved:.1f} MB\")\n",
    "        \n",
    "        # Show top candidates\n",
    "        print(f\"\\n   Top checkpoint candidates:\")\n",
    "        for i, (layer_name, profile) in enumerate(sorted_layers[:5]):\n",
    "            selected = \"‚úÖ\" if layer_name in selected_layers else \"‚ùå\"\n",
    "            print(f\"   {i+1}. {selected} {layer_name}: {profile['activation_memory_mb']:.1f} MB, \"\n",
    "                  f\"Ratio: {profile['memory_compute_ratio']:.2f}\")\n",
    "        \n",
    "        return selected_layers\n",
    "    \n",
    "    def print_analysis(self):\n",
    "        \"\"\"Print detailed analysis of checkpointing opportunities\"\"\"\n",
    "        \n",
    "        if not self.layer_profiles:\n",
    "            print(\"‚ùå No layer profiles available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìä Layer-by-Layer Analysis:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Layer Name':<30} {'Memory (MB)':<12} {'Compute (ms)':<12} {'Ratio':<8} {'Priority'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Sort by memory usage\n",
    "        sorted_layers = sorted(\n",
    "            self.layer_profiles.items(),\n",
    "            key=lambda x: x[1]['activation_memory_mb'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        for layer_name, profile in sorted_layers:\n",
    "            priority = \"High\" if profile['checkpoint_priority'] > 1.0 else \"Medium\" if profile['checkpoint_priority'] > 0.5 else \"Low\"\n",
    "            print(f\"{layer_name:<30} {profile['activation_memory_mb']:>8.1f}     \"\n",
    "                  f\"{profile['estimated_compute_ms']:>8.1f}     \"\n",
    "                  f\"{profile['memory_compute_ratio']:>6.2f}   {priority}\")\n",
    "\n",
    "# Demonstrate adaptive checkpointing\n",
    "def demonstrate_adaptive_checkpointing():\n",
    "    \"\"\"\n",
    "    Demonstrate adaptive checkpointing strategy selection\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß† Adaptive Checkpointing Strategy Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Create a model for analysis\n",
    "        model = SimpleTransformer(\n",
    "            vocab_size=10000,\n",
    "            hidden_dim=768,\n",
    "            num_layers=8,\n",
    "            num_heads=12,\n",
    "            max_seq_len=512\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        \n",
    "        # Create sample input\n",
    "        device = next(model.parameters()).device\n",
    "        sample_input = torch.randint(0, 10000, (2, 256), device=device)\n",
    "        \n",
    "        # Initialize adaptive strategy\n",
    "        strategy = AdaptiveCheckpointingStrategy(model, memory_budget_mb=500)\n",
    "        \n",
    "        # Profile layers\n",
    "        layer_profiles = strategy.profile_layers(sample_input)\n",
    "        \n",
    "        # Print analysis\n",
    "        strategy.print_analysis()\n",
    "        \n",
    "        # Compute optimal strategy\n",
    "        optimal_layers = strategy.compute_optimal_strategy()\n",
    "        \n",
    "        print(f\"\\nüéØ Adaptive checkpointing analysis complete!\")\n",
    "        print(f\"   Analyzed {len(layer_profiles)} layers\")\n",
    "        print(f\"   Recommended {len(optimal_layers)} layers for checkpointing\")\n",
    "        \n",
    "        return strategy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Adaptive checkpointing demonstration failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run adaptive checkpointing demonstration\n",
    "adaptive_strategy = demonstrate_adaptive_checkpointing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways from Memory Optimization\n",
    "\n",
    "### **Memory is the Critical Constraint**\n",
    "- **Activations dominate** memory usage in transformer training\n",
    "- **Quadratic attention** creates severe memory pressure\n",
    "- **Batch size** is often limited by memory, not compute\n",
    "\n",
    "### **Gradient Checkpointing Trade-offs**\n",
    "- **Memory reduction**: 20-60% typical savings\n",
    "- **Compute overhead**: 20-40% additional time\n",
    "- **Net benefit**: Enables larger models/batches\n",
    "- **Sweet spot**: Models with 6+ layers\n",
    "\n",
    "### **Optimization Strategy**\n",
    "- **Profile first**: Understand your memory bottlenecks\n",
    "- **Selective checkpointing**: Not all layers need it\n",
    "- **Memory-compute analysis**: Optimize based on actual ratios\n",
    "- **Adaptive strategies**: Adjust based on hardware constraints\n",
    "\n",
    "### **Production Implications**\n",
    "- **Training efficiency**: More data per GPU hour\n",
    "- **Cost reduction**: Fewer GPUs needed for same model\n",
    "- **Scalability**: Enables larger context lengths\n",
    "- **Flexibility**: Better resource utilization\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Advanced Memory Techniques\n",
    "\n",
    "### **Dynamic Memory Allocation**\n",
    "```python\n",
    "# Sequence length adaptive batching\n",
    "def adaptive_batch_size(seq_lengths, memory_budget):\n",
    "    # Adjust batch size based on sequence length distribution\n",
    "    max_seq_len = max(seq_lengths)\n",
    "    memory_per_sample = estimate_memory(max_seq_len)\n",
    "    return min(memory_budget // memory_per_sample, len(seq_lengths))\n",
    "```\n",
    "\n",
    "### **Activation Compression**\n",
    "```python\n",
    "# Store activations in lower precision during checkpointing\n",
    "def compressed_checkpoint(func, *args):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        return checkpoint.checkpoint(func, *args)\n",
    "```\n",
    "\n",
    "### **Memory-Mapped Activations**\n",
    "```python\n",
    "# Offload activations to CPU memory or disk\n",
    "def cpu_offload_checkpoint(func, *args):\n",
    "    # Move activations to CPU during storage\n",
    "    pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Exercises\n",
    "\n",
    "### **Exercise 1: Custom Checkpointing**\n",
    "Implement a custom checkpointing strategy that selectively checkpoints attention vs feed-forward layers.\n",
    "\n",
    "### **Exercise 2: Memory Budget Optimizer**\n",
    "Create a system that automatically determines optimal batch sizes given hardware memory constraints.\n",
    "\n",
    "### **Exercise 3: Sequence Length Analysis**\n",
    "Analyze how memory usage scales with sequence length and implement dynamic batching.\n",
    "\n",
    "---\n",
    "\n",
    "**Next: Chapter 4 - DeepSpeed ZeRO Deep Dive** ‚ö°\n",
    "\n",
    "*In the next chapter, we'll explore DeepSpeed ZeRO's revolutionary approach to parameter partitioning and how it enables training models that don't fit on single GPUs.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}