{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡ Chapter 5: Mixed Precision Training Mastery\n",
    "\n",
    "## 🧮 Theoretical Foundations of Mixed Precision Computing\n",
    "\n",
    "### Understanding Numerical Precision in Deep Learning\n",
    "\n",
    "Mixed precision training represents one of the most significant advances in deep learning optimization, enabling 1.5-2x speedup while reducing memory usage by up to 50%. This chapter provides comprehensive theoretical understanding and practical implementation of FP16, BF16, and emerging FP8 training.\n",
    "\n",
    "### Precision Format Analysis\n",
    "\n",
    "#### **FP32 (Single Precision)**\n",
    "```\n",
    "Sign | Exponent (8 bits) | Mantissa (23 bits)\n",
    " 1   |    8 bits         |    23 bits\n",
    "```\n",
    "- **Range**: ±3.4 × 10^38\n",
    "- **Precision**: ~7 decimal digits\n",
    "- **Memory**: 4 bytes per parameter\n",
    "\n",
    "#### **FP16 (Half Precision)**\n",
    "```\n",
    "Sign | Exponent (5 bits) | Mantissa (10 bits)\n",
    " 1   |    5 bits         |    10 bits\n",
    "```\n",
    "- **Range**: ±65,504 (limited!)\n",
    "- **Precision**: ~3 decimal digits\n",
    "- **Memory**: 2 bytes per parameter\n",
    "- **Challenge**: Gradient underflow\n",
    "\n",
    "#### **BF16 (Brain Float 16)**\n",
    "```\n",
    "Sign | Exponent (8 bits) | Mantissa (7 bits)\n",
    " 1   |    8 bits         |    7 bits\n",
    "```\n",
    "- **Range**: Same as FP32 (±3.4 × 10^38)\n",
    "- **Precision**: ~2-3 decimal digits\n",
    "- **Memory**: 2 bytes per parameter\n",
    "- **Advantage**: No gradient scaling needed\n",
    "\n",
    "#### **FP8 (8-bit Floating Point)**\n",
    "```\n",
    "E4M3: Sign | Exponent (4 bits) | Mantissa (3 bits)\n",
    "E5M2: Sign | Exponent (5 bits) | Mantissa (2 bits)\n",
    "```\n",
    "- **Range**: E4M3 ±448, E5M2 ±57,344\n",
    "- **Memory**: 1 byte per parameter\n",
    "- **Status**: Experimental, H100+ support\n",
    "\n",
    "### Mathematical Framework for Gradient Scaling\n",
    "\n",
    "**Loss Scaling Formula:**\n",
    "```\n",
    "scaled_loss = loss × scale_factor\n",
    "scaled_gradients = ∇(scaled_loss) = scale_factor × ∇(loss)\n",
    "true_gradients = scaled_gradients / scale_factor\n",
    "```\n",
    "\n",
    "**Dynamic Scaling Algorithm:**\n",
    "```\n",
    "if has_inf_or_nan(gradients):\n",
    "    scale_factor = scale_factor / backoff_factor\n",
    "    skip_update()\n",
    "else:\n",
    "    if consecutive_successful_steps > growth_interval:\n",
    "        scale_factor = scale_factor × growth_factor\n",
    "    apply_gradients(true_gradients)\n",
    "```\n",
    "\n",
    "### Tensor Core Optimization\n",
    "\n",
    "Modern GPUs (V100, A100, H100) include specialized **Tensor Cores** that provide massive acceleration for mixed-precision operations:\n",
    "\n",
    "- **V100**: 125 TFLOPS (FP16)\n",
    "- **A100**: 312 TFLOPS (BF16), 624 TFLOPS (FP16)\n",
    "- **H100**: 989 TFLOPS (BF16), 1978 TFLOPS (FP8)\n",
    "\n",
    "**Tensor Core Requirements:**\n",
    "1. Matrix dimensions must be multiples of 8 (FP16/BF16) or 16 (FP8)\n",
    "2. Contiguous memory layout required\n",
    "3. Specific CUDA operations (GEMM, Conv2D, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 Hands-On Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies for mixed precision training implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import struct\n",
    "from enum import Enum\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"⚡ Mixed Precision Training Mastery Environment Ready!\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {device_props.name}\")\n",
    "    print(f\"CUDA Compute Capability: {device_props.major}.{device_props.minor}\")\n",
    "    print(f\"Tensor Core Support: {'✅' if device_props.major >= 7 else '❌'}\")\n",
    "    print(f\"BF16 Support: {'✅' if device_props.major >= 8 else '❌'}\")\n",
    "    \n",
    "    # Check for specific precision support\n",
    "    print(f\"\\nPrecision Format Support:\")\n",
    "    print(f\"  • FP32: ✅ (Always available)\")\n",
    "    print(f\"  • FP16: {'✅' if torch.cuda.is_available() else '❌'}\")\n",
    "    print(f\"  • BF16: {'✅' if torch.cuda.is_bf16_supported() else '❌'}\")\n",
    "    \n",
    "    # Memory info\n",
    "    print(f\"\\nGPU Memory: {device_props.total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"🔸 CUDA not available - using CPU for demonstrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Precision Format Analysis and Comparison\n",
    "\n",
    "### Understanding Numerical Behavior Across Formats\n",
    "\n",
    "This section implements comprehensive analysis tools to understand how different precision formats behave with real neural network operations, gradients, and numerical stability considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionFormat(Enum):\n",
    "    \"\"\"Enumeration of supported precision formats.\"\"\"\n",
    "    FP32 = \"float32\"\n",
    "    FP16 = \"float16\" \n",
    "    BF16 = \"bfloat16\"\n",
    "    FP64 = \"float64\"  # For reference\n",
    "\n",
    "@dataclass\n",
    "class PrecisionAnalysisConfig:\n",
    "    \"\"\"Configuration for precision analysis experiments.\"\"\"\n",
    "    formats: List[PrecisionFormat] = None\n",
    "    test_ranges: List[Tuple[float, float]] = None\n",
    "    num_samples: int = 10000\n",
    "    gradient_scaling: bool = True\n",
    "    tensor_core_aligned: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.formats is None:\n",
    "            self.formats = [PrecisionFormat.FP32, PrecisionFormat.FP16]\n",
    "            if torch.cuda.is_bf16_supported():\n",
    "                self.formats.append(PrecisionFormat.BF16)\n",
    "        \n",
    "        if self.test_ranges is None:\n",
    "            self.test_ranges = [\n",
    "                (1e-8, 1e-6),   # Very small gradients\n",
    "                (1e-6, 1e-4),   # Small gradients  \n",
    "                (1e-4, 1e-2),   # Normal gradients\n",
    "                (1e-2, 1.0),    # Large gradients\n",
    "                (1.0, 100.0),   # Very large values\n",
    "            ]\n",
    "\n",
    "class PrecisionAnalyzer:\n",
    "    \"\"\"Comprehensive analyzer for mixed precision training behavior.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PrecisionAnalysisConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Results storage\n",
    "        self.analysis_results = {}\n",
    "        \n",
    "    def analyze_numerical_precision(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze numerical precision characteristics of different formats.\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for precision_format in self.config.formats:\n",
    "            format_results = {\n",
    "                'representable_range': self._analyze_representable_range(precision_format),\n",
    "                'precision_loss': self._analyze_precision_loss(precision_format),\n",
    "                'gradient_underflow': self._analyze_gradient_underflow(precision_format),\n",
    "                'overflow_behavior': self._analyze_overflow_behavior(precision_format),\n",
    "                'tensor_core_efficiency': self._analyze_tensor_core_efficiency(precision_format)\n",
    "            }\n",
    "            \n",
    "            results[precision_format.value] = format_results\n",
    "        \n",
    "        self.analysis_results['numerical_precision'] = results\n",
    "        return results\n",
    "    \n",
    "    def _analyze_representable_range(self, precision_format: PrecisionFormat) -> Dict[str, float]:\n",
    "        \"\"\"Analyze the representable range of a precision format.\"\"\"\n",
    "        \n",
    "        dtype = getattr(torch, precision_format.value)\n",
    "        \n",
    "        # Create test tensor\n",
    "        test_values = torch.logspace(-10, 10, 1000, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        # Convert to target precision and back\n",
    "        converted = test_values.to(dtype).to(torch.float32)\n",
    "        \n",
    "        # Find representable range\n",
    "        is_finite = torch.isfinite(converted)\n",
    "        is_nonzero = (converted != 0.0)\n",
    "        \n",
    "        valid_values = converted[is_finite & is_nonzero]\n",
    "        \n",
    "        if len(valid_values) > 0:\n",
    "            min_representable = valid_values.min().item()\n",
    "            max_representable = valid_values.max().item()\n",
    "        else:\n",
    "            min_representable = float('nan')\n",
    "            max_representable = float('nan')\n",
    "        \n",
    "        # Calculate precision (smallest representable difference)\n",
    "        eps_test = torch.tensor([1.0], device=self.device, dtype=dtype)\n",
    "        eps = torch.finfo(eps_test.dtype).eps if hasattr(torch.finfo(eps_test.dtype), 'eps') else 1e-7\n",
    "        \n",
    "        return {\n",
    "            'min_representable': min_representable,\n",
    "            'max_representable': max_representable,\n",
    "            'machine_epsilon': float(eps),\n",
    "            'dynamic_range_db': 20 * np.log10(max_representable / min_representable) if min_representable > 0 else float('inf')\n",
    "        }\n",
    "    \n",
    "    def _analyze_precision_loss(self, precision_format: PrecisionFormat) -> Dict[str, float]:\n",
    "        \"\"\"Analyze precision loss through format conversion.\"\"\"\n",
    "        \n",
    "        dtype = getattr(torch, precision_format.value)\n",
    "        \n",
    "        # Generate test data across different ranges\n",
    "        precision_errors = []\n",
    "        \n",
    "        for min_val, max_val in self.config.test_ranges:\n",
    "            # Generate random values in range\n",
    "            original = torch.rand(1000, device=self.device) * (max_val - min_val) + min_val\n",
    "            \n",
    "            # Convert to target precision and back\n",
    "            converted = original.to(dtype).to(torch.float32)\n",
    "            \n",
    "            # Calculate relative error\n",
    "            valid_mask = (original != 0) & torch.isfinite(converted)\n",
    "            if valid_mask.sum() > 0:\n",
    "                relative_error = torch.abs((converted - original) / original)[valid_mask]\n",
    "                precision_errors.extend(relative_error.cpu().numpy())\n",
    "        \n",
    "        if precision_errors:\n",
    "            precision_errors = np.array(precision_errors)\n",
    "            return {\n",
    "                'mean_relative_error': float(np.mean(precision_errors)),\n",
    "                'max_relative_error': float(np.max(precision_errors)),\n",
    "                'std_relative_error': float(np.std(precision_errors)),\n",
    "                'p95_relative_error': float(np.percentile(precision_errors, 95)),\n",
    "                'p99_relative_error': float(np.percentile(precision_errors, 99))\n",
    "            }\n",
    "        else:\n",
    "            return {'error': 'No valid precision measurements'}\n",
    "    \n",
    "    def _analyze_gradient_underflow(self, precision_format: PrecisionFormat) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze gradient underflow behavior.\"\"\"\n",
    "        \n",
    "        dtype = getattr(torch, precision_format.value)\n",
    "        \n",
    "        # Simulate typical gradient magnitudes\n",
    "        gradient_magnitudes = torch.logspace(-10, -1, 1000, device=self.device)\n",
    "        \n",
    "        # Convert gradients to target precision\n",
    "        converted_gradients = gradient_magnitudes.to(dtype)\n",
    "        \n",
    "        # Analyze underflow\n",
    "        underflow_mask = (converted_gradients == 0.0) & (gradient_magnitudes != 0.0)\n",
    "        finite_mask = torch.isfinite(converted_gradients) & (converted_gradients != 0.0)\n",
    "        \n",
    "        underflow_threshold = None\n",
    "        if underflow_mask.sum() > 0:\n",
    "            # Find the largest gradient that underflows\n",
    "            underflow_gradients = gradient_magnitudes[underflow_mask]\n",
    "            if len(underflow_gradients) > 0:\n",
    "                underflow_threshold = underflow_gradients.max().item()\n",
    "        \n",
    "        return {\n",
    "            'underflow_rate': float(underflow_mask.sum() / len(gradient_magnitudes)),\n",
    "            'finite_gradient_rate': float(finite_mask.sum() / len(gradient_magnitudes)),\n",
    "            'underflow_threshold': underflow_threshold,\n",
    "            'recommended_loss_scaling': self._calculate_recommended_scaling(precision_format, underflow_threshold)\n",
    "        }\n",
    "    \n",
    "    def _analyze_overflow_behavior(self, precision_format: PrecisionFormat) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze overflow behavior with different value ranges.\"\"\"\n",
    "        \n",
    "        dtype = getattr(torch, precision_format.value)\n",
    "        \n",
    "        # Test large values\n",
    "        large_values = torch.logspace(1, 6, 1000, device=self.device)\n",
    "        converted_values = large_values.to(dtype)\n",
    "        \n",
    "        # Analyze overflow\n",
    "        overflow_mask = ~torch.isfinite(converted_values)\n",
    "        \n",
    "        overflow_threshold = None\n",
    "        if overflow_mask.sum() > 0:\n",
    "            # Find the smallest value that overflows\n",
    "            overflow_values = large_values[overflow_mask]\n",
    "            if len(overflow_values) > 0:\n",
    "                overflow_threshold = overflow_values.min().item()\n",
    "        \n",
    "        return {\n",
    "            'overflow_rate': float(overflow_mask.sum() / len(large_values)),\n",
    "            'overflow_threshold': overflow_threshold,\n",
    "            'max_safe_value': float(large_values[~overflow_mask].max()) if (~overflow_mask).sum() > 0 else None\n",
    "        }\n",
    "    \n",
    "    def _analyze_tensor_core_efficiency(self, precision_format: PrecisionFormat) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze Tensor Core efficiency for different precision formats.\"\"\"\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            return {'error': 'CUDA not available for Tensor Core analysis'}\n",
    "        \n",
    "        dtype = getattr(torch, precision_format.value)\n",
    "        \n",
    "        # Test matrix sizes (Tensor Core optimized vs non-optimized)\n",
    "        test_sizes = [\n",
    "            (768, 768),    # Non-aligned\n",
    "            (768, 3072),   # Semi-aligned\n",
    "            (1024, 4096),  # Tensor Core aligned\n",
    "            (2048, 8192),  # Large aligned\n",
    "        ]\n",
    "        \n",
    "        performance_results = []\n",
    "        \n",
    "        for m, n in test_sizes:\n",
    "            # Create test matrices\n",
    "            if self.config.tensor_core_aligned:\n",
    "                # Ensure alignment for optimal Tensor Core usage\n",
    "                m = ((m + 7) // 8) * 8\n",
    "                n = ((n + 7) // 8) * 8\n",
    "            \n",
    "            a = torch.randn(m, n, device=self.device, dtype=dtype)\n",
    "            b = torch.randn(n, m, device=self.device, dtype=dtype)\n",
    "            \n",
    "            # Warm up\n",
    "            for _ in range(10):\n",
    "                _ = torch.mm(a, b)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Benchmark\n",
    "            start_time = time.time()\n",
    "            for _ in range(100):\n",
    "                result = torch.mm(a, b)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            avg_time = (end_time - start_time) / 100\n",
    "            \n",
    "            # Calculate FLOPS\n",
    "            flops = 2 * m * n * n  # Matrix multiplication FLOPS\n",
    "            tflops = (flops / avg_time) / 1e12\n",
    "            \n",
    "            performance_results.append({\n",
    "                'matrix_size': f'{m}x{n}',\n",
    "                'time_ms': avg_time * 1000,\n",
    "                'tflops': tflops,\n",
    "                'tensor_core_aligned': (m % 8 == 0) and (n % 8 == 0)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'performance_results': performance_results,\n",
    "            'max_tflops': max(r['tflops'] for r in performance_results),\n",
    "            'aligned_vs_unaligned_speedup': self._calculate_alignment_speedup(performance_results)\n",
    "        }\n",
    "    \n",
    "    def _calculate_recommended_scaling(self, precision_format: PrecisionFormat, underflow_threshold: Optional[float]) -> Optional[float]:\n",
    "        \"\"\"Calculate recommended loss scaling factor.\"\"\"\n",
    "        \n",
    "        if precision_format == PrecisionFormat.BF16:\n",
    "            # BF16 typically doesn't need gradient scaling\n",
    "            return 1.0\n",
    "        elif precision_format == PrecisionFormat.FP16 and underflow_threshold is not None:\n",
    "            # Scale to bring gradients into representable range\n",
    "            # Target: gradients around 1e-4 to 1e-2\n",
    "            target_gradient_magnitude = 1e-3\n",
    "            recommended_scaling = target_gradient_magnitude / underflow_threshold\n",
    "            \n",
    "            # Clamp to reasonable range\n",
    "            return min(max(recommended_scaling, 1.0), 65536.0)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _calculate_alignment_speedup(self, performance_results: List[Dict]) -> Optional[float]:\n",
    "        \"\"\"Calculate speedup from Tensor Core alignment.\"\"\"\n",
    "        \n",
    "        aligned_results = [r for r in performance_results if r['tensor_core_aligned']]\n",
    "        unaligned_results = [r for r in performance_results if not r['tensor_core_aligned']]\n",
    "        \n",
    "        if aligned_results and unaligned_results:\n",
    "            avg_aligned_tflops = np.mean([r['tflops'] for r in aligned_results])\n",
    "            avg_unaligned_tflops = np.mean([r['tflops'] for r in unaligned_results])\n",
    "            \n",
    "            return avg_aligned_tflops / avg_unaligned_tflops if avg_unaligned_tflops > 0 else None\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Initialize and run precision analysis\n",
    "print(\"🧪 Starting Comprehensive Precision Analysis...\")\n",
    "\n",
    "# Configure analysis\n",
    "config = PrecisionAnalysisConfig(\n",
    "    num_samples=5000,  # Reduce for faster execution in demo\n",
    "    gradient_scaling=True,\n",
    "    tensor_core_aligned=True\n",
    ")\n",
    "\n",
    "analyzer = PrecisionAnalyzer(config)\n",
    "\n",
    "# Run numerical precision analysis\n",
    "print(\"📊 Analyzing numerical precision characteristics...\")\n",
    "precision_results = analyzer.analyze_numerical_precision()\n",
    "\n",
    "print(\"\\n✅ Precision Analysis Complete!\")\n",
    "print(\"\\n📈 Summary Results:\")\n",
    "\n",
    "for format_name, results in precision_results.items():\n",
    "    print(f\"\\n{format_name.upper()}:\")\n",
    "    \n",
    "    # Representable range\n",
    "    range_info = results['representable_range']\n",
    "    print(f\"  • Range: {range_info['min_representable']:.2e} to {range_info['max_representable']:.2e}\")\n",
    "    print(f\"  • Dynamic Range: {range_info['dynamic_range_db']:.1f} dB\")\n",
    "    \n",
    "    # Precision loss\n",
    "    if 'mean_relative_error' in results['precision_loss']:\n",
    "        precision_info = results['precision_loss']\n",
    "        print(f\"  • Mean Precision Error: {precision_info['mean_relative_error']:.2e}\")\n",
    "        print(f\"  • P95 Precision Error: {precision_info['p95_relative_error']:.2e}\")\n",
    "    \n",
    "    # Gradient underflow\n",
    "    underflow_info = results['gradient_underflow']\n",
    "    print(f\"  • Gradient Underflow Rate: {underflow_info['underflow_rate']:.1%}\")\n",
    "    if underflow_info['recommended_loss_scaling']:\n",
    "        print(f\"  • Recommended Loss Scaling: {underflow_info['recommended_loss_scaling']:.0f}x\")\n",
    "    \n",
    "    # Tensor Core performance\n",
    "    if 'max_tflops' in results['tensor_core_efficiency']:\n",
    "        tc_info = results['tensor_core_efficiency']\n",
    "        print(f\"  • Peak Performance: {tc_info['max_tflops']:.1f} TFLOPS\")\n",
    "        if tc_info['aligned_vs_unaligned_speedup']:\n",
    "            print(f\"  • Alignment Speedup: {tc_info['aligned_vs_unaligned_speedup']:.1f}x\")\n",
    "\n",
    "print(f\"\\n🎯 Analysis complete for {len(config.formats)} precision formats!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔥 Advanced Mixed Precision Training Implementation\n",
    "\n",
    "### Production-Grade Mixed Precision System\n",
    "\n",
    "This section implements a comprehensive mixed precision training system with automatic loss scaling, gradient clipping, and performance optimization. The implementation demonstrates advanced techniques used in production LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMixedPrecisionTrainer:\n",
    "    \"\"\"Production-grade mixed precision trainer with advanced optimizations.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 precision_format: str = 'fp16',\n",
    "                 loss_scaling: str = 'dynamic',\n",
    "                 max_grad_norm: float = 1.0,\n",
    "                 device: Optional[torch.device] = None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.precision_format = precision_format\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Initialize gradient scaler for FP16\n",
    "        self.scaler = None\n",
    "        if precision_format == 'fp16':\n",
    "            if loss_scaling == 'dynamic':\n",
    "                self.scaler = GradScaler(\n",
    "                    init_scale=2**16,  # Initial scale\n",
    "                    growth_factor=2.0,  # Scale growth factor\n",
    "                    backoff_factor=0.5,  # Scale reduction factor\n",
    "                    growth_interval=2000,  # Steps before scale increase\n",
    "                    enabled=True\n",
    "                )\n",
    "            elif isinstance(loss_scaling, (int, float)):\n",
    "                self.scaler = GradScaler(init_scale=loss_scaling, growth_factor=1.0, \n",
    "                                       backoff_factor=1.0, growth_interval=float('inf'))\n",
    "        \n",
    "        # Training statistics\n",
    "        self.training_stats = {\n",
    "            'total_steps': 0,\n",
    "            'successful_steps': 0,\n",
    "            'overflow_steps': 0,\n",
    "            'scale_updates': [],\n",
    "            'gradient_norms': [],\n",
    "            'loss_values': [],\n",
    "            'step_times': [],\n",
    "            'memory_usage': []\n",
    "        }\n",
    "        \n",
    "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, Any]:\n",
    "        \"\"\"Perform a single training step with mixed precision.\"\"\"\n",
    "        \n",
    "        step_start_time = time.time()\n",
    "        \n",
    "        # Clear gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        if self.precision_format in ['fp16', 'bf16']:\n",
    "            with autocast(dtype=torch.float16 if self.precision_format == 'fp16' else torch.bfloat16):\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs['loss'] if isinstance(outputs, dict) else outputs\n",
    "        else:\n",
    "            outputs = self.model(**batch)\n",
    "            loss = outputs['loss'] if isinstance(outputs, dict) else outputs\n",
    "        \n",
    "        # Backward pass with scaling\n",
    "        if self.scaler is not None:\n",
    "            scaled_loss = self.scaler.scale(loss)\n",
    "            scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # Gradient processing and optimization\n",
    "        step_successful = self._optimize_step()\n",
    "        \n",
    "        # Update statistics\n",
    "        step_time = time.time() - step_start_time\n",
    "        self._update_statistics(loss.item(), step_time, step_successful)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'step_time': step_time,\n",
    "            'successful': step_successful,\n",
    "            'current_scale': self.scaler.get_scale() if self.scaler else 1.0,\n",
    "            'gradient_norm': self.training_stats['gradient_norms'][-1] if self.training_stats['gradient_norms'] else 0.0\n",
    "        }\n",
    "    \n",
    "    def _optimize_step(self) -> bool:\n",
    "        \"\"\"Perform optimization step with gradient scaling and clipping.\"\"\"\n",
    "        \n",
    "        if self.scaler is not None:\n",
    "            # Unscale gradients for gradient clipping\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            \n",
    "            # Check for inf/nan gradients\n",
    "            if self._has_inf_or_nan_gradients():\n",
    "                self.scaler.update()  # Skip step and update scale\n",
    "                return False\n",
    "        \n",
    "        # Calculate gradient norm before clipping\n",
    "        grad_norm = self._calculate_gradient_norm()\n",
    "        self.training_stats['gradient_norms'].append(grad_norm)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if self.max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        \n",
    "        # Optimizer step\n",
    "        if self.scaler is not None:\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _has_inf_or_nan_gradients(self) -> bool:\n",
    "        \"\"\"Check if gradients contain inf or nan values.\"\"\"\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def _calculate_gradient_norm(self) -> float:\n",
    "        \"\"\"Calculate the norm of gradients.\"\"\"\n",
    "        \n",
    "        total_norm = 0.0\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        \n",
    "        return total_norm ** 0.5\n",
    "    \n",
    "    def _update_statistics(self, loss_value: float, step_time: float, successful: bool):\n",
    "        \"\"\"Update training statistics.\"\"\"\n",
    "        \n",
    "        self.training_stats['total_steps'] += 1\n",
    "        self.training_stats['loss_values'].append(loss_value)\n",
    "        self.training_stats['step_times'].append(step_time)\n",
    "        \n",
    "        if successful:\n",
    "            self.training_stats['successful_steps'] += 1\n",
    "        else:\n",
    "            self.training_stats['overflow_steps'] += 1\n",
    "        \n",
    "        # Track scale updates\n",
    "        if self.scaler is not None:\n",
    "            current_scale = self.scaler.get_scale()\n",
    "            if not self.training_stats['scale_updates'] or self.training_stats['scale_updates'][-1] != current_scale:\n",
    "                self.training_stats['scale_updates'].append(current_scale)\n",
    "        \n",
    "        # Track memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            memory_mb = torch.cuda.memory_allocated() / 1e6\n",
    "            self.training_stats['memory_usage'].append(memory_mb)\n",
    "    \n",
    "    def get_training_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive training statistics.\"\"\"\n",
    "        \n",
    "        stats = self.training_stats.copy()\n",
    "        \n",
    "        if stats['total_steps'] > 0:\n",
    "            stats['success_rate'] = stats['successful_steps'] / stats['total_steps']\n",
    "            stats['overflow_rate'] = stats['overflow_steps'] / stats['total_steps']\n",
    "            stats['avg_step_time'] = np.mean(stats['step_times']) if stats['step_times'] else 0\n",
    "            stats['avg_loss'] = np.mean(stats['loss_values']) if stats['loss_values'] else 0\n",
    "            stats['avg_gradient_norm'] = np.mean(stats['gradient_norms']) if stats['gradient_norms'] else 0\n",
    "            stats['max_memory_mb'] = max(stats['memory_usage']) if stats['memory_usage'] else 0\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Create a simple transformer model for testing\n",
    "class SimpleTransformerLayer(nn.Module):\n",
    "    \"\"\"Simple transformer layer for mixed precision testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 1024, nhead: int = 16, dim_feedforward: int = 4096):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Self-attention with residual connection\n",
    "        attn_out, _ = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feedforward with residual connection\n",
    "        ff_out = self.feedforward(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TestTransformerModel(nn.Module):\n",
    "    \"\"\"Test transformer model for mixed precision experiments.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 32000, d_model: int = 1024, num_layers: int = 6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([SimpleTransformerLayer(d_model) for _ in range(num_layers)])\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        # Embedding\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        # Transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        output = {'logits': logits}\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Flatten for loss calculation\n",
    "            loss = self.criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            output['loss'] = loss\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test mixed precision training\n",
    "print(\"🔥 Testing Advanced Mixed Precision Training...\")\n",
    "\n",
    "# Create test model and data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TestTransformerModel(vocab_size=1000, d_model=512, num_layers=4)\n",
    "\n",
    "print(f\"📊 Model Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "# Test different precision formats\n",
    "precision_formats = ['fp32', 'fp16']\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    precision_formats.append('bf16')\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "for precision_format in precision_formats:\n",
    "    print(f\"\\n🧪 Testing {precision_format.upper()} Training...\")\n",
    "    \n",
    "    # Create fresh model and optimizer for each test\n",
    "    test_model = TestTransformerModel(vocab_size=1000, d_model=512, num_layers=4)\n",
    "    optimizer = torch.optim.AdamW(test_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = AdvancedMixedPrecisionTrainer(\n",
    "        model=test_model,\n",
    "        optimizer=optimizer,\n",
    "        precision_format=precision_format,\n",
    "        loss_scaling='dynamic' if precision_format == 'fp16' else None,\n",
    "        max_grad_norm=1.0,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Generate test batch\n",
    "    batch_size = 8\n",
    "    seq_length = 128\n",
    "    \n",
    "    # Run training steps\n",
    "    num_steps = 50\n",
    "    for step in range(num_steps):\n",
    "        # Generate random batch\n",
    "        input_ids = torch.randint(0, 1000, (batch_size, seq_length), device=device)\n",
    "        labels = torch.randint(0, 1000, (batch_size, seq_length), device=device)\n",
    "        \n",
    "        batch = {'input_ids': input_ids, 'labels': labels}\n",
    "        \n",
    "        # Training step\n",
    "        step_result = trainer.train_step(batch)\n",
    "        \n",
    "        # Print progress every 10 steps\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"  Step {step + 1}: Loss = {step_result['loss']:.4f}, \"\n",
    "                  f\"Time = {step_result['step_time']*1000:.1f}ms, \"\n",
    "                  f\"Scale = {step_result['current_scale']:.0f}\")\n",
    "    \n",
    "    # Get final statistics\n",
    "    final_stats = trainer.get_training_statistics()\n",
    "    training_results[precision_format] = final_stats\n",
    "    \n",
    "    print(f\"\\n📈 {precision_format.upper()} Results:\")\n",
    "    print(f\"  • Success Rate: {final_stats['success_rate']:.1%}\")\n",
    "    print(f\"  • Average Step Time: {final_stats['avg_step_time']*1000:.1f}ms\")\n",
    "    print(f\"  • Average Loss: {final_stats['avg_loss']:.4f}\")\n",
    "    print(f\"  • Average Gradient Norm: {final_stats['avg_gradient_norm']:.4f}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  • Peak Memory: {final_stats['max_memory_mb']:.1f}MB\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del test_model, optimizer, trainer\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"\\n✅ Mixed Precision Training Tests Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Comprehensive Performance Analysis and Visualization\n",
    "\n",
    "### Mixed Precision Training Comparison\n",
    "\n",
    "This section creates comprehensive visualizations comparing the performance, memory usage, and numerical behavior of different precision formats in realistic training scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mixed_precision_visualizations(precision_results: Dict, training_results: Dict):\n",
    "    \"\"\"Create comprehensive visualizations for mixed precision analysis.\"\"\"\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('⚡ Mixed Precision Training Comprehensive Analysis', fontsize=16, y=0.98)\n",
    "    \n",
    "    # 1. Numerical Precision Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    formats = list(precision_results.keys())\n",
    "    precision_errors = []\n",
    "    dynamic_ranges = []\n",
    "    \n",
    "    for fmt in formats:\n",
    "        if 'mean_relative_error' in precision_results[fmt]['precision_loss']:\n",
    "            precision_errors.append(precision_results[fmt]['precision_loss']['mean_relative_error'])\n",
    "        else:\n",
    "            precision_errors.append(0)\n",
    "        \n",
    "        dynamic_ranges.append(precision_results[fmt]['representable_range']['dynamic_range_db'])\n",
    "    \n",
    "    x = np.arange(len(formats))\n",
    "    ax1.bar(x, precision_errors, alpha=0.7, color=['blue', 'orange', 'green'][:len(formats)])\n",
    "    ax1.set_xlabel('Precision Format')\n",
    "    ax1.set_ylabel('Mean Relative Error')\n",
    "    ax1.set_title('Numerical Precision Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([fmt.upper() for fmt in formats])\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Training Performance Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    training_formats = list(training_results.keys())\n",
    "    step_times = [training_results[fmt]['avg_step_time'] * 1000 for fmt in training_formats]  # Convert to ms\n",
    "    memory_usage = [training_results[fmt].get('max_memory_mb', 0) for fmt in training_formats]\n",
    "    \n",
    "    x = np.arange(len(training_formats))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax2.bar(x - width/2, step_times, width, label='Step Time (ms)', alpha=0.7)\n",
    "    ax2_twin = ax2.twinx()\n",
    "    bars2 = ax2_twin.bar(x + width/2, memory_usage, width, label='Memory (MB)', alpha=0.7, color='orange')\n",
    "    \n",
    "    ax2.set_xlabel('Precision Format')\n",
    "    ax2.set_ylabel('Step Time (ms)', color='blue')\n",
    "    ax2_twin.set_ylabel('Memory Usage (MB)', color='orange')\n",
    "    ax2.set_title('Training Performance Comparison')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([fmt.upper() for fmt in training_formats])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars1, step_times):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Gradient Underflow Analysis\n",
    "    ax3 = axes[0, 2]\n",
    "    \n",
    "    underflow_rates = []\n",
    "    recommended_scalings = []\n",
    "    \n",
    "    for fmt in formats:\n",
    "        underflow_info = precision_results[fmt]['gradient_underflow']\n",
    "        underflow_rates.append(underflow_info['underflow_rate'])\n",
    "        scaling = underflow_info.get('recommended_loss_scaling')\n",
    "        recommended_scalings.append(scaling if scaling else 1.0)\n",
    "    \n",
    "    x = np.arange(len(formats))\n",
    "    ax3.bar(x, underflow_rates, alpha=0.7, color=['red', 'orange', 'green'][:len(formats)])\n",
    "    ax3.set_xlabel('Precision Format')\n",
    "    ax3.set_ylabel('Gradient Underflow Rate')\n",
    "    ax3.set_title('Gradient Underflow Analysis')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([fmt.upper() for fmt in formats])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add recommended scaling as text\n",
    "    for i, (rate, scaling) in enumerate(zip(underflow_rates, recommended_scalings)):\n",
    "        ax3.text(i, rate + 0.01, f'Scale: {scaling:.0f}x', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Training Success Rate and Convergence\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    success_rates = [training_results[fmt]['success_rate'] for fmt in training_formats]\n",
    "    avg_losses = [training_results[fmt]['avg_loss'] for fmt in training_formats]\n",
    "    \n",
    "    x = np.arange(len(training_formats))\n",
    "    bars = ax4.bar(x, success_rates, alpha=0.7, color=['blue', 'orange', 'green'][:len(training_formats)])\n",
    "    ax4.set_xlabel('Precision Format')\n",
    "    ax4.set_ylabel('Training Success Rate')\n",
    "    ax4.set_title('Training Stability Comparison')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels([fmt.upper() for fmt in training_formats])\n",
    "    ax4.set_ylim(0, 1.1)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, rate in zip(bars, success_rates):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # 5. Memory Efficiency Analysis\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    # Calculate memory savings relative to FP32\n",
    "    fp32_memory = memory_usage[0] if training_formats[0] == 'fp32' else max(memory_usage)\n",
    "    memory_savings = [(fp32_memory - mem) / fp32_memory * 100 for mem in memory_usage]\n",
    "    \n",
    "    bars = ax5.bar(range(len(training_formats)), memory_savings, \n",
    "                   alpha=0.7, color=['blue', 'orange', 'green'][:len(training_formats)])\n",
    "    ax5.set_xlabel('Precision Format')\n",
    "    ax5.set_ylabel('Memory Savings (%)')\n",
    "    ax5.set_title('Memory Efficiency Gains')\n",
    "    ax5.set_xticks(range(len(training_formats)))\n",
    "    ax5.set_xticklabels([fmt.upper() for fmt in training_formats])\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, saving in enumerate(memory_savings):\n",
    "        ax5.text(i, saving + 1, f'{saving:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 6. Tensor Core Performance Analysis\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    tflops_data = []\n",
    "    format_labels = []\n",
    "    \n",
    "    for fmt in formats:\n",
    "        if 'max_tflops' in precision_results[fmt]['tensor_core_efficiency']:\n",
    "            tflops = precision_results[fmt]['tensor_core_efficiency']['max_tflops']\n",
    "            tflops_data.append(tflops)\n",
    "            format_labels.append(fmt.upper())\n",
    "    \n",
    "    if tflops_data:\n",
    "        bars = ax6.bar(range(len(format_labels)), tflops_data, alpha=0.7,\n",
    "                      color=['blue', 'orange', 'green'][:len(format_labels)])\n",
    "        ax6.set_xlabel('Precision Format')\n",
    "        ax6.set_ylabel('Peak Performance (TFLOPS)')\n",
    "        ax6.set_title('Tensor Core Performance')\n",
    "        ax6.set_xticks(range(len(format_labels)))\n",
    "        ax6.set_xticklabels(format_labels)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add TFLOPS labels\n",
    "        for i, tflops in enumerate(tflops_data):\n",
    "            ax6.text(i, tflops + max(tflops_data) * 0.02, f'{tflops:.1f}', \n",
    "                    ha='center', va='bottom')\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'Tensor Core\\nAnalysis\\nNot Available', \n",
    "                ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
    "        ax6.set_title('Tensor Core Performance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def generate_mixed_precision_recommendations(precision_results: Dict, training_results: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Generate comprehensive recommendations for mixed precision training.\"\"\"\n",
    "    \n",
    "    recommendations = {\n",
    "        'format_recommendations': {},\n",
    "        'optimization_strategies': {},\n",
    "        'hardware_considerations': {},\n",
    "        'production_guidelines': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze each precision format\n",
    "    for fmt in precision_results.keys():\n",
    "        precision_info = precision_results[fmt]\n",
    "        training_info = training_results.get(fmt, {})\n",
    "        \n",
    "        format_rec = {\n",
    "            'memory_savings': '50%' if fmt in ['float16', 'bfloat16'] else '0%',\n",
    "            'speed_improvement': 'Up to 2x' if fmt in ['float16', 'bfloat16'] else 'Baseline',\n",
    "            'numerical_stability': 'High' if fmt == 'bfloat16' else 'Medium' if fmt == 'float16' else 'Highest',\n",
    "            'gradient_scaling_required': fmt == 'float16',\n",
    "            'tensor_core_support': fmt in ['float16', 'bfloat16'],\n",
    "            'recommended_use_cases': []\n",
    "        }\n",
    "        \n",
    "        # Use case recommendations\n",
    "        if fmt == 'float32':\n",
    "            format_rec['recommended_use_cases'] = [\n",
    "                'Debugging and development',\n",
    "                'Small models where memory is not a constraint',\n",
    "                'Research requiring highest numerical precision'\n",
    "            ]\n",
    "        elif fmt == 'float16':\n",
    "            format_rec['recommended_use_cases'] = [\n",
    "                'Large model training with V100/A100 GPUs',\n",
    "                'Memory-constrained environments',\n",
    "                'Production training with careful gradient scaling'\n",
    "            ]\n",
    "        elif fmt == 'bfloat16':\n",
    "            format_rec['recommended_use_cases'] = [\n",
    "                'Large model training with A100/H100 GPUs',\n",
    "                'Production training requiring stability',\n",
    "                'Training without gradient scaling complexity'\n",
    "            ]\n",
    "        \n",
    "        # Performance analysis\n",
    "        if training_info:\n",
    "            format_rec['training_stability'] = training_info.get('success_rate', 0)\n",
    "            format_rec['average_step_time_ms'] = training_info.get('avg_step_time', 0) * 1000\n",
    "        \n",
    "        recommendations['format_recommendations'][fmt] = format_rec\n",
    "    \n",
    "    # Optimization strategies\n",
    "    recommendations['optimization_strategies'] = {\n",
    "        'gradient_scaling': {\n",
    "            'fp16': {\n",
    "                'strategy': 'Dynamic scaling',\n",
    "                'initial_scale': 2**16,\n",
    "                'growth_factor': 2.0,\n",
    "                'backoff_factor': 0.5,\n",
    "                'growth_interval': 2000\n",
    "            },\n",
    "            'bf16': {\n",
    "                'strategy': 'No scaling required',\n",
    "                'reason': 'Same exponent range as FP32'\n",
    "            }\n",
    "        },\n",
    "        'gradient_clipping': {\n",
    "            'recommended_norm': 1.0,\n",
    "            'adaptive_clipping': True,\n",
    "            'clip_before_scaling': True\n",
    "        },\n",
    "        'tensor_core_optimization': {\n",
    "            'matrix_alignment': 'Multiple of 8 for FP16/BF16',\n",
    "            'memory_layout': 'Contiguous tensors required',\n",
    "            'operation_coverage': 'GEMM, Conv2D, BatchNorm'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Hardware considerations\n",
    "    recommendations['hardware_considerations'] = {\n",
    "        'V100': {\n",
    "            'fp16_support': 'Excellent',\n",
    "            'bf16_support': 'Not available',\n",
    "            'tensor_cores': 'First generation',\n",
    "            'recommended_format': 'FP16'\n",
    "        },\n",
    "        'A100': {\n",
    "            'fp16_support': 'Excellent',\n",
    "            'bf16_support': 'Excellent',\n",
    "            'tensor_cores': 'Third generation',\n",
    "            'recommended_format': 'BF16'\n",
    "        },\n",
    "        'H100': {\n",
    "            'fp16_support': 'Excellent',\n",
    "            'bf16_support': 'Excellent',\n",
    "            'fp8_support': 'Experimental',\n",
    "            'tensor_cores': 'Fourth generation',\n",
    "            'recommended_format': 'BF16 (FP8 for research)'\n",
    "        },\n",
    "        'T4': {\n",
    "            'fp16_support': 'Good',\n",
    "            'bf16_support': 'Not available',\n",
    "            'tensor_cores': 'Second generation',\n",
    "            'recommended_format': 'FP16 (with caution)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Production guidelines\n",
    "    recommendations['production_guidelines'] = {\n",
    "        'monitoring': [\n",
    "            'Track gradient overflow rates',\n",
    "            'Monitor loss scaling updates',\n",
    "            'Watch for training instability',\n",
    "            'Profile memory usage patterns'\n",
    "        ],\n",
    "        'best_practices': [\n",
    "            'Start with FP32 for debugging',\n",
    "            'Use BF16 for A100+ hardware when available',\n",
    "            'Implement gradual precision reduction',\n",
    "            'Test extensively before production deployment'\n",
    "        ],\n",
    "        'fallback_strategies': [\n",
    "            'Automatic fallback to FP32 on overflow',\n",
    "            'Layer-specific precision selection',\n",
    "            'Dynamic precision adjustment during training'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "print(\"📊 Creating Comprehensive Mixed Precision Visualizations...\")\n",
    "fig = create_mixed_precision_visualizations(precision_results, training_results)\n",
    "\n",
    "# Generate production recommendations\n",
    "print(\"\\n🎯 Generating Production Recommendations...\")\n",
    "recommendations = generate_mixed_precision_recommendations(precision_results, training_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"⚡ MIXED PRECISION TRAINING RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Format-specific recommendations\n",
    "print(\"\\n🎯 PRECISION FORMAT RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for fmt, rec in recommendations['format_recommendations'].items():\n",
    "    print(f\"\\n{fmt.upper()}:\")\n",
    "    print(f\"  • Memory Savings: {rec['memory_savings']}\")\n",
    "    print(f\"  • Speed Improvement: {rec['speed_improvement']}\")\n",
    "    print(f\"  • Numerical Stability: {rec['numerical_stability']}\")\n",
    "    print(f\"  • Gradient Scaling Required: {rec['gradient_scaling_required']}\")\n",
    "    print(f\"  • Tensor Core Support: {rec['tensor_core_support']}\")\n",
    "    print(f\"  • Use Cases:\")\n",
    "    for use_case in rec['recommended_use_cases']:\n",
    "        print(f\"    - {use_case}\")\n",
    "\n",
    "# Hardware recommendations\n",
    "print(\"\\n🖥️ HARDWARE-SPECIFIC RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for gpu, specs in recommendations['hardware_considerations'].items():\n",
    "    print(f\"\\n{gpu}:\")\n",
    "    print(f\"  • Recommended Format: {specs['recommended_format']}\")\n",
    "    print(f\"  • FP16 Support: {specs['fp16_support']}\")\n",
    "    print(f\"  • BF16 Support: {specs['bf16_support']}\")\n",
    "    print(f\"  • Tensor Cores: {specs['tensor_cores']}\")\n",
    "\n",
    "# Production guidelines\n",
    "print(\"\\n🏭 PRODUCTION DEPLOYMENT GUIDELINES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nMonitoring:\")\n",
    "for guideline in recommendations['production_guidelines']['monitoring']:\n",
    "    print(f\"  • {guideline}\")\n",
    "\n",
    "print(\"\\nBest Practices:\")\n",
    "for practice in recommendations['production_guidelines']['best_practices']:\n",
    "    print(f\"  • {practice}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ Chapter 5: Mixed Precision Training Mastery Complete!\")\n",
    "\n",
    "print(\"\\n📚 Key Learning Outcomes:\")\n",
    "print(\"  • Deep understanding of FP16, BF16, and emerging FP8 formats\")\n",
    "print(\"  • Advanced gradient scaling and numerical stability techniques\")\n",
    "print(\"  • Tensor Core optimization strategies\")\n",
    "print(\"  • Production-grade mixed precision training implementation\")\n",
    "print(\"  • Comprehensive performance analysis and monitoring\")\n",
    "\n",
    "print(\"\\n🎓 Next Chapter: Advanced Inference Optimization\")\n",
    "print(\"Continue to Chapter 6 for deep dive into vLLM, continuous batching, and inference optimization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}