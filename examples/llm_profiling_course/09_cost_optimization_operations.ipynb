{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí∞ Chapter 9: Cost Optimization & Operations\n",
    "\n",
    "## üìä Theoretical Foundations of AI Infrastructure Economics\n",
    "\n",
    "### The Economics of Large-Scale AI Training\n",
    "\n",
    "Training large language models represents one of the most expensive computational workloads in modern computing. This chapter explores the comprehensive economic analysis, cost optimization strategies, and operational excellence practices required for sustainable AI infrastructure at scale.\n",
    "\n",
    "### Cost Structure Analysis\n",
    "\n",
    "#### **Primary Cost Components:**\n",
    "\n",
    "**1. Compute Costs (60-80% of total)**\n",
    "- GPU instance costs ($/hour)\n",
    "- CPU instance costs for data processing\n",
    "- Memory and storage costs\n",
    "- Network bandwidth costs\n",
    "\n",
    "**2. Infrastructure Costs (10-20%)**\n",
    "- Kubernetes cluster management\n",
    "- Load balancers and networking\n",
    "- Persistent storage systems\n",
    "- Monitoring and observability tools\n",
    "\n",
    "**3. Operational Costs (10-20%)**\n",
    "- Personnel costs (ML engineers, DevOps)\n",
    "- Data pipeline processing\n",
    "- Model versioning and artifact storage\n",
    "- Compliance and security tools\n",
    "\n",
    "**4. Opportunity Costs (Variable)**\n",
    "- Failed experiments and restarts\n",
    "- Idle resource time\n",
    "- Inefficient resource allocation\n",
    "- Technical debt accumulation\n",
    "\n",
    "### Mathematical Framework for Cost Optimization\n",
    "\n",
    "**Total Cost of Ownership (TCO) Formula:**\n",
    "```\n",
    "TCO = (Compute_Cost + Infrastructure_Cost + Operational_Cost) √ó Efficiency_Factor\n",
    "\n",
    "where:\n",
    "Efficiency_Factor = 1 / (Resource_Utilization √ó Training_Success_Rate)\n",
    "```\n",
    "\n",
    "**Cost Per Model Formula:**\n",
    "```\n",
    "Cost_Per_Model = Training_Duration √ó (GPU_Cost_Per_Hour √ó Num_GPUs + \n",
    "                                    Infrastructure_Cost_Per_Hour + \n",
    "                                    Operational_Cost_Per_Hour) √ó \n",
    "                                    (1 + Failure_Rate + Idle_Time_Ratio)\n",
    "```\n",
    "\n",
    "**Return on Investment (ROI) Analysis:**\n",
    "```\n",
    "ROI = (Model_Value - Total_Training_Cost) / Total_Training_Cost\n",
    "\n",
    "Model_Value = Performance_Improvement √ó Business_Impact √ó Model_Lifetime\n",
    "```\n",
    "\n",
    "### Cost Optimization Strategies\n",
    "\n",
    "#### **Resource Optimization:**\n",
    "1. **Spot Instance Usage**: 60-90% cost reduction with preemption handling\n",
    "2. **Mixed Instance Types**: Optimize for compute vs memory requirements\n",
    "3. **Auto-scaling**: Dynamic resource allocation based on workload\n",
    "4. **Resource Pooling**: Shared infrastructure across multiple projects\n",
    "\n",
    "#### **Training Optimization:**\n",
    "1. **Efficient Parallelization**: Minimize communication overhead\n",
    "2. **Gradient Accumulation**: Reduce memory requirements\n",
    "3. **Mixed Precision**: 2x speedup with minimal quality impact\n",
    "4. **Curriculum Learning**: Faster convergence through strategic data ordering\n",
    "\n",
    "#### **Operational Optimization:**\n",
    "1. **Experiment Tracking**: Avoid duplicate work\n",
    "2. **Checkpointing**: Minimize restart costs\n",
    "3. **Resource Monitoring**: Real-time cost tracking\n",
    "4. **Automated Lifecycle**: Reduce manual operational overhead\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Hands-On Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies for cost optimization and operations\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "print(\"üí∞ Cost Optimization & Operations Environment Ready!\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"PyTorch Available: {torch.__version__}\")\n",
    "\n",
    "print(\"\\nüîß Environment Features:\")\n",
    "print(\"  ‚Ä¢ Comprehensive cost modeling and analysis\")\n",
    "print(\"  ‚Ä¢ Real-time resource optimization algorithms\")\n",
    "print(\"  ‚Ä¢ Advanced financial forecasting and budgeting\")\n",
    "print(\"  ‚Ä¢ Production-grade monitoring and alerting\")\n",
    "print(\"  ‚Ä¢ Multi-cloud cost optimization strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Advanced Cost Modeling Framework\n",
    "\n",
    "### Comprehensive Training Cost Analysis System\n",
    "\n",
    "This section implements a sophisticated cost modeling framework that accounts for all aspects of LLM training costs including compute resources, infrastructure overhead, operational expenses, and efficiency factors. The system provides detailed cost breakdowns and optimization recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudProvider(Enum):\n",
    "    \"\"\"Enumeration of cloud providers.\"\"\"\n",
    "    AWS = \"aws\"\n",
    "    AZURE = \"azure\"\n",
    "    GCP = \"gcp\"\n",
    "    ON_PREMISE = \"on_premise\"\n",
    "\n",
    "class InstanceType(Enum):\n",
    "    \"\"\"Enumeration of GPU instance types.\"\"\"\n",
    "    # AWS instances\n",
    "    P4D_24XLARGE = \"p4d.24xlarge\"  # 8x A100 80GB\n",
    "    P4DE_24XLARGE = \"p4de.24xlarge\"  # 8x A100 80GB + NVMe\n",
    "    P3_16XLARGE = \"p3.16xlarge\"  # 8x V100 16GB\n",
    "    G5_48XLARGE = \"g5.48xlarge\"  # 8x A10G 24GB\n",
    "    \n",
    "    # Azure instances\n",
    "    ND96AMV4_A100 = \"Standard_ND96amv4_A100_v4\"  # 8x A100 80GB\n",
    "    ND40RS_V2 = \"Standard_ND40rs_v2\"  # 8x V100 32GB\n",
    "    \n",
    "    # GCP instances\n",
    "    A100_8 = \"a2-megagpu-16g\"  # 8x A100 80GB\n",
    "    V100_8 = \"n1-standard-96-v100-8\"  # 8x V100 16GB\n",
    "\n",
    "@dataclass\n",
    "class ResourcePricing:\n",
    "    \"\"\"Pricing information for compute resources.\"\"\"\n",
    "    # GPU instance pricing (per hour)\n",
    "    gpu_instances: Dict[InstanceType, Dict[str, float]]\n",
    "    \n",
    "    # Storage pricing (per GB per month)\n",
    "    persistent_storage: float\n",
    "    ephemeral_storage: float\n",
    "    \n",
    "    # Network pricing (per GB)\n",
    "    network_ingress: float\n",
    "    network_egress: float\n",
    "    \n",
    "    # Additional services\n",
    "    load_balancer_per_hour: float\n",
    "    kubernetes_management_per_cluster_hour: float\n",
    "    monitoring_per_resource_hour: float\n",
    "\n",
    "@dataclass \n",
    "class TrainingConfiguration:\n",
    "    \"\"\"Configuration for training cost analysis.\"\"\"\n",
    "    # Model parameters\n",
    "    model_size_billion: float\n",
    "    sequence_length: int\n",
    "    vocabulary_size: int\n",
    "    \n",
    "    # Training parameters\n",
    "    global_batch_size: int\n",
    "    num_training_tokens_billion: float\n",
    "    learning_rate: float\n",
    "    \n",
    "    # Hardware configuration\n",
    "    instance_type: InstanceType\n",
    "    num_instances: int\n",
    "    use_spot_instances: bool = True\n",
    "    spot_interruption_rate: float = 0.1  # 10% chance per hour\n",
    "    \n",
    "    # Infrastructure configuration\n",
    "    storage_gb: float = 1000.0\n",
    "    network_usage_gb: float = 100.0\n",
    "    \n",
    "    # Optimization settings\n",
    "    mixed_precision: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    pipeline_parallel_degree: int = 1\n",
    "    tensor_parallel_degree: int = 1\n",
    "\n",
    "class ComprehensiveCostAnalyzer:\n",
    "    \"\"\"Advanced cost analysis system for LLM training operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: CloudProvider):\n",
    "        self.provider = provider\n",
    "        self.pricing = self._initialize_pricing(provider)\n",
    "        \n",
    "        # Cost tracking\n",
    "        self.cost_history = []\n",
    "        self.optimization_recommendations = []\n",
    "        \n",
    "    def _initialize_pricing(self, provider: CloudProvider) -> ResourcePricing:\n",
    "        \"\"\"Initialize pricing for the specified cloud provider.\"\"\"\n",
    "        \n",
    "        if provider == CloudProvider.AWS:\n",
    "            return ResourcePricing(\n",
    "                gpu_instances={\n",
    "                    InstanceType.P4D_24XLARGE: {'on_demand': 32.77, 'spot': 9.83},  # 8x A100\n",
    "                    InstanceType.P4DE_24XLARGE: {'on_demand': 40.96, 'spot': 12.29}, # 8x A100 + NVMe\n",
    "                    InstanceType.P3_16XLARGE: {'on_demand': 24.48, 'spot': 7.34},   # 8x V100\n",
    "                    InstanceType.G5_48XLARGE: {'on_demand': 16.29, 'spot': 4.89}    # 8x A10G\n",
    "                },\n",
    "                persistent_storage=0.10,  # EBS gp3\n",
    "                ephemeral_storage=0.045,  # EBS st1\n",
    "                network_ingress=0.0,\n",
    "                network_egress=0.09,\n",
    "                load_balancer_per_hour=0.025,\n",
    "                kubernetes_management_per_cluster_hour=0.10,  # EKS\n",
    "                monitoring_per_resource_hour=0.005\n",
    "            )\n",
    "        elif provider == CloudProvider.GCP:\n",
    "            return ResourcePricing(\n",
    "                gpu_instances={\n",
    "                    InstanceType.A100_8: {'on_demand': 30.48, 'spot': 9.14},    # 8x A100\n",
    "                    InstanceType.V100_8: {'on_demand': 22.32, 'spot': 6.70}     # 8x V100\n",
    "                },\n",
    "                persistent_storage=0.08,   # Persistent disk SSD\n",
    "                ephemeral_storage=0.04,    # Persistent disk standard\n",
    "                network_ingress=0.0,\n",
    "                network_egress=0.085,\n",
    "                load_balancer_per_hour=0.025,\n",
    "                kubernetes_management_per_cluster_hour=0.10,  # GKE\n",
    "                monitoring_per_resource_hour=0.004\n",
    "            )\n",
    "        elif provider == CloudProvider.AZURE:\n",
    "            return ResourcePricing(\n",
    "                gpu_instances={\n",
    "                    InstanceType.ND96AMV4_A100: {'on_demand': 35.57, 'spot': 10.67}, # 8x A100\n",
    "                    InstanceType.ND40RS_V2: {'on_demand': 25.20, 'spot': 7.56}       # 8x V100\n",
    "                },\n",
    "                persistent_storage=0.12,   # Premium SSD\n",
    "                ephemeral_storage=0.05,    # Standard HDD\n",
    "                network_ingress=0.0,\n",
    "                network_egress=0.087,\n",
    "                load_balancer_per_hour=0.028,\n",
    "                kubernetes_management_per_cluster_hour=0.12,  # AKS\n",
    "                monitoring_per_resource_hour=0.006\n",
    "            )\n",
    "        else:  # ON_PREMISE\n",
    "            # Amortized costs for on-premise hardware\n",
    "            return ResourcePricing(\n",
    "                gpu_instances={\n",
    "                    InstanceType.A100_8: {'on_demand': 8.50, 'spot': 8.50},  # Amortized A100 cost\n",
    "                    InstanceType.V100_8: {'on_demand': 6.20, 'spot': 6.20}   # Amortized V100 cost\n",
    "                },\n",
    "                persistent_storage=0.02,   # Amortized storage cost\n",
    "                ephemeral_storage=0.01,\n",
    "                network_ingress=0.0,\n",
    "                network_egress=0.0,\n",
    "                load_balancer_per_hour=0.0,\n",
    "                kubernetes_management_per_cluster_hour=0.0,\n",
    "                monitoring_per_resource_hour=0.001\n",
    "            )\n",
    "    \n",
    "    def calculate_training_cost(self, config: TrainingConfiguration) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive training cost analysis.\"\"\"\n",
    "        \n",
    "        # Step 1: Estimate training duration\n",
    "        duration_analysis = self._estimate_training_duration(config)\n",
    "        \n",
    "        # Step 2: Calculate compute costs\n",
    "        compute_costs = self._calculate_compute_costs(config, duration_analysis)\n",
    "        \n",
    "        # Step 3: Calculate infrastructure costs\n",
    "        infrastructure_costs = self._calculate_infrastructure_costs(config, duration_analysis)\n",
    "        \n",
    "        # Step 4: Calculate operational costs\n",
    "        operational_costs = self._calculate_operational_costs(config, duration_analysis)\n",
    "        \n",
    "        # Step 5: Apply efficiency factors\n",
    "        efficiency_analysis = self._analyze_efficiency_factors(config)\n",
    "        \n",
    "        # Step 6: Calculate total cost with inefficiencies\n",
    "        base_cost = compute_costs['total'] + infrastructure_costs['total'] + operational_costs['total']\n",
    "        total_cost = base_cost * efficiency_analysis['cost_multiplier']\n",
    "        \n",
    "        return {\n",
    "            'duration_analysis': duration_analysis,\n",
    "            'compute_costs': compute_costs,\n",
    "            'infrastructure_costs': infrastructure_costs,\n",
    "            'operational_costs': operational_costs,\n",
    "            'efficiency_analysis': efficiency_analysis,\n",
    "            'cost_summary': {\n",
    "                'base_cost_usd': base_cost,\n",
    "                'total_cost_usd': total_cost,\n",
    "                'cost_per_parameter_million': total_cost / config.model_size_billion / 1000,\n",
    "                'cost_per_token_trained': total_cost / (config.num_training_tokens_billion * 1e9),\n",
    "                'daily_burn_rate': total_cost / (duration_analysis['total_duration_hours'] / 24)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _estimate_training_duration(self, config: TrainingConfiguration) -> Dict[str, float]:\n",
    "        \"\"\"Estimate training duration based on model and hardware configuration.\"\"\"\n",
    "        \n",
    "        # Calculate FLOPs per token (approximate for transformer models)\n",
    "        flops_per_token = 6 * config.model_size_billion * 1e9  # 6 * parameters\n",
    "        total_flops = flops_per_token * config.num_training_tokens_billion * 1e9\n",
    "        \n",
    "        # Estimate GPU performance based on instance type\n",
    "        gpu_tflops = self._get_gpu_performance(config.instance_type, config.mixed_precision)\n",
    "        total_gpu_tflops = gpu_tflops * self._get_gpus_per_instance(config.instance_type) * config.num_instances\n",
    "        \n",
    "        # Account for parallelization efficiency\n",
    "        parallelization_efficiency = self._calculate_parallelization_efficiency(\n",
    "            config.tensor_parallel_degree, config.pipeline_parallel_degree, config.num_instances\n",
    "        )\n",
    "        \n",
    "        effective_tflops = total_gpu_tflops * parallelization_efficiency\n",
    "        \n",
    "        # Calculate base training time\n",
    "        base_training_hours = total_flops / (effective_tflops * 1e12) / 3600\n",
    "        \n",
    "        # Account for overhead (data loading, checkpointing, etc.)\n",
    "        overhead_factor = 1.3  # 30% overhead\n",
    "        actual_training_hours = base_training_hours * overhead_factor\n",
    "        \n",
    "        return {\n",
    "            'total_flops': total_flops,\n",
    "            'effective_tflops': effective_tflops,\n",
    "            'parallelization_efficiency': parallelization_efficiency,\n",
    "            'base_training_hours': base_training_hours,\n",
    "            'overhead_factor': overhead_factor,\n",
    "            'total_duration_hours': actual_training_hours,\n",
    "            'total_duration_days': actual_training_hours / 24\n",
    "        }\n",
    "    \n",
    "    def _calculate_compute_costs(self, config: TrainingConfiguration, \n",
    "                               duration_analysis: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate compute costs including spot instance considerations.\"\"\"\n",
    "        \n",
    "        instance_pricing = self.pricing.gpu_instances[config.instance_type]\n",
    "        \n",
    "        # Determine hourly rate\n",
    "        if config.use_spot_instances:\n",
    "            base_hourly_rate = instance_pricing['spot']\n",
    "            \n",
    "            # Account for spot interruptions\n",
    "            # When interrupted, need to restart from last checkpoint (assume 1 hour loss average)\n",
    "            interruption_overhead = config.spot_interruption_rate * 1.0  # 1 hour average loss\n",
    "            actual_hourly_rate = base_hourly_rate * (1 + interruption_overhead)\n",
    "        else:\n",
    "            actual_hourly_rate = instance_pricing['on_demand']\n",
    "        \n",
    "        # Calculate total compute cost\n",
    "        total_compute_hours = duration_analysis['total_duration_hours'] * config.num_instances\n",
    "        total_compute_cost = total_compute_hours * actual_hourly_rate\n",
    "        \n",
    "        return {\n",
    "            'hourly_rate_per_instance': actual_hourly_rate,\n",
    "            'total_compute_hours': total_compute_hours,\n",
    "            'total': total_compute_cost,\n",
    "            'spot_savings': (instance_pricing['on_demand'] - instance_pricing['spot']) * total_compute_hours if config.use_spot_instances else 0\n",
    "        }\n",
    "    \n",
    "    def _calculate_infrastructure_costs(self, config: TrainingConfiguration,\n",
    "                                      duration_analysis: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate infrastructure costs (storage, networking, management).\"\"\"\n",
    "        \n",
    "        training_duration_hours = duration_analysis['total_duration_hours']\n",
    "        \n",
    "        # Storage costs\n",
    "        storage_cost = (config.storage_gb * self.pricing.persistent_storage * \n",
    "                       training_duration_hours / (24 * 30))  # Convert monthly to hourly\n",
    "        \n",
    "        # Network costs\n",
    "        network_cost = config.network_usage_gb * self.pricing.network_egress\n",
    "        \n",
    "        # Load balancer costs\n",
    "        load_balancer_cost = self.pricing.load_balancer_per_hour * training_duration_hours\n",
    "        \n",
    "        # Kubernetes management costs\n",
    "        k8s_management_cost = (self.pricing.kubernetes_management_per_cluster_hour * \n",
    "                              training_duration_hours)\n",
    "        \n",
    "        # Monitoring costs\n",
    "        monitoring_cost = (self.pricing.monitoring_per_resource_hour * \n",
    "                          config.num_instances * training_duration_hours)\n",
    "        \n",
    "        total_infrastructure_cost = (storage_cost + network_cost + load_balancer_cost + \n",
    "                                   k8s_management_cost + monitoring_cost)\n",
    "        \n",
    "        return {\n",
    "            'storage': storage_cost,\n",
    "            'network': network_cost,\n",
    "            'load_balancer': load_balancer_cost,\n",
    "            'kubernetes_management': k8s_management_cost,\n",
    "            'monitoring': monitoring_cost,\n",
    "            'total': total_infrastructure_cost\n",
    "        }\n",
    "    \n",
    "    def _calculate_operational_costs(self, config: TrainingConfiguration,\n",
    "                                   duration_analysis: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate operational costs (personnel, tools, overhead).\"\"\"\n",
    "        \n",
    "        training_duration_days = duration_analysis['total_duration_days']\n",
    "        \n",
    "        # Personnel costs (assume ML engineer monitoring)\n",
    "        # Rough estimate: 25% of ML engineer time during training\n",
    "        ml_engineer_daily_cost = 500  # $500/day loaded cost\n",
    "        personnel_cost = ml_engineer_daily_cost * training_duration_days * 0.25\n",
    "        \n",
    "        # Data pipeline costs (rough estimate)\n",
    "        data_processing_cost = config.num_training_tokens_billion * 10  # $10 per billion tokens\n",
    "        \n",
    "        # Experiment tracking and versioning\n",
    "        mlops_tooling_cost = training_duration_days * 50  # $50/day for tooling\n",
    "        \n",
    "        # Compliance and security overhead\n",
    "        compliance_cost = training_duration_days * 25  # $25/day\n",
    "        \n",
    "        total_operational_cost = (personnel_cost + data_processing_cost + \n",
    "                                mlops_tooling_cost + compliance_cost)\n",
    "        \n",
    "        return {\n",
    "            'personnel': personnel_cost,\n",
    "            'data_processing': data_processing_cost,\n",
    "            'mlops_tooling': mlops_tooling_cost,\n",
    "            'compliance': compliance_cost,\n",
    "            'total': total_operational_cost\n",
    "        }\n",
    "    \n",
    "    def _analyze_efficiency_factors(self, config: TrainingConfiguration) -> Dict[str, float]:\n",
    "        \"\"\"Analyze various efficiency factors that impact total cost.\"\"\"\n",
    "        \n",
    "        # Base efficiency factors\n",
    "        resource_utilization = 0.85  # 85% average GPU utilization\n",
    "        \n",
    "        # Training success rate (probability of successful completion without restarts)\n",
    "        training_success_rate = 0.9  # 90% success rate\n",
    "        \n",
    "        # Idle time factor (time when resources are allocated but not training)\n",
    "        idle_time_factor = 0.1  # 10% idle time\n",
    "        \n",
    "        # Development overhead (failed experiments, hyperparameter tuning)\n",
    "        development_overhead = 0.3  # 30% additional cost for exploration\n",
    "        \n",
    "        # Calculate overall efficiency multiplier\n",
    "        efficiency_multiplier = 1 / (resource_utilization * training_success_rate)\n",
    "        cost_multiplier = efficiency_multiplier * (1 + idle_time_factor + development_overhead)\n",
    "        \n",
    "        return {\n",
    "            'resource_utilization': resource_utilization,\n",
    "            'training_success_rate': training_success_rate,\n",
    "            'idle_time_factor': idle_time_factor,\n",
    "            'development_overhead': development_overhead,\n",
    "            'efficiency_multiplier': efficiency_multiplier,\n",
    "            'cost_multiplier': cost_multiplier\n",
    "        }\n",
    "    \n",
    "    def _get_gpu_performance(self, instance_type: InstanceType, mixed_precision: bool) -> float:\n",
    "        \"\"\"Get GPU performance in TFLOPS.\"\"\"\n",
    "        \n",
    "        base_performance = {\n",
    "            InstanceType.P4D_24XLARGE: 156,    # A100 FP16\n",
    "            InstanceType.P4DE_24XLARGE: 156,   # A100 FP16\n",
    "            InstanceType.P3_16XLARGE: 62.5,    # V100 FP16\n",
    "            InstanceType.G5_48XLARGE: 31.2,    # A10G FP16\n",
    "            InstanceType.ND96AMV4_A100: 156,   # A100 FP16\n",
    "            InstanceType.ND40RS_V2: 62.5,      # V100 FP16\n",
    "            InstanceType.A100_8: 156,          # A100 FP16\n",
    "            InstanceType.V100_8: 62.5          # V100 FP16\n",
    "        }\n",
    "        \n",
    "        performance = base_performance.get(instance_type, 100)\n",
    "        \n",
    "        # Mixed precision typically provides 1.5-2x speedup\n",
    "        if mixed_precision:\n",
    "            performance *= 1.7\n",
    "        else:\n",
    "            performance /= 2  # FP32 is roughly half the performance\n",
    "        \n",
    "        return performance\n",
    "    \n",
    "    def _get_gpus_per_instance(self, instance_type: InstanceType) -> int:\n",
    "        \"\"\"Get number of GPUs per instance.\"\"\"\n",
    "        \n",
    "        gpu_counts = {\n",
    "            InstanceType.P4D_24XLARGE: 8,\n",
    "            InstanceType.P4DE_24XLARGE: 8,\n",
    "            InstanceType.P3_16XLARGE: 8,\n",
    "            InstanceType.G5_48XLARGE: 8,\n",
    "            InstanceType.ND96AMV4_A100: 8,\n",
    "            InstanceType.ND40RS_V2: 8,\n",
    "            InstanceType.A100_8: 8,\n",
    "            InstanceType.V100_8: 8\n",
    "        }\n",
    "        \n",
    "        return gpu_counts.get(instance_type, 8)\n",
    "    \n",
    "    def _calculate_parallelization_efficiency(self, tp_degree: int, pp_degree: int, \n",
    "                                            num_instances: int) -> float:\n",
    "        \"\"\"Calculate efficiency loss due to parallelization.\"\"\"\n",
    "        \n",
    "        # Data parallel efficiency (communication overhead)\n",
    "        dp_degree = num_instances * 8 // (tp_degree * pp_degree)  # Assume 8 GPUs per instance\n",
    "        dp_efficiency = 1.0 - (0.05 * math.log2(max(1, dp_degree)))  # 5% loss per doubling\n",
    "        \n",
    "        # Tensor parallel efficiency (activation synchronization)\n",
    "        tp_efficiency = 1.0 - (0.03 * math.log2(max(1, tp_degree)))  # 3% loss per doubling\n",
    "        \n",
    "        # Pipeline parallel efficiency (bubble time)\n",
    "        pp_efficiency = 1.0 - (0.1 / pp_degree) if pp_degree > 1 else 1.0  # Pipeline bubble overhead\n",
    "        \n",
    "        # Combined efficiency\n",
    "        total_efficiency = dp_efficiency * tp_efficiency * pp_efficiency\n",
    "        \n",
    "        return max(0.6, total_efficiency)  # Minimum 60% efficiency\n",
    "\n",
    "# Initialize cost analyzer for different cloud providers\n",
    "print(\"üèóÔ∏è Initializing Comprehensive Cost Analysis System...\")\n",
    "\n",
    "# Test configurations for different model sizes\n",
    "test_configs = {\n",
    "    '7B': TrainingConfiguration(\n",
    "        model_size_billion=7.0,\n",
    "        sequence_length=2048,\n",
    "        vocabulary_size=32000,\n",
    "        global_batch_size=256,\n",
    "        num_training_tokens_billion=1000,  # 1T tokens\n",
    "        learning_rate=1e-4,\n",
    "        instance_type=InstanceType.P4D_24XLARGE,\n",
    "        num_instances=4,  # 32 A100s\n",
    "        use_spot_instances=True,\n",
    "        mixed_precision=True,\n",
    "        gradient_checkpointing=True,\n",
    "        tensor_parallel_degree=8,\n",
    "        pipeline_parallel_degree=1\n",
    "    ),\n",
    "    '30B': TrainingConfiguration(\n",
    "        model_size_billion=30.0,\n",
    "        sequence_length=2048,\n",
    "        vocabulary_size=32000,\n",
    "        global_batch_size=512,\n",
    "        num_training_tokens_billion=1000,  # 1T tokens\n",
    "        learning_rate=8e-5,\n",
    "        instance_type=InstanceType.P4D_24XLARGE,\n",
    "        num_instances=16,  # 128 A100s\n",
    "        use_spot_instances=True,\n",
    "        mixed_precision=True,\n",
    "        gradient_checkpointing=True,\n",
    "        tensor_parallel_degree=8,\n",
    "        pipeline_parallel_degree=2\n",
    "    ),\n",
    "    '70B': TrainingConfiguration(\n",
    "        model_size_billion=70.0,\n",
    "        sequence_length=2048,\n",
    "        vocabulary_size=32000,\n",
    "        global_batch_size=1024,\n",
    "        num_training_tokens_billion=1000,  # 1T tokens\n",
    "        learning_rate=6e-5,\n",
    "        instance_type=InstanceType.P4D_24XLARGE,\n",
    "        num_instances=32,  # 256 A100s\n",
    "        use_spot_instances=True,\n",
    "        mixed_precision=True,\n",
    "        gradient_checkpointing=True,\n",
    "        tensor_parallel_degree=8,\n",
    "        pipeline_parallel_degree=4\n",
    "    )\n",
    "}\n",
    "\n",
    "# Initialize analyzers for different cloud providers\n",
    "analyzers = {\n",
    "    'AWS': ComprehensiveCostAnalyzer(CloudProvider.AWS),\n",
    "    'GCP': ComprehensiveCostAnalyzer(CloudProvider.GCP),\n",
    "    'Azure': ComprehensiveCostAnalyzer(CloudProvider.AZURE),\n",
    "    'On-Premise': ComprehensiveCostAnalyzer(CloudProvider.ON_PREMISE)\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Cost Analysis System Initialized for {len(analyzers)} providers\")\n",
    "print(f\"üìä Ready to analyze {len(test_configs)} model configurations\")\n",
    "print(\"\\nüîß System Features:\")\n",
    "print(\"  ‚Ä¢ Comprehensive multi-cloud cost modeling\")\n",
    "print(\"  ‚Ä¢ Advanced efficiency factor analysis\")\n",
    "print(\"  ‚Ä¢ Spot instance optimization calculations\")\n",
    "print(\"  ‚Ä¢ Infrastructure and operational cost tracking\")\n",
    "print(\"  ‚Ä¢ ROI and TCO analysis capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comprehensive Cost Analysis and Comparison\n",
    "\n",
    "### Multi-Provider and Multi-Model Cost Analysis\n",
    "\n",
    "This section runs comprehensive cost analysis across multiple cloud providers and model sizes, providing detailed cost breakdowns, efficiency analysis, and optimization recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_cost_analysis():\n",
    "    \"\"\"Run comprehensive cost analysis across all providers and configurations.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"üöÄ Running Comprehensive Cost Analysis...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_size, config in test_configs.items():\n",
    "        print(f\"\\nüìä Analyzing {model_size} Parameter Model:\")\n",
    "        print(f\"  ‚Ä¢ Model Size: {config.model_size_billion}B parameters\")\n",
    "        print(f\"  ‚Ä¢ Training Tokens: {config.num_training_tokens_billion}B\")\n",
    "        print(f\"  ‚Ä¢ Hardware: {config.num_instances}x {config.instance_type.value}\")\n",
    "        print(f\"  ‚Ä¢ Parallelism: TP={config.tensor_parallel_degree}, PP={config.pipeline_parallel_degree}\")\n",
    "        \n",
    "        results[model_size] = {}\n",
    "        \n",
    "        for provider_name, analyzer in analyzers.items():\n",
    "            try:\n",
    "                # Calculate cost for this provider\n",
    "                cost_analysis = analyzer.calculate_training_cost(config)\n",
    "                results[model_size][provider_name] = cost_analysis\n",
    "                \n",
    "                # Print summary\n",
    "                total_cost = cost_analysis['cost_summary']['total_cost_usd']\n",
    "                duration_days = cost_analysis['duration_analysis']['total_duration_days']\n",
    "                daily_cost = cost_analysis['cost_summary']['daily_burn_rate']\n",
    "                \n",
    "                print(f\"\\n    {provider_name}:\")\n",
    "                print(f\"      üí∞ Total Cost: ${total_cost:,.0f}\")\n",
    "                print(f\"      ‚è±Ô∏è  Duration: {duration_days:.1f} days\")\n",
    "                print(f\"      üî• Daily Burn: ${daily_cost:,.0f}/day\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n    {provider_name}: ‚ùå Error - {e}\")\n",
    "                results[model_size][provider_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_cost_comparison_analysis(results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Create structured DataFrame for cost comparison analysis.\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_size, provider_results in results.items():\n",
    "        for provider, analysis in provider_results.items():\n",
    "            if analysis is not None:\n",
    "                row = {\n",
    "                    'Model_Size': model_size,\n",
    "                    'Provider': provider,\n",
    "                    'Total_Cost_USD': analysis['cost_summary']['total_cost_usd'],\n",
    "                    'Duration_Days': analysis['duration_analysis']['total_duration_days'],\n",
    "                    'Daily_Burn_Rate': analysis['cost_summary']['daily_burn_rate'],\n",
    "                    'Cost_Per_Parameter_M': analysis['cost_summary']['cost_per_parameter_million'],\n",
    "                    'Cost_Per_Token': analysis['cost_summary']['cost_per_token_trained'] * 1e9,  # Cost per billion tokens\n",
    "                    'Compute_Cost': analysis['compute_costs']['total'],\n",
    "                    'Infrastructure_Cost': analysis['infrastructure_costs']['total'],\n",
    "                    'Operational_Cost': analysis['operational_costs']['total'],\n",
    "                    'Efficiency_Multiplier': analysis['efficiency_analysis']['cost_multiplier'],\n",
    "                    'GPU_Utilization': analysis['efficiency_analysis']['resource_utilization'],\n",
    "                    'Parallelization_Efficiency': analysis['duration_analysis']['parallelization_efficiency']\n",
    "                }\n",
    "                comparison_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "def generate_optimization_recommendations(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Generate cost optimization recommendations based on analysis.\"\"\"\n",
    "    \n",
    "    recommendations = {\n",
    "        'provider_rankings': {},\n",
    "        'cost_optimization_strategies': {},\n",
    "        'scaling_insights': {},\n",
    "        'efficiency_improvements': {}\n",
    "    }\n",
    "    \n",
    "    # Provider cost rankings by model size\n",
    "    for model_size in df['Model_Size'].unique():\n",
    "        model_data = df[df['Model_Size'] == model_size].sort_values('Total_Cost_USD')\n",
    "        rankings = []\n",
    "        \n",
    "        for _, row in model_data.iterrows():\n",
    "            savings = ((model_data['Total_Cost_USD'].max() - row['Total_Cost_USD']) / \n",
    "                      model_data['Total_Cost_USD'].max() * 100)\n",
    "            \n",
    "            rankings.append({\n",
    "                'provider': row['Provider'],\n",
    "                'cost': row['Total_Cost_USD'],\n",
    "                'savings_percent': savings,\n",
    "                'duration_days': row['Duration_Days']\n",
    "            })\n",
    "        \n",
    "        recommendations['provider_rankings'][model_size] = rankings\n",
    "    \n",
    "    # Cost optimization strategies\n",
    "    avg_efficiency = df['Efficiency_Multiplier'].mean()\n",
    "    avg_utilization = df['GPU_Utilization'].mean()\n",
    "    \n",
    "    recommendations['cost_optimization_strategies'] = {\n",
    "        'spot_instance_usage': {\n",
    "            'potential_savings': '60-70%',\n",
    "            'implementation': 'Use spot instances with proper checkpointing and fault tolerance',\n",
    "            'considerations': 'Higher complexity, potential training interruptions'\n",
    "        },\n",
    "        'mixed_precision_training': {\n",
    "            'potential_savings': '40-50%',\n",
    "            'implementation': 'Enable FP16/BF16 training with gradient scaling',\n",
    "            'considerations': 'Minimal impact on model quality for most use cases'\n",
    "        },\n",
    "        'gradient_checkpointing': {\n",
    "            'potential_savings': '20-30% memory reduction',\n",
    "            'implementation': 'Trade compute for memory to use smaller instances',\n",
    "            'considerations': '10-20% compute overhead'\n",
    "        },\n",
    "        'efficient_parallelization': {\n",
    "            'current_efficiency': f'{avg_efficiency:.2f}x overhead',\n",
    "            'optimization_target': '1.5x overhead or better',\n",
    "            'implementation': 'Optimize tensor/pipeline parallelism degrees'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Scaling insights\n",
    "    cost_per_param_by_size = df.groupby('Model_Size')['Cost_Per_Parameter_M'].mean().to_dict()\n",
    "    \n",
    "    recommendations['scaling_insights'] = {\n",
    "        'cost_per_parameter_scaling': cost_per_param_by_size,\n",
    "        'scaling_efficiency': 'Larger models have better cost efficiency per parameter',\n",
    "        'sweet_spot': '30B-70B parameters for optimal cost/performance trade-off',\n",
    "        'diminishing_returns': 'Models >100B show diminishing returns on investment'\n",
    "    }\n",
    "    \n",
    "    # Efficiency improvement opportunities\n",
    "    recommendations['efficiency_improvements'] = {\n",
    "        'current_average_utilization': f'{avg_utilization:.1%}',\n",
    "        'target_utilization': '90%+',\n",
    "        'improvement_strategies': [\n",
    "            'Implement dynamic batching to maximize GPU utilization',\n",
    "            'Use gradient accumulation to increase effective batch size',\n",
    "            'Optimize data loading pipeline to prevent GPU starvation',\n",
    "            'Implement efficient checkpointing to minimize restart costs'\n",
    "        ],\n",
    "        'monitoring_recommendations': [\n",
    "            'Track real-time GPU utilization and memory usage',\n",
    "            'Monitor training throughput (tokens/second)',\n",
    "            'Set up cost alerts and budget controls',\n",
    "            'Implement automated scaling based on queue depth'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Run comprehensive analysis\n",
    "analysis_results = run_comprehensive_cost_analysis()\n",
    "\n",
    "# Create comparison DataFrame\n",
    "print(\"\\nüìà Creating Cost Comparison Analysis...\")\n",
    "comparison_df = create_cost_comparison_analysis(analysis_results)\n",
    "\n",
    "print(f\"‚úÖ Analysis complete! Generated {len(comparison_df)} cost scenarios\")\n",
    "print(\"\\nüìä Sample Results:\")\n",
    "print(comparison_df.head().to_string(index=False, float_format='{:,.0f}'.format))\n",
    "\n",
    "# Generate optimization recommendations\n",
    "print(\"\\nüéØ Generating Optimization Recommendations...\")\n",
    "optimization_recommendations = generate_optimization_recommendations(comparison_df)\n",
    "\n",
    "print(\"‚úÖ Comprehensive Cost Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Advanced Cost Visualization and Analysis\n",
    "\n",
    "### Multi-Dimensional Cost Comparison Visualizations\n",
    "\n",
    "This section creates comprehensive visualizations comparing costs across different providers, model sizes, and optimization strategies, providing actionable insights for cost optimization decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_cost_visualizations(df: pd.DataFrame, recommendations: Dict):\n",
    "    \"\"\"Create comprehensive cost analysis visualizations.\"\"\"\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(24, 18))\n",
    "    fig.suptitle('üí∞ Comprehensive LLM Training Cost Analysis', fontsize=20, y=0.98)\n",
    "    \n",
    "    # 1. Total Cost Comparison by Provider and Model Size\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    pivot_cost = df.pivot(index='Model_Size', columns='Provider', values='Total_Cost_USD')\n",
    "    pivot_cost.plot(kind='bar', ax=ax1, width=0.8, alpha=0.8)\n",
    "    ax1.set_title('Total Training Cost by Provider', fontsize=14, pad=20)\n",
    "    ax1.set_xlabel('Model Size')\n",
    "    ax1.set_ylabel('Total Cost (USD)')\n",
    "    ax1.legend(title='Provider', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Format y-axis as currency\n",
    "    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "    \n",
    "    # 2. Cost Breakdown Analysis\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Average cost breakdown across all scenarios\n",
    "    avg_costs = df[['Compute_Cost', 'Infrastructure_Cost', 'Operational_Cost']].mean()\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    wedges, texts, autotexts = ax2.pie(avg_costs.values, labels=avg_costs.index, \n",
    "                                      autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax2.set_title('Average Cost Breakdown', fontsize=14, pad=20)\n",
    "    \n",
    "    # Enhance pie chart appearance\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontsize(10)\n",
    "        autotext.set_weight('bold')\n",
    "    \n",
    "    # 3. Cost Efficiency Analysis\n",
    "    ax3 = axes[0, 2]\n",
    "    \n",
    "    # Cost per parameter vs model size\n",
    "    for provider in df['Provider'].unique():\n",
    "        provider_data = df[df['Provider'] == provider]\n",
    "        ax3.plot(provider_data['Model_Size'], provider_data['Cost_Per_Parameter_M'], \n",
    "                marker='o', linewidth=2, label=provider, markersize=6)\n",
    "    \n",
    "    ax3.set_title('Cost Efficiency by Scale', fontsize=14, pad=20)\n",
    "    ax3.set_xlabel('Model Size')\n",
    "    ax3.set_ylabel('Cost per Million Parameters ($)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # 4. Training Duration Comparison\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    pivot_duration = df.pivot(index='Model_Size', columns='Provider', values='Duration_Days')\n",
    "    pivot_duration.plot(kind='bar', ax=ax4, width=0.8, alpha=0.8)\n",
    "    ax4.set_title('Training Duration by Provider', fontsize=14, pad=20)\n",
    "    ax4.set_xlabel('Model Size')\n",
    "    ax4.set_ylabel('Training Duration (Days)')\n",
    "    ax4.legend(title='Provider', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 5. Daily Burn Rate Analysis\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    # Scatter plot of daily burn rate vs total cost\n",
    "    providers = df['Provider'].unique()\n",
    "    colors_map = plt.cm.Set3(np.linspace(0, 1, len(providers)))\n",
    "    \n",
    "    for i, provider in enumerate(providers):\n",
    "        provider_data = df[df['Provider'] == provider]\n",
    "        ax5.scatter(provider_data['Daily_Burn_Rate'], provider_data['Total_Cost_USD'], \n",
    "                   c=[colors_map[i]], label=provider, s=100, alpha=0.7)\n",
    "    \n",
    "    ax5.set_title('Daily Burn Rate vs Total Cost', fontsize=14, pad=20)\n",
    "    ax5.set_xlabel('Daily Burn Rate (USD)')\n",
    "    ax5.set_ylabel('Total Training Cost (USD)')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format axes as currency\n",
    "    ax5.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "    ax5.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "    \n",
    "    # 6. Efficiency Metrics Heatmap\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Create heatmap data\n",
    "    heatmap_data = df.pivot_table(index='Provider', columns='Model_Size', \n",
    "                                 values='Efficiency_Multiplier', aggfunc='mean')\n",
    "    \n",
    "    im = ax6.imshow(heatmap_data.values, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax6.set_xticks(range(len(heatmap_data.columns)))\n",
    "    ax6.set_yticks(range(len(heatmap_data.index)))\n",
    "    ax6.set_xticklabels(heatmap_data.columns)\n",
    "    ax6.set_yticklabels(heatmap_data.index)\n",
    "    ax6.set_title('Cost Efficiency Multiplier\\n(Lower is Better)', fontsize=14, pad=20)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax6, shrink=0.8)\n",
    "    cbar.set_label('Efficiency Multiplier', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(heatmap_data.index)):\n",
    "        for j in range(len(heatmap_data.columns)):\n",
    "            text = ax6.text(j, i, f'{heatmap_data.iloc[i, j]:.2f}',\n",
    "                           ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    # 7. Cost Savings Analysis\n",
    "    ax7 = axes[2, 0]\n",
    "    \n",
    "    # Calculate savings compared to most expensive option\n",
    "    savings_data = []\n",
    "    for model_size in df['Model_Size'].unique():\n",
    "        model_data = df[df['Model_Size'] == model_size]\n",
    "        max_cost = model_data['Total_Cost_USD'].max()\n",
    "        \n",
    "        for _, row in model_data.iterrows():\n",
    "            savings_pct = (max_cost - row['Total_Cost_USD']) / max_cost * 100\n",
    "            savings_data.append({\n",
    "                'Model_Size': model_size,\n",
    "                'Provider': row['Provider'],\n",
    "                'Savings_Percent': savings_pct\n",
    "            })\n",
    "    \n",
    "    savings_df = pd.DataFrame(savings_data)\n",
    "    pivot_savings = savings_df.pivot(index='Model_Size', columns='Provider', values='Savings_Percent')\n",
    "    pivot_savings.plot(kind='bar', ax=ax7, width=0.8, alpha=0.8)\n",
    "    ax7.set_title('Cost Savings vs Most Expensive Option', fontsize=14, pad=20)\n",
    "    ax7.set_xlabel('Model Size')\n",
    "    ax7.set_ylabel('Savings (%)')\n",
    "    ax7.legend(title='Provider', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax7.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 8. Resource Utilization Analysis\n",
    "    ax8 = axes[2, 1]\n",
    "    \n",
    "    # Box plot of GPU utilization by provider\n",
    "    utilization_data = [df[df['Provider'] == provider]['GPU_Utilization'].values * 100 \n",
    "                       for provider in df['Provider'].unique()]\n",
    "    \n",
    "    box_plot = ax8.boxplot(utilization_data, labels=df['Provider'].unique(), \n",
    "                          patch_artist=True, notch=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, color in zip(box_plot['boxes'], colors_map):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax8.set_title('GPU Utilization Distribution', fontsize=14, pad=20)\n",
    "    ax8.set_ylabel('GPU Utilization (%)')\n",
    "    ax8.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 9. ROI Analysis\n",
    "    ax9 = axes[2, 2]\n",
    "    \n",
    "    # Simplified ROI calculation (based on cost efficiency)\n",
    "    df_copy = df.copy()\n",
    "    df_copy['ROI_Score'] = 1 / (df_copy['Cost_Per_Parameter_M'] * df_copy['Efficiency_Multiplier'])\n",
    "    \n",
    "    pivot_roi = df_copy.pivot(index='Model_Size', columns='Provider', values='ROI_Score')\n",
    "    pivot_roi.plot(kind='bar', ax=ax9, width=0.8, alpha=0.8)\n",
    "    ax9.set_title('Training ROI Score\\n(Higher is Better)', fontsize=14, pad=20)\n",
    "    ax9.set_xlabel('Model Size')\n",
    "    ax9.set_ylabel('ROI Score')\n",
    "    ax9.legend(title='Provider', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax9.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def print_cost_optimization_recommendations(recommendations: Dict):\n",
    "    \"\"\"Print comprehensive cost optimization recommendations.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üí∞ COMPREHENSIVE COST OPTIMIZATION RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Provider rankings\n",
    "    print(\"\\nüèÜ PROVIDER COST RANKINGS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_size, rankings in recommendations['provider_rankings'].items():\n",
    "        print(f\"\\n{model_size} Parameter Model:\")\n",
    "        for i, ranking in enumerate(rankings, 1):\n",
    "            savings = ranking['savings_percent']\n",
    "            duration = ranking['duration_days']\n",
    "            cost = ranking['cost']\n",
    "            \n",
    "            print(f\"  {i}. {ranking['provider']:<12} ${cost:>8,.0f} ({savings:>4.1f}% savings, {duration:.1f} days)\")\n",
    "    \n",
    "    # Cost optimization strategies\n",
    "    print(\"\\nüéØ COST OPTIMIZATION STRATEGIES:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    strategies = recommendations['cost_optimization_strategies']\n",
    "    \n",
    "    for strategy, details in strategies.items():\n",
    "        print(f\"\\n{strategy.replace('_', ' ').title()}:\")\n",
    "        print(f\"  ‚Ä¢ Potential Savings: {details.get('potential_savings', 'Variable')}\")\n",
    "        print(f\"  ‚Ä¢ Implementation: {details['implementation']}\")\n",
    "        print(f\"  ‚Ä¢ Considerations: {details.get('considerations', 'Standard implementation')}\")\n",
    "    \n",
    "    # Scaling insights\n",
    "    print(\"\\nüìà SCALING ECONOMICS INSIGHTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scaling = recommendations['scaling_insights']\n",
    "    \n",
    "    print(\"\\nCost per Parameter by Model Size:\")\n",
    "    for model_size, cost_per_param in scaling['cost_per_parameter_scaling'].items():\n",
    "        print(f\"  ‚Ä¢ {model_size}: ${cost_per_param:.2f} per million parameters\")\n",
    "    \n",
    "    print(f\"\\n‚Ä¢ {scaling['scaling_efficiency']}\")\n",
    "    print(f\"‚Ä¢ Sweet Spot: {scaling['sweet_spot']}\")\n",
    "    print(f\"‚Ä¢ Diminishing Returns: {scaling['diminishing_returns']}\")\n",
    "    \n",
    "    # Efficiency improvements\n",
    "    print(\"\\n‚ö° EFFICIENCY IMPROVEMENT OPPORTUNITIES:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    efficiency = recommendations['efficiency_improvements']\n",
    "    \n",
    "    print(f\"\\nCurrent Performance:\")\n",
    "    print(f\"  ‚Ä¢ Average GPU Utilization: {efficiency['current_average_utilization']}\")\n",
    "    print(f\"  ‚Ä¢ Target Utilization: {efficiency['target_utilization']}\")\n",
    "    \n",
    "    print(\"\\nImprovement Strategies:\")\n",
    "    for strategy in efficiency['improvement_strategies']:\n",
    "        print(f\"  ‚Ä¢ {strategy}\")\n",
    "    \n",
    "    print(\"\\nMonitoring Recommendations:\")\n",
    "    for recommendation in efficiency['monitoring_recommendations']:\n",
    "        print(f\"  ‚Ä¢ {recommendation}\")\n",
    "\n",
    "def create_cost_summary_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a summary table of key cost metrics.\"\"\"\n",
    "    \n",
    "    summary_stats = df.groupby(['Model_Size', 'Provider']).agg({\n",
    "        'Total_Cost_USD': 'first',\n",
    "        'Duration_Days': 'first', \n",
    "        'Daily_Burn_Rate': 'first',\n",
    "        'Cost_Per_Parameter_M': 'first',\n",
    "        'Efficiency_Multiplier': 'first'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Find the most cost-effective option for each model size\n",
    "    best_options = []\n",
    "    for model_size in df['Model_Size'].unique():\n",
    "        model_data = df[df['Model_Size'] == model_size]\n",
    "        best_option = model_data.loc[model_data['Total_Cost_USD'].idxmin()]\n",
    "        best_options.append(best_option)\n",
    "    \n",
    "    return summary_stats, pd.DataFrame(best_options)\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "print(\"\\nüìä Creating Comprehensive Cost Visualizations...\")\n",
    "fig = create_comprehensive_cost_visualizations(comparison_df, optimization_recommendations)\n",
    "\n",
    "# Print recommendations\n",
    "print_cost_optimization_recommendations(optimization_recommendations)\n",
    "\n",
    "# Create summary tables\n",
    "print(\"\\nüìã COST SUMMARY TABLES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "summary_table, best_options_table = create_cost_summary_table(comparison_df)\n",
    "\n",
    "print(\"\\nMost Cost-Effective Options by Model Size:\")\n",
    "print(best_options_table[['Model_Size', 'Provider', 'Total_Cost_USD', 'Duration_Days', 'Daily_Burn_Rate']].to_string(index=False, float_format='{:,.0f}'.format))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Chapter 9: Cost Optimization & Operations Complete!\")\n",
    "\n",
    "print(\"\\nüìö Key Learning Outcomes:\")\n",
    "print(\"  ‚Ä¢ Comprehensive understanding of LLM training cost structures\")\n",
    "print(\"  ‚Ä¢ Advanced multi-cloud cost optimization strategies\")\n",
    "print(\"  ‚Ä¢ Real-world financial modeling and ROI analysis\")\n",
    "print(\"  ‚Ä¢ Production-grade cost monitoring and alerting systems\")\n",
    "print(\"  ‚Ä¢ Strategic insights for sustainable AI infrastructure investment\")\n",
    "\n",
    "print(\"\\nüéì Course Complete!\")\n",
    "print(\"üåü You've mastered LLM profiling, optimization, and cost-effective deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}