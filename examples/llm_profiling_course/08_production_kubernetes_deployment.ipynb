{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Production Kubernetes Deployment\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will:\n",
    "- **Master Kubernetes GPU resource management** for LLM workloads\n",
    "- **Implement auto-scaling strategies** with custom metrics\n",
    "- **Deploy production-grade monitoring** and observability\n",
    "- **Build fault-tolerant LLM services** with high availability\n",
    "- **Optimize costs** through intelligent resource allocation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ Why Kubernetes for LLM Production?\n",
    "\n",
    "### **The Production Reality**\n",
    "\n",
    "Running LLMs in production requires solving complex operational challenges:\n",
    "\n",
    "#### **Scale Challenges**\n",
    "- **Dynamic demand**: Traffic varies 10x between peak and off-peak\n",
    "- **GPU scarcity**: Limited, expensive compute resources\n",
    "- **Multi-model serving**: Different models for different use cases\n",
    "- **Geographic distribution**: Global user base requires edge deployment\n",
    "\n",
    "#### **Reliability Requirements**\n",
    "- **99.9% uptime**: Downtime costs millions in revenue\n",
    "- **Graceful degradation**: Maintain service during partial failures\n",
    "- **Rolling updates**: Deploy new models without service interruption\n",
    "- **Disaster recovery**: Multi-region failover capabilities\n",
    "\n",
    "#### **Cost Optimization**\n",
    "- **GPU costs**: $2-8/hour per GPU in cloud environments\n",
    "- **Utilization optimization**: Keep expensive GPUs busy\n",
    "- **Right-sizing**: Match resource allocation to actual needs\n",
    "- **Spot instances**: 70% savings with preemption handling\n",
    "\n",
    "### **Kubernetes Advantages**\n",
    "\n",
    "**Kubernetes** provides enterprise-grade orchestration specifically suited for AI workloads:\n",
    "\n",
    "#### **Native GPU Support**\n",
    "- **Device plugins**: First-class GPU resource management\n",
    "- **Resource quotas**: Prevent GPU resource monopolization\n",
    "- **Node affinity**: Schedule workloads on appropriate hardware\n",
    "- **Multi-GPU allocation**: Support for complex model architectures\n",
    "\n",
    "#### **Auto-scaling Excellence**\n",
    "- **HPA**: Horizontal scaling based on custom metrics\n",
    "- **VPA**: Vertical scaling for right-sizing resources\n",
    "- **Cluster autoscaler**: Dynamic node provisioning\n",
    "- **Custom controllers**: Domain-specific scaling logic\n",
    "\n",
    "#### **Production Features**\n",
    "- **Service mesh**: Load balancing, circuit breaking, observability\n",
    "- **Secrets management**: Secure API keys and credentials\n",
    "- **Configuration management**: Environment-specific settings\n",
    "- **Health checks**: Automatic failure detection and recovery\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Production-Ready Kubernetes Manifests\n",
    "\n",
    "Let's build comprehensive Kubernetes manifests for LLM deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class KubernetesManifestGenerator:\n",
    "    \"\"\"\n",
    "    Production-grade Kubernetes manifest generator for LLM deployments\n",
    "    \n",
    "    Educational Focus:\n",
    "    This class demonstrates best practices for deploying ML workloads\n",
    "    in production Kubernetes environments with proper resource management,\n",
    "    monitoring, and scalability configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Deployment configuration\n",
    "    namespace: str = \"llm-inference\"\n",
    "    app_name: str = \"llm-server\"\n",
    "    model_name: str = \"llama2-7b-chat\"\n",
    "    image: str = \"vllm/vllm-openai:v0.2.0\"\n",
    "    \n",
    "    # Resource configuration\n",
    "    gpu_count: int = 1\n",
    "    gpu_type: str = \"nvidia-tesla-t4\"\n",
    "    cpu_request: str = \"4\"\n",
    "    cpu_limit: str = \"8\"\n",
    "    memory_request: str = \"16Gi\"\n",
    "    memory_limit: str = \"32Gi\"\n",
    "    \n",
    "    # Scaling configuration\n",
    "    min_replicas: int = 2\n",
    "    max_replicas: int = 10\n",
    "    \n",
    "    def generate_namespace(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate namespace with resource quotas\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"apiVersion\": \"v1\",\n",
    "            \"kind\": \"Namespace\",\n",
    "            \"metadata\": {\n",
    "                \"name\": self.namespace,\n",
    "                \"labels\": {\n",
    "                    \"purpose\": \"ai-inference\",\n",
    "                    \"tier\": \"production\",\n",
    "                    \"team\": \"ml-platform\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_resource_quota(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate resource quota to prevent resource monopolization\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"apiVersion\": \"v1\",\n",
    "            \"kind\": \"ResourceQuota\",\n",
    "            \"metadata\": {\n",
    "                \"name\": f\"{self.app_name}-resource-quota\",\n",
    "                \"namespace\": self.namespace\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"hard\": {\n",
    "                    \"requests.nvidia.com/gpu\": \"20\",  # Max 20 GPUs\n",
    "                    \"limits.nvidia.com/gpu\": \"20\",\n",
    "                    \"requests.cpu\": \"80\",  # Max 80 CPU cores\n",
    "                    \"requests.memory\": \"320Gi\",  # Max 320GB RAM\n",
    "                    \"persistentvolumeclaims\": \"10\",  # Max 10 PVCs\n",
    "                    \"pods\": \"50\"  # Max 50 pods\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_configmap(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate ConfigMap for application configuration\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"apiVersion\": \"v1\",\n",
    "            \"kind\": \"ConfigMap\",\n",
    "            \"metadata\": {\n",
    "                \"name\": f\"{self.app_name}-config\",\n",
    "                \"namespace\": self.namespace\n",
    "            },\n",
    "            \"data\": {\n",
    "                # Model configuration\n",
    "                \"MODEL_NAME\": self.model_name,\n",
    "                \"MAX_SEQUENCE_LENGTH\": \"4096\",\n",
    "                \"MAX_BATCH_SIZE\": \"32\",\n",
    "                \"TENSOR_PARALLEL_SIZE\": \"1\",\n",
    "                \n",
    "                # vLLM configuration\n",
    "                \"GPU_MEMORY_UTILIZATION\": \"0.9\",\n",
    "                \"MAX_NUM_BATCHED_TOKENS\": \"8192\",\n",
    "                \"MAX_NUM_SEQS\": \"256\",\n",
    "                \"BLOCK_SIZE\": \"16\",\n",
    "                \n",
    "                # Server configuration\n",
    "                \"SERVER_PORT\": \"8000\",\n",
    "                \"HEALTH_CHECK_PATH\": \"/health\",\n",
    "                \"METRICS_PORT\": \"9090\",\n",
    "                \n",
    "                # Performance tuning\n",
    "                \"CUDA_VISIBLE_DEVICES\": \"0\",\n",
    "                \"PYTORCH_CUDA_ALLOC_CONF\": \"max_split_size_mb:128\",\n",
    "                \"VLLM_ENGINE_ITERATION_TIMEOUT_S\": \"60\",\n",
    "                \n",
    "                # Logging and monitoring\n",
    "                \"LOG_LEVEL\": \"INFO\",\n",
    "                \"ENABLE_PROMETHEUS_METRICS\": \"true\",\n",
    "                \"ENABLE_DISTRIBUTED_TRACING\": \"true\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_deployment(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate production-ready Deployment manifest\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"apiVersion\": \"apps/v1\",\n",
    "            \"kind\": \"Deployment\",\n",
    "            \"metadata\": {\n",
    "                \"name\": f\"{self.app_name}-deployment\",\n",
    "                \"namespace\": self.namespace,\n",
    "                \"labels\": {\n",
    "                    \"app\": self.app_name,\n",
    "                    \"component\": \"inference\",\n",
    "                    \"version\": \"v1.0\",\n",
    "                    \"model\": self.model_name\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"replicas\": self.min_replicas,\n",
    "                \"strategy\": {\n",
    "                    \"type\": \"RollingUpdate\",\n",
    "                    \"rollingUpdate\": {\n",
    "                        \"maxSurge\": 1,\n",
    "                        \"maxUnavailable\": 0  # Zero downtime deployment\n",
    "                    }\n",
    "                },\n",
    "                \"selector\": {\n",
    "                    \"matchLabels\": {\n",
    "                        \"app\": self.app_name\n",
    "                    }\n",
    "                },\n",
    "                \"template\": {\n",
    "                    \"metadata\": {\n",
    "                        \"labels\": {\n",
    "                            \"app\": self.app_name,\n",
    "                            \"component\": \"inference\",\n",
    "                            \"version\": \"v1.0\"\n",
    "                        },\n",
    "                        \"annotations\": {\n",
    "                            # Prometheus scraping\n",
    "                            \"prometheus.io/scrape\": \"true\",\n",
    "                            \"prometheus.io/port\": \"9090\",\n",
    "                            \"prometheus.io/path\": \"/metrics\",\n",
    "                            \n",
    "                            # Istio injection (if using service mesh)\n",
    "                            \"sidecar.istio.io/inject\": \"true\",\n",
    "                            \n",
    "                            # Resource tracking\n",
    "                            \"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"spec\": {\n",
    "                        # Node selection for GPU nodes\n",
    "                        \"nodeSelector\": {\n",
    "                            \"accelerator\": self.gpu_type\n",
    "                        },\n",
    "                        \n",
    "                        # Tolerations for GPU node taints\n",
    "                        \"tolerations\": [\n",
    "                            {\n",
    "                                \"key\": \"nvidia.com/gpu\",\n",
    "                                \"operator\": \"Exists\",\n",
    "                                \"effect\": \"NoSchedule\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"key\": \"kubernetes.io/arch\",\n",
    "                                \"operator\": \"Equal\",\n",
    "                                \"value\": \"amd64\",\n",
    "                                \"effect\": \"NoSchedule\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \n",
    "                        # Affinity rules for better scheduling\n",
    "                        \"affinity\": {\n",
    "                            \"podAntiAffinity\": {\n",
    "                                \"preferredDuringSchedulingIgnoredDuringExecution\": [\n",
    "                                    {\n",
    "                                        \"weight\": 100,\n",
    "                                        \"podAffinityTerm\": {\n",
    "                                            \"labelSelector\": {\n",
    "                                                \"matchExpressions\": [\n",
    "                                                    {\n",
    "                                                        \"key\": \"app\",\n",
    "                                                        \"operator\": \"In\",\n",
    "                                                        \"values\": [self.app_name]\n",
    "                                                    }\n",
    "                                                ]\n",
    "                                            },\n",
    "                                            \"topologyKey\": \"kubernetes.io/hostname\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \n",
    "                        \"containers\": [\n",
    "                            {\n",
    "                                \"name\": \"vllm-server\",\n",
    "                                \"image\": self.image,\n",
    "                                \"imagePullPolicy\": \"Always\",\n",
    "                                \n",
    "                                # Resource requirements (CRITICAL for GPU scheduling)\n",
    "                                \"resources\": {\n",
    "                                    \"requests\": {\n",
    "                                        \"nvidia.com/gpu\": str(self.gpu_count),\n",
    "                                        \"cpu\": self.cpu_request,\n",
    "                                        \"memory\": self.memory_request\n",
    "                                    },\n",
    "                                    \"limits\": {\n",
    "                                        \"nvidia.com/gpu\": str(self.gpu_count),\n",
    "                                        \"cpu\": self.cpu_limit,\n",
    "                                        \"memory\": self.memory_limit\n",
    "                                    }\n",
    "                                },\n",
    "                                \n",
    "                                # Environment variables from ConfigMap\n",
    "                                \"envFrom\": [\n",
    "                                    {\n",
    "                                        \"configMapRef\": {\n",
    "                                            \"name\": f\"{self.app_name}-config\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                ],\n",
    "                                \n",
    "                                # Additional environment variables\n",
    "                                \"env\": [\n",
    "                                    {\n",
    "                                        \"name\": \"POD_NAME\",\n",
    "                                        \"valueFrom\": {\n",
    "                                            \"fieldRef\": {\n",
    "                                                \"fieldPath\": \"metadata.name\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"POD_IP\",\n",
    "                                        \"valueFrom\": {\n",
    "                                            \"fieldRef\": {\n",
    "                                                \"fieldPath\": \"status.podIP\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"NODE_NAME\",\n",
    "                                        \"valueFrom\": {\n",
    "                                            \"fieldRef\": {\n",
    "                                                \"fieldPath\": \"spec.nodeName\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                ],\n",
    "                                \n",
    "                                # Container ports\n",
    "                                \"ports\": [\n",
    "                                    {\n",
    "                                        \"name\": \"http\",\n",
    "                                        \"containerPort\": 8000,\n",
    "                                        \"protocol\": \"TCP\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"metrics\",\n",
    "                                        \"containerPort\": 9090,\n",
    "                                        \"protocol\": \"TCP\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \n",
    "                                # Health checks (ESSENTIAL for production)\n",
    "                                \"livenessProbe\": {\n",
    "                                    \"httpGet\": {\n",
    "                                        \"path\": \"/health\",\n",
    "                                        \"port\": 8000\n",
    "                                    },\n",
    "                                    \"initialDelaySeconds\": 120,  # Model loading time\n",
    "                                    \"periodSeconds\": 30,\n",
    "                                    \"timeoutSeconds\": 10,\n",
    "                                    \"failureThreshold\": 3,\n",
    "                                    \"successThreshold\": 1\n",
    "                                },\n",
    "                                \n",
    "                                \"readinessProbe\": {\n",
    "                                    \"httpGet\": {\n",
    "                                        \"path\": \"/health\",\n",
    "                                        \"port\": 8000\n",
    "                                    },\n",
    "                                    \"initialDelaySeconds\": 60,\n",
    "                                    \"periodSeconds\": 10,\n",
    "                                    \"timeoutSeconds\": 5,\n",
    "                                    \"failureThreshold\": 3,\n",
    "                                    \"successThreshold\": 1\n",
    "                                },\n",
    "                                \n",
    "                                # Startup probe for slow model loading\n",
    "                                \"startupProbe\": {\n",
    "                                    \"httpGet\": {\n",
    "                                        \"path\": \"/health\",\n",
    "                                        \"port\": 8000\n",
    "                                    },\n",
    "                                    \"initialDelaySeconds\": 30,\n",
    "                                    \"periodSeconds\": 15,\n",
    "                                    \"timeoutSeconds\": 10,\n",
    "                                    \"failureThreshold\": 20  # Up to 5 minutes\n",
    "                                },\n",
    "                                \n",
    "                                # Volume mounts\n",
    "                                \"volumeMounts\": [\n",
    "                                    {\n",
    "                                        \"name\": \"model-cache\",\n",
    "                                        \"mountPath\": \"/root/.cache/huggingface\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"tmp-dir\",\n",
    "                                        \"mountPath\": \"/tmp\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \n",
    "                                # Security context\n",
    "                                \"securityContext\": {\n",
    "                                    \"runAsNonRoot\": False,  # GPU access often requires root\n",
    "                                    \"allowPrivilegeEscalation\": False,\n",
    "                                    \"readOnlyRootFilesystem\": False,\n",
    "                                    \"capabilities\": {\n",
    "                                        \"drop\": [\"ALL\"]\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                        \n",
    "                        # Shared volumes\n",
    "                        \"volumes\": [\n",
    "                            {\n",
    "                                \"name\": \"model-cache\",\n",
    "                                \"persistentVolumeClaim\": {\n",
    "                                    \"claimName\": \"model-cache-pvc\"\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"tmp-dir\",\n",
    "                                \"emptyDir\": {\n",
    "                                    \"sizeLimit\": \"1Gi\"\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                        \n",
    "                        \"restartPolicy\": \"Always\",\n",
    "                        \"terminationGracePeriodSeconds\": 60,\n",
    "                        \n",
    "                        # Pod security context\n",
    "                        \"securityContext\": {\n",
    "                            \"fsGroup\": 1000\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_service(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate Service for load balancing\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"apiVersion\": \"v1\",\n",
    "            \"kind\": \"Service\",\n",
    "            \"metadata\": {\n",
    "                \"name\": f\"{self.app_name}-service\",\n",
    "                \"namespace\": self.namespace,\n",
    "                \"labels\": {\n",
    "                    \"app\": self.app_name\n",
    "                },\n",
    "                \"annotations\": {\n",
    "                    # Cloud provider annotations\n",
    "                    \"service.beta.kubernetes.io/aws-load-balancer-type\": \"nlb\",\n",
    "                    \"service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled\": \"true\",\n",
    "                    \n",
    "                    # Prometheus scraping\n",
    "                    \"prometheus.io/scrape\": \"true\",\n",
    "                    \"prometheus.io/port\": \"9090\",\n",
    "                    \"prometheus.io/path\": \"/metrics\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"type\": \"LoadBalancer\",\n",
    "                \"ports\": [\n",
    "                    {\n",
    "                        \"name\": \"http\",\n",
    "                        \"port\": 80,\n",
    "                        \"targetPort\": 8000,\n",
    "                        \"protocol\": \"TCP\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"https\",\n",
    "                        \"port\": 443,\n",
    "                        \"targetPort\": 8000,\n",
    "                        \"protocol\": \"TCP\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"metrics\",\n",
    "                        \"port\": 9090,\n",
    "                        \"targetPort\": 9090,\n",
    "                        \"protocol\": \"TCP\"\n",
    "                    }\n",
    "                ],\n",
    "                \"selector\": {\n",
    "                    \"app\": self.app_name\n",
    "                },\n",
    "                # Session affinity for consistent routing\n",
    "                \"sessionAffinity\": \"ClientIP\",\n",
    "                \"sessionAffinityConfig\": {\n",
    "                    \"clientIP\": {\n",
    "                        \"timeoutSeconds\": 3600  # 1 hour\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize the manifest generator\n",
    "manifest_generator = KubernetesManifestGenerator(\n",
    "    namespace=\"llm-inference\",\n",
    "    app_name=\"llm-server\",\n",
    "    model_name=\"llama2-7b-chat\",\n",
    "    gpu_type=\"nvidia-tesla-t4\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Kubernetes Manifest Generator Initialized!\")\n",
    "print(f\"ðŸ“¦ Ready to generate production manifests for {manifest_generator.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ Advanced Auto-Scaling Configuration\n",
    "\n",
    "Let's implement sophisticated auto-scaling with custom metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_advanced_autoscaling_manifests(generator: KubernetesManifestGenerator) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate advanced auto-scaling manifests with custom metrics\n",
    "    \n",
    "    Educational Focus:\n",
    "    This demonstrates how to implement intelligent scaling based on\n",
    "    LLM-specific metrics like queue depth, tokens/second, and GPU utilization.\n",
    "    \"\"\"\n",
    "    \n",
    "    manifests = {}\n",
    "    \n",
    "    # 1. Horizontal Pod Autoscaler with multiple metrics\n",
    "    manifests['hpa'] = {\n",
    "        \"apiVersion\": \"autoscaling/v2\",\n",
    "        \"kind\": \"HorizontalPodAutoscaler\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"{generator.app_name}-hpa\",\n",
    "            \"namespace\": generator.namespace\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"scaleTargetRef\": {\n",
    "                \"apiVersion\": \"apps/v1\",\n",
    "                \"kind\": \"Deployment\",\n",
    "                \"name\": f\"{generator.app_name}-deployment\"\n",
    "            },\n",
    "            \"minReplicas\": generator.min_replicas,\n",
    "            \"maxReplicas\": generator.max_replicas,\n",
    "            \n",
    "            # Multi-metric scaling strategy\n",
    "            \"metrics\": [\n",
    "                # CPU-based scaling\n",
    "                {\n",
    "                    \"type\": \"Resource\",\n",
    "                    \"resource\": {\n",
    "                        \"name\": \"cpu\",\n",
    "                        \"target\": {\n",
    "                            \"type\": \"Utilization\",\n",
    "                            \"averageUtilization\": 70\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                # Memory-based scaling\n",
    "                {\n",
    "                    \"type\": \"Resource\",\n",
    "                    \"resource\": {\n",
    "                        \"name\": \"memory\",\n",
    "                        \"target\": {\n",
    "                            \"type\": \"Utilization\",\n",
    "                            \"averageUtilization\": 80\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                # Custom metric: Requests per second\n",
    "                {\n",
    "                    \"type\": \"Pods\",\n",
    "                    \"pods\": {\n",
    "                        \"metric\": {\n",
    "                            \"name\": \"http_requests_per_second\"\n",
    "                        },\n",
    "                        \"target\": {\n",
    "                            \"type\": \"AverageValue\",\n",
    "                            \"averageValue\": \"100\"  # Scale when > 100 RPS per pod\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                # Custom metric: Queue depth\n",
    "                {\n",
    "                    \"type\": \"Object\",\n",
    "                    \"object\": {\n",
    "                        \"metric\": {\n",
    "                            \"name\": \"llm_queue_depth\"\n",
    "                        },\n",
    "                        \"describedObject\": {\n",
    "                            \"apiVersion\": \"v1\",\n",
    "                            \"kind\": \"Service\",\n",
    "                            \"name\": f\"{generator.app_name}-service\"\n",
    "                        },\n",
    "                        \"target\": {\n",
    "                            \"type\": \"Value\",\n",
    "                            \"value\": \"50\"  # Scale when queue depth > 50\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                # Custom metric: GPU utilization\n",
    "                {\n",
    "                    \"type\": \"Pods\",\n",
    "                    \"pods\": {\n",
    "                        \"metric\": {\n",
    "                            \"name\": \"nvidia_gpu_utilization\"\n",
    "                        },\n",
    "                        \"target\": {\n",
    "                            \"type\": \"AverageValue\",\n",
    "                            \"averageValue\": \"85\"  # Scale when GPU util > 85%\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \n",
    "            # Advanced scaling behavior\n",
    "            \"behavior\": {\n",
    "                \"scaleUp\": {\n",
    "                    \"stabilizationWindowSeconds\": 300,  # Wait 5 min before scaling up\n",
    "                    \"policies\": [\n",
    "                        {\n",
    "                            \"type\": \"Percent\",\n",
    "                            \"value\": 50,  # Scale up by max 50%\n",
    "                            \"periodSeconds\": 60\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"Pods\",\n",
    "                            \"value\": 2,  # Or add max 2 pods\n",
    "                            \"periodSeconds\": 60\n",
    "                        }\n",
    "                    ],\n",
    "                    \"selectPolicy\": \"Min\"  # Choose more conservative\n",
    "                },\n",
    "                \"scaleDown\": {\n",
    "                    \"stabilizationWindowSeconds\": 600,  # Wait 10 min before scaling down\n",
    "                    \"policies\": [\n",
    "                        {\n",
    "                            \"type\": \"Percent\",\n",
    "                            \"value\": 25,  # Scale down by max 25%\n",
    "                            \"periodSeconds\": 60\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 2. Vertical Pod Autoscaler for right-sizing\n",
    "    manifests['vpa'] = {\n",
    "        \"apiVersion\": \"autoscaling.k8s.io/v1\",\n",
    "        \"kind\": \"VerticalPodAutoscaler\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"{generator.app_name}-vpa\",\n",
    "            \"namespace\": generator.namespace\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"targetRef\": {\n",
    "                \"apiVersion\": \"apps/v1\",\n",
    "                \"kind\": \"Deployment\",\n",
    "                \"name\": f\"{generator.app_name}-deployment\"\n",
    "            },\n",
    "            \"updatePolicy\": {\n",
    "                \"updateMode\": \"Off\"  # Only provide recommendations\n",
    "            },\n",
    "            \"resourcePolicy\": {\n",
    "                \"containerPolicies\": [\n",
    "                    {\n",
    "                        \"containerName\": \"vllm-server\",\n",
    "                        \"minAllowed\": {\n",
    "                            \"cpu\": \"2\",\n",
    "                            \"memory\": \"8Gi\"\n",
    "                        },\n",
    "                        \"maxAllowed\": {\n",
    "                            \"cpu\": \"16\",\n",
    "                            \"memory\": \"64Gi\"\n",
    "                        },\n",
    "                        \"controlledResources\": [\"cpu\", \"memory\"]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 3. Pod Disruption Budget for high availability\n",
    "    manifests['pdb'] = {\n",
    "        \"apiVersion\": \"policy/v1\",\n",
    "        \"kind\": \"PodDisruptionBudget\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"{generator.app_name}-pdb\",\n",
    "            \"namespace\": generator.namespace\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"minAvailable\": \"50%\",  # Keep at least 50% of pods running\n",
    "            \"selector\": {\n",
    "                \"matchLabels\": {\n",
    "                    \"app\": generator.app_name\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 4. Storage configuration\n",
    "    manifests['pvc'] = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"PersistentVolumeClaim\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"model-cache-pvc\",\n",
    "            \"namespace\": generator.namespace\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"accessModes\": [\"ReadWriteMany\"],  # Multiple pods can access\n",
    "            \"resources\": {\n",
    "                \"requests\": {\n",
    "                    \"storage\": \"200Gi\"  # 200GB for model cache\n",
    "                }\n",
    "            },\n",
    "            \"storageClassName\": \"fast-ssd\"  # Use high-performance storage\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return manifests\n",
    "\n",
    "# Generate advanced auto-scaling manifests\n",
    "autoscaling_manifests = generate_advanced_autoscaling_manifests(manifest_generator)\n",
    "\n",
    "print(\"âœ… Advanced Auto-Scaling Manifests Generated!\")\n",
    "print(f\"ðŸ“ˆ Generated {len(autoscaling_manifests)} advanced configurations:\")\n",
    "for manifest_type in autoscaling_manifests.keys():\n",
    "    print(f\"   â€¢ {manifest_type.upper()}: Advanced {manifest_type} configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Production Monitoring and Observability\n",
    "\n",
    "Let's implement comprehensive monitoring for LLM services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monitoring_manifests(generator: KubernetesManifestGenerator) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive monitoring and observability manifests\n",
    "    \n",
    "    Educational Focus:\n",
    "    This demonstrates production-grade monitoring setup for LLM services\n",
    "    including metrics collection, alerting, and distributed tracing.\n",
    "    \"\"\"\n",
    "    \n",
    "    manifests = {}\n",
    "    \n",
    "    # 1. ServiceMonitor for Prometheus\n",
    "    manifests['servicemonitor'] = {\n",
    "        \"apiVersion\": \"monitoring.coreos.com/v1\",\n",
    "        \"kind\": \"ServiceMonitor\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"{generator.app_name}-metrics\",\n",
    "            \"namespace\": generator.namespace,\n",
    "            \"labels\": {\n",
    "                \"app\": generator.app_name,\n",
    "                \"release\": \"prometheus\"\n",
    "            }\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"selector\": {\n",
    "                \"matchLabels\": {\n",
    "                    \"app\": generator.app_name\n",
    "                }\n",
    "            },\n",
    "            \"endpoints\": [\n",
    "                {\n",
    "                    \"port\": \"metrics\",\n",
    "                    \"path\": \"/metrics\",\n",
    "                    \"interval\": \"30s\",\n",
    "                    \"scrapeTimeout\": \"10s\",\n",
    "                    \"honorLabels\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 2. PrometheusRule for alerting\n",
    "    manifests['prometheusrule'] = {\n",
    "        \"apiVersion\": \"monitoring.coreos.com/v1\",\n",
    "        \"kind\": \"PrometheusRule\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"{generator.app_name}-alerts\",\n",
    "            \"namespace\": generator.namespace,\n",
    "            \"labels\": {\n",
    "                \"app\": generator.app_name,\n",
    "                \"release\": \"prometheus\"\n",
    "            }\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"groups\": [\n",
    "                {\n",
    "                    \"name\": f\"{generator.app_name}.rules\",\n",
    "                    \"rules\": [\n",
    "                        # High latency alert\n",
    "                        {\n",
    "                            \"alert\": \"LLMHighLatency\",\n",
    "                            \"expr\": 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 10',\n",
    "                            \"for\": \"5m\",\n",
    "                            \"labels\": {\n",
    "                                \"severity\": \"warning\",\n",
    "                                \"service\": generator.app_name\n",
    "                            },\n",
    "                            \"annotations\": {\n",
    "                                \"summary\": \"LLM inference latency is high\",\n",
    "                                \"description\": \"95th percentile latency is {{ $value }}s for service {{ $labels.service }}\"\n",
    "                            }\n",
    "                        },\n",
    "                        # High error rate alert\n",
    "                        {\n",
    "                            \"alert\": \"LLMHighErrorRate\",\n",
    "                            \"expr\": 'rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m]) > 0.05',\n",
    "                            \"for\": \"5m\",\n",
    "                            \"labels\": {\n",
    "                                \"severity\": \"critical\",\n",
    "                                \"service\": generator.app_name\n",
    "                            },\n",
    "                            \"annotations\": {\n",
    "                                \"summary\": \"LLM service has high error rate\",\n",
    "                                \"description\": \"Error rate is {{ $value | humanizePercentage }} for service {{ $labels.service }}\"\n",
    "                            }\n",
    "                        },\n",
    "                        # GPU utilization alert\n",
    "                        {\n",
    "                            \"alert\": \"LLMGPUUtilizationLow\",\n",
    "                            \"expr\": 'nvidia_gpu_utilization < 50',\n",
    "                            \"for\": \"10m\",\n",
    "                            \"labels\": {\n",
    "                                \"severity\": \"info\",\n",
    "                                \"service\": generator.app_name\n",
    "                            },\n",
    "                            \"annotations\": {\n",
    "                                \"summary\": \"LLM GPU utilization is low\",\n",
    "                                \"description\": \"GPU utilization is {{ $value }}% on {{ $labels.instance }} - consider scaling down\"\n",
    "                            }\n",
    "                        },\n",
    "                        # Memory pressure alert\n",
    "                        {\n",
    "                            \"alert\": \"LLMMemoryPressure\",\n",
    "                            \"expr\": 'container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9',\n",
    "                            \"for\": \"2m\",\n",
    "                            \"labels\": {\n",
    "                                \"severity\": \"critical\",\n",
    "                                \"service\": generator.app_name\n",
    "                            },\n",
    "                            \"annotations\": {\n",
    "                                \"summary\": \"LLM container memory pressure\",\n",
    "                                \"description\": \"Memory usage is {{ $value | humanizePercentage }} on {{ $labels.pod }}\"\n",
    "                            }\n",
    "                        },\n",
    "                        # Pod crash loop alert\n",
    "                        {\n",
    "                            \"alert\": \"LLMPodCrashLooping\",\n",
    "                            \"expr\": 'rate(kube_pod_container_status_restarts_total[15m]) > 0',\n",
    "                            \"for\": \"5m\",\n",
    "                            \"labels\": {\n",
    "                                \"severity\": \"critical\",\n",
    "                                \"service\": generator.app_name\n",
    "                            },\n",
    "                            \"annotations\": {\n",
    "                                \"summary\": \"LLM pod is crash looping\",\n",
    "                                \"description\": \"Pod {{ $labels.pod }} is restarting frequently\"\n",
    "                            }\n",
    "                        },\n",
    "                        # Queue depth alert\n",
    "                        {\n",
    "                            \"alert\": \"LLMQueueDepthHigh\",\n",
    "                            \"expr\": 'llm_queue_depth > 100',\n",
    "                            \"for\": \"2m\",\n",
    "                            \"labels\": {\n",
    "                                \"severity\": \"warning\",\n",
    "                                \"service\": generator.app_name\n",
    "                            },\n",
    "                            \"annotations\": {\n",
    "                                \"summary\": \"LLM request queue depth is high\",\n",
    "                                \"description\": \"Queue depth is {{ $value }} requests - consider scaling up\"\n",
    "                            }\n",
    "                        },\n",
    "                        # Service availability alert\n",
    "                        {\n",
    "                            \"alert\": \"LLMServiceDown\",\n",
    "                            \"expr\": 'up == 0',\n",
    "                            \"for\": \"1m\",\n",
    "                            \"labels\": {\n",
    "                                \"severity\": \"critical\",\n",
    "                                \"service\": generator.app_name\n",
    "                            },\n",
    "                            \"annotations\": {\n",
    "                                \"summary\": \"LLM service is down\",\n",
    "                                \"description\": \"Service {{ $labels.service }} is not responding on {{ $labels.instance }}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 3. Grafana Dashboard ConfigMap\n",
    "    dashboard_json = {\n",
    "        \"dashboard\": {\n",
    "            \"id\": None,\n",
    "            \"title\": f\"LLM Inference - {generator.model_name}\",\n",
    "            \"tags\": [\"llm\", \"inference\", \"ai\"],\n",
    "            \"timezone\": \"UTC\",\n",
    "            \"panels\": [\n",
    "                {\n",
    "                    \"id\": 1,\n",
    "                    \"title\": \"Request Rate\",\n",
    "                    \"type\": \"graph\",\n",
    "                    \"targets\": [\n",
    "                        {\n",
    "                            \"expr\": \"rate(http_requests_total[5m])\",\n",
    "                            \"legendFormat\": \"{{ instance }}\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"id\": 2,\n",
    "                    \"title\": \"Response Time (P95)\",\n",
    "                    \"type\": \"graph\",\n",
    "                    \"targets\": [\n",
    "                        {\n",
    "                            \"expr\": \"histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\",\n",
    "                            \"legendFormat\": \"P95 Latency\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"id\": 3,\n",
    "                    \"title\": \"GPU Utilization\",\n",
    "                    \"type\": \"graph\",\n",
    "                    \"targets\": [\n",
    "                        {\n",
    "                            \"expr\": \"nvidia_gpu_utilization\",\n",
    "                            \"legendFormat\": \"GPU {{ gpu }}\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"id\": 4,\n",
    "                    \"title\": \"Memory Usage\",\n",
    "                    \"type\": \"graph\",\n",
    "                    \"targets\": [\n",
    "                        {\n",
    "                            \"expr\": \"container_memory_usage_bytes / container_spec_memory_limit_bytes\",\n",
    "                            \"legendFormat\": \"{{ pod }}\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"id\": 5,\n",
    "                    \"title\": \"Queue Depth\",\n",
    "                    \"type\": \"graph\",\n",
    "                    \"targets\": [\n",
    "                        {\n",
    "                            \"expr\": \"llm_queue_depth\",\n",
    "                            \"legendFormat\": \"Queue Depth\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"id\": 6,\n",
    "                    \"title\": \"Tokens per Second\",\n",
    "                    \"type\": \"graph\",\n",
    "                    \"targets\": [\n",
    "                        {\n",
    "                            \"expr\": \"rate(llm_tokens_generated_total[5m])\",\n",
    "                            \"legendFormat\": \"Tokens/sec\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"time\": {\n",
    "                \"from\": \"now-1h\",\n",
    "                \"to\": \"now\"\n",
    "            },\n",
    "            \"refresh\": \"30s\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    manifests['grafana_dashboard'] = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"ConfigMap\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"{generator.app_name}-dashboard\",\n",
    "            \"namespace\": generator.namespace,\n",
    "            \"labels\": {\n",
    "                \"grafana_dashboard\": \"1\"  # Auto-discovery by Grafana\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"dashboard.json\": json.dumps(dashboard_json, indent=2)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 4. Network Policy for security\n",
    "    manifests['networkpolicy'] = {\n",
    "        \"apiVersion\": \"networking.k8s.io/v1\",\n",
    "        \"kind\": \"NetworkPolicy\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"{generator.app_name}-netpol\",\n",
    "            \"namespace\": generator.namespace\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"podSelector\": {\n",
    "                \"matchLabels\": {\n",
    "                    \"app\": generator.app_name\n",
    "                }\n",
    "            },\n",
    "            \"policyTypes\": [\"Ingress\", \"Egress\"],\n",
    "            \"ingress\": [\n",
    "                {\n",
    "                    \"from\": [\n",
    "                        {\n",
    "                            \"namespaceSelector\": {\n",
    "                                \"matchLabels\": {\n",
    "                                    \"name\": \"ingress-nginx\"\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"namespaceSelector\": {\n",
    "                                \"matchLabels\": {\n",
    "                                    \"name\": \"monitoring\"\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"protocol\": \"TCP\",\n",
    "                            \"port\": 8000\n",
    "                        },\n",
    "                        {\n",
    "                            \"protocol\": \"TCP\",\n",
    "                            \"port\": 9090\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"egress\": [\n",
    "                {\n",
    "                    \"to\": [],  # Allow all egress (for model downloads, etc.)\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"protocol\": \"TCP\",\n",
    "                            \"port\": 443  # HTTPS\n",
    "                        },\n",
    "                        {\n",
    "                            \"protocol\": \"TCP\",\n",
    "                            \"port\": 80   # HTTP\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return manifests\n",
    "\n",
    "# Generate monitoring manifests\n",
    "monitoring_manifests = generate_monitoring_manifests(manifest_generator)\n",
    "\n",
    "print(\"âœ… Production Monitoring Manifests Generated!\")\n",
    "print(f\"ðŸ“Š Generated {len(monitoring_manifests)} monitoring configurations:\")\n",
    "for manifest_type in monitoring_manifests.keys():\n",
    "    print(f\"   â€¢ {manifest_type.upper()}: Production-grade {manifest_type}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Monitoring Features:\")\n",
    "print(\"   ðŸ“ˆ Prometheus metrics collection with 30s intervals\")\n",
    "print(\"   ðŸš¨ 7 comprehensive alerts for production scenarios\")\n",
    "print(\"   ðŸ“Š Grafana dashboard with 6 key performance panels\")\n",
    "print(\"   ðŸ”’ Network policies for security isolation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Complete Manifest Generation and Deployment Guide\n",
    "\n",
    "Let's generate all manifests and create a deployment guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complete_deployment_package():\n",
    "    \"\"\"\n",
    "    Generate complete deployment package with all manifests and deployment guide\n",
    "    \n",
    "    Educational Focus:\n",
    "    This demonstrates how to organize and package a complete production\n",
    "    deployment with proper documentation and deployment procedures.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ—ï¸ Generating Complete Production Deployment Package\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate all manifest categories\n",
    "    all_manifests = {}\n",
    "    \n",
    "    # Core manifests\n",
    "    print(\"ðŸ“¦ Generating core manifests...\")\n",
    "    all_manifests['namespace'] = manifest_generator.generate_namespace()\n",
    "    all_manifests['resource_quota'] = manifest_generator.generate_resource_quota()\n",
    "    all_manifests['configmap'] = manifest_generator.generate_configmap()\n",
    "    all_manifests['deployment'] = manifest_generator.generate_deployment()\n",
    "    all_manifests['service'] = manifest_generator.generate_service()\n",
    "    \n",
    "    # Auto-scaling manifests\n",
    "    print(\"âš–ï¸ Generating auto-scaling manifests...\")\n",
    "    autoscaling = generate_advanced_autoscaling_manifests(manifest_generator)\n",
    "    all_manifests.update(autoscaling)\n",
    "    \n",
    "    # Monitoring manifests\n",
    "    print(\"ðŸ“Š Generating monitoring manifests...\")\n",
    "    monitoring = generate_monitoring_manifests(manifest_generator)\n",
    "    all_manifests.update(monitoring)\n",
    "    \n",
    "    # Generate YAML files\n",
    "    manifest_yamls = {}\n",
    "    for name, manifest in all_manifests.items():\n",
    "        manifest_yamls[name] = yaml.dump(manifest, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    # Generate deployment script\n",
    "    deployment_script = f'''#!/bin/bash\n",
    "# Production LLM Deployment Script\n",
    "# Model: {manifest_generator.model_name}\n",
    "# Generated: $(date)\n",
    "\n",
    "set -e  # Exit on error\n",
    "\n",
    "echo \"ðŸš€ Starting LLM Production Deployment\"\n",
    "echo \"Model: {manifest_generator.model_name}\"\n",
    "echo \"Namespace: {manifest_generator.namespace}\"\n",
    "echo \"=\"*50\n",
    "\n",
    "# Check prerequisites\n",
    "echo \"ðŸ” Checking prerequisites...\"\n",
    "kubectl version --client=true\n",
    "if ! kubectl get nodes -l accelerator={manifest_generator.gpu_type} | grep -q \"Ready\"; then\n",
    "    echo \"âŒ No GPU nodes found with accelerator={manifest_generator.gpu_type}\"\n",
    "    echo \"Please ensure GPU nodes are available and properly labeled\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Deploy in correct order\n",
    "echo \"\\nðŸ“¦ Phase 1: Core Infrastructure\"\n",
    "kubectl apply -f namespace.yaml\n",
    "kubectl apply -f resource-quota.yaml\n",
    "kubectl apply -f pvc.yaml\n",
    "kubectl apply -f configmap.yaml\n",
    "\n",
    "echo \"\\nðŸš€ Phase 2: Application Deployment\"\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl apply -f service.yaml\n",
    "kubectl apply -f pdb.yaml\n",
    "\n",
    "echo \"\\nâš–ï¸ Phase 3: Auto-scaling Setup\"\n",
    "kubectl apply -f hpa.yaml\n",
    "kubectl apply -f vpa.yaml\n",
    "\n",
    "echo \"\\nðŸ“Š Phase 4: Monitoring Setup\"\n",
    "kubectl apply -f servicemonitor.yaml\n",
    "kubectl apply -f prometheusrule.yaml\n",
    "kubectl apply -f grafana-dashboard.yaml\n",
    "\n",
    "echo \"\\nðŸ”’ Phase 5: Security Setup\"\n",
    "kubectl apply -f networkpolicy.yaml\n",
    "\n",
    "echo \"\\nâœ… Deployment Complete!\"\n",
    "echo \"\\nðŸ” Checking deployment status...\"\n",
    "kubectl get pods -n {manifest_generator.namespace} -l app={manifest_generator.app_name}\n",
    "kubectl get svc -n {manifest_generator.namespace}\n",
    "kubectl get hpa -n {manifest_generator.namespace}\n",
    "\n",
    "echo \"\\nðŸ“Š Waiting for pods to be ready...\"\n",
    "kubectl wait --for=condition=ready pod -l app={manifest_generator.app_name} -n {manifest_generator.namespace} --timeout=600s\n",
    "\n",
    "echo \"\\nðŸŽ‰ LLM Service is now running!\"\n",
    "echo \"\\nðŸ“‹ Next steps:\"\n",
    "echo \"1. Check service health: kubectl logs -n {manifest_generator.namespace} -l app={manifest_generator.app_name}\"\n",
    "echo \"2. Monitor metrics: kubectl port-forward -n {manifest_generator.namespace} svc/{manifest_generator.app_name}-service 8080:9090\"\n",
    "echo \"3. Test inference: curl http://localhost:8080/v1/chat/completions\"\n",
    "echo \"4. View Grafana dashboard: Import the generated dashboard JSON\"\n",
    "'''\n",
    "    \n",
    "    # Generate cleanup script\n",
    "    cleanup_script = f'''#!/bin/bash\n",
    "# LLM Deployment Cleanup Script\n",
    "\n",
    "echo \"ðŸ§¹ Cleaning up LLM deployment...\"\n",
    "echo \"Namespace: {manifest_generator.namespace}\"\n",
    "echo \"Model: {manifest_generator.model_name}\"\n",
    "\n",
    "read -p \"Are you sure you want to delete the entire deployment? [y/N] \" -n 1 -r\n",
    "echo\n",
    "if [[ $REPLY =~ ^[Yy]$ ]]; then\n",
    "    echo \"Deleting all resources...\"\n",
    "    kubectl delete namespace {manifest_generator.namespace}\n",
    "    echo \"âœ… Cleanup complete!\"\n",
    "else\n",
    "    echo \"âŒ Cleanup cancelled\"\n",
    "fi\n",
    "'''\n",
    "    \n",
    "    # Generate README\n",
    "    readme_content = f'''# LLM Production Deployment: {manifest_generator.model_name}\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "\n",
    "This deployment package contains production-ready Kubernetes manifests for deploying {manifest_generator.model_name} with:\n",
    "\n",
    "- **High Availability**: Multi-replica deployment with pod disruption budgets\n",
    "- **Auto-scaling**: HPA and VPA for dynamic resource management\n",
    "- **Monitoring**: Comprehensive Prometheus metrics and Grafana dashboards\n",
    "- **Security**: Network policies and resource quotas\n",
    "- **Performance**: GPU-optimized scheduling and resource allocation\n",
    "\n",
    "## ðŸ—ï¸ Architecture\n",
    "\n",
    "```\n",
    "Internet â†’ Load Balancer â†’ Service â†’ Pods (GPU-enabled)\n",
    "                â†“\n",
    "         Prometheus â† Metrics\n",
    "                â†“\n",
    "            Grafana Dashboard\n",
    "```\n",
    "\n",
    "## ðŸ“¦ Components\n",
    "\n",
    "### Core Resources\n",
    "- `namespace.yaml`: Isolated namespace with resource quotas\n",
    "- `configmap.yaml`: Application configuration\n",
    "- `deployment.yaml`: Main application deployment\n",
    "- `service.yaml`: Load balancer service\n",
    "- `pvc.yaml`: Persistent volume for model cache\n",
    "\n",
    "### Auto-scaling\n",
    "- `hpa.yaml`: Horizontal Pod Autoscaler with custom metrics\n",
    "- `vpa.yaml`: Vertical Pod Autoscaler for right-sizing\n",
    "- `pdb.yaml`: Pod Disruption Budget for availability\n",
    "\n",
    "### Monitoring\n",
    "- `servicemonitor.yaml`: Prometheus metrics collection\n",
    "- `prometheusrule.yaml`: Alerting rules\n",
    "- `grafana-dashboard.yaml`: Performance dashboard\n",
    "\n",
    "### Security\n",
    "- `networkpolicy.yaml`: Network isolation policies\n",
    "\n",
    "## ðŸš€ Quick Start\n",
    "\n",
    "### Prerequisites\n",
    "- Kubernetes cluster with GPU nodes\n",
    "- NVIDIA GPU Operator installed\n",
    "- Prometheus Operator (for monitoring)\n",
    "- At least {manifest_generator.gpu_count} Ã— {manifest_generator.gpu_type} GPU available\n",
    "\n",
    "### Deploy\n",
    "```bash\n",
    "chmod +x deploy.sh\n",
    "./deploy.sh\n",
    "```\n",
    "\n",
    "### Verify\n",
    "```bash\n",
    "# Check pods\n",
    "kubectl get pods -n {manifest_generator.namespace}\n",
    "\n",
    "# Check service\n",
    "kubectl get svc -n {manifest_generator.namespace}\n",
    "\n",
    "# Check logs\n",
    "kubectl logs -n {manifest_generator.namespace} -l app={manifest_generator.app_name} --tail=100\n",
    "```\n",
    "\n",
    "### Test\n",
    "```bash\n",
    "# Port forward\n",
    "kubectl port-forward -n {manifest_generator.namespace} svc/{manifest_generator.app_name}-service 8080:80\n",
    "\n",
    "# Test inference\n",
    "curl -X POST http://localhost:8080/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{{\n",
    "    \"model\": \"{manifest_generator.model_name}\",\n",
    "    \"messages\": [{{\"role\": \"user\", \"content\": \"Hello!\"}}],\n",
    "    \"max_tokens\": 100\n",
    "  }}'\n",
    "```\n",
    "\n",
    "## ðŸ“Š Monitoring\n",
    "\n",
    "### Key Metrics\n",
    "- Request rate and latency (P95, P99)\n",
    "- GPU utilization and memory usage\n",
    "- Queue depth and throughput\n",
    "- Error rates and availability\n",
    "\n",
    "### Alerts\n",
    "- High latency (>10s P95)\n",
    "- High error rate (>5%)\n",
    "- Low GPU utilization (<50%)\n",
    "- Memory pressure (>90%)\n",
    "- Service down\n",
    "\n",
    "## ðŸ”§ Configuration\n",
    "\n",
    "### Scaling\n",
    "- Min replicas: {manifest_generator.min_replicas}\n",
    "- Max replicas: {manifest_generator.max_replicas}\n",
    "- CPU target: 70% utilization\n",
    "- Memory target: 80% utilization\n",
    "\n",
    "### Resources\n",
    "- GPU: {manifest_generator.gpu_count} Ã— {manifest_generator.gpu_type}\n",
    "- CPU: {manifest_generator.cpu_request}-{manifest_generator.cpu_limit} cores\n",
    "- Memory: {manifest_generator.memory_request}-{manifest_generator.memory_limit}\n",
    "\n",
    "## ðŸ§¹ Cleanup\n",
    "\n",
    "```bash\n",
    "chmod +x cleanup.sh\n",
    "./cleanup.sh\n",
    "```\n",
    "\n",
    "## ðŸ“ž Support\n",
    "\n",
    "For issues or questions:\n",
    "1. Check pod logs: `kubectl logs -n {manifest_generator.namespace} -l app={manifest_generator.app_name}`\n",
    "2. Check events: `kubectl get events -n {manifest_generator.namespace}`\n",
    "3. Check resource usage: `kubectl top pods -n {manifest_generator.namespace}`\n",
    "'''\n",
    "    \n",
    "    # Create deployment package\n",
    "    deployment_package = {\n",
    "        'manifests': manifest_yamls,\n",
    "        'scripts': {\n",
    "            'deploy.sh': deployment_script,\n",
    "            'cleanup.sh': cleanup_script\n",
    "        },\n",
    "        'documentation': {\n",
    "            'README.md': readme_content\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return deployment_package\n",
    "\n",
    "# Generate complete deployment package\n",
    "print(\"ðŸš€ Generating Complete Production Deployment Package...\")\n",
    "deployment_package = generate_complete_deployment_package()\n",
    "\n",
    "print(f\"\\nâœ… Production Deployment Package Generated!\")\n",
    "print(f\"\\nðŸ“¦ Package Contents:\")\n",
    "print(f\"   ðŸ“„ {len(deployment_package['manifests'])} Kubernetes manifests\")\n",
    "print(f\"   ðŸš€ {len(deployment_package['scripts'])} deployment scripts\")\n",
    "print(f\"   ðŸ“š {len(deployment_package['documentation'])} documentation files\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Generated Manifests:\")\n",
    "for manifest_name in deployment_package['manifests'].keys():\n",
    "    print(f\"   â€¢ {manifest_name}.yaml\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Generated Scripts:\")\n",
    "for script_name in deployment_package['scripts'].keys():\n",
    "    print(f\"   â€¢ {script_name}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ready for Production Deployment!\")\n",
    "print(f\"   Model: {manifest_generator.model_name}\")\n",
    "print(f\"   Namespace: {manifest_generator.namespace}\")\n",
    "print(f\"   GPU Type: {manifest_generator.gpu_type}\")\n",
    "print(f\"   Scaling: {manifest_generator.min_replicas}-{manifest_generator.max_replicas} replicas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways from Production Kubernetes Deployment\n",
    "\n",
    "### **Kubernetes is Essential for LLM Production**\n",
    "- **Resource management**: Native GPU support and intelligent scheduling\n",
    "- **Auto-scaling**: Dynamic scaling based on custom LLM metrics\n",
    "- **High availability**: Multi-replica deployments with disruption budgets\n",
    "- **Operational excellence**: Health checks, monitoring, and automated recovery\n",
    "\n",
    "### **Production-Grade Features**\n",
    "- **Zero-downtime deployments**: Rolling updates with proper readiness checks\n",
    "- **Comprehensive monitoring**: 7 alerts covering all critical scenarios\n",
    "- **Security isolation**: Network policies and resource quotas\n",
    "- **Cost optimization**: Right-sizing through VPA and intelligent scaling\n",
    "\n",
    "### **Operational Considerations**\n",
    "- **GPU scheduling**: Proper node selection and resource allocation\n",
    "- **Storage management**: Persistent volumes for model caching\n",
    "- **Network policies**: Security isolation without breaking functionality\n",
    "- **Resource quotas**: Prevent resource monopolization and cost runaway\n",
    "\n",
    "### **Monitoring and Alerting**\n",
    "- **Custom metrics**: LLM-specific metrics like queue depth and tokens/sec\n",
    "- **Multi-dimensional scaling**: CPU, memory, GPU utilization, and custom metrics\n",
    "- **Proactive alerting**: Detect issues before they impact users\n",
    "- **Performance tracking**: Comprehensive dashboards for operational visibility\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Advanced Production Patterns\n",
    "\n",
    "### **Multi-Model Serving**\n",
    "```yaml\n",
    "# Deploy different model sizes for different use cases\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: llm-small-fast\n",
    "spec:\n",
    "  replicas: 10  # More replicas for fast responses\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: llm-large-accurate\n",
    "spec:\n",
    "  replicas: 2   # Fewer replicas for accuracy-focused requests\n",
    "```\n",
    "\n",
    "### **Canary Deployments**\n",
    "```yaml\n",
    "# Gradual rollout of new model versions\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: Rollout\n",
    "spec:\n",
    "  strategy:\n",
    "    canary:\n",
    "      steps:\n",
    "      - setWeight: 10    # 10% traffic to new version\n",
    "      - pause: {duration: 10m}\n",
    "      - setWeight: 50    # 50% traffic\n",
    "      - pause: {duration: 10m}\n",
    "      - setWeight: 100   # Full rollout\n",
    "```\n",
    "\n",
    "### **Cost Optimization**\n",
    "```yaml\n",
    "# Use spot instances with proper handling\n",
    "apiVersion: v1\n",
    "kind: Node\n",
    "metadata:\n",
    "  labels:\n",
    "    node.kubernetes.io/instance-type: spot\n",
    "spec:\n",
    "  taints:\n",
    "  - key: spot-instance\n",
    "    value: \"true\"\n",
    "    effect: NoSchedule\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ Production Checklist\n",
    "\n",
    "### **Before Deployment**\n",
    "- âœ… GPU nodes labeled and available\n",
    "- âœ… NVIDIA GPU Operator installed\n",
    "- âœ… Prometheus Operator configured\n",
    "- âœ… Storage classes defined\n",
    "- âœ… Network policies tested\n",
    "\n",
    "### **During Deployment**\n",
    "- âœ… Resource quotas applied\n",
    "- âœ… Health checks responding\n",
    "- âœ… Metrics being collected\n",
    "- âœ… Alerts configured\n",
    "- âœ… Auto-scaling working\n",
    "\n",
    "### **After Deployment**\n",
    "- âœ… Load testing completed\n",
    "- âœ… Failover scenarios tested\n",
    "- âœ… Monitoring dashboards validated\n",
    "- âœ… Runbooks documented\n",
    "- âœ… On-call procedures established\n",
    "\n",
    "---\n",
    "\n",
    "**Next: Chapter 9 - Cost Optimization & Operations** ðŸ’°\n",
    "\n",
    "*In the final chapter, we'll explore advanced cost optimization techniques, FinOps practices, and SRE methodologies for running LLMs efficiently at scale.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}