{"cells":[{"cell_type":"markdown","metadata":{"id":"8AL9XVwmnz3t"},"source":["# Marco v3 Colab Inference API\n","\n","This notebook creates a FastAPI inference service for the Marco v3 Italian Teacher model.\n","\n","**What it does:**\n","- Loads your fine-tuned Marco v3 LoRA model with vLLM (4.4x faster inference)\n","- Exposes a FastAPI endpoint for generating Italian exercises\n","- Creates a public tunnel via ngrok so your local API can access it\n","\n","**Usage:**\n","1. Run all cells in order\n","2. Copy the ngrok URL from the output\n","3. Export it locally: `export INFERENCE_API_URL=\"https://your-url.ngrok.io\"`\n","4. Start your local API: `./run_api.sh`\n","5. Your local API will now use Colab GPU for homework generation!"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WOAf9Lmsnz3x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759503568742,"user_tz":-60,"elapsed":58554,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"c9204a84-fd1e-40b7-a0c8-504e25cce494"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.9/387.9 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.9/284.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.1/951.1 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m✅ Dependencies installed\n"]}],"source":["# Cell 1: Install dependencies\n","!pip install fastapi uvicorn pyngrok vllm nest-asyncio -q\n","print(\"✅ Dependencies installed\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8dTrxCtJnz3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759503587820,"user_tz":-60,"elapsed":19073,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"5c5e6dd6-4d44-4bed-d7d2-3c82f1c6449d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","✅ Google Drive mounted\n"]}],"source":["# Cell 2: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"✅ Google Drive mounted\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hOuMynWRnz3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759503588820,"user_tz":-60,"elapsed":998,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"05ab6891-d9a9-42f4-c2b8-1f4bfddcaa6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Model found at: /content/drive/MyDrive/Colab Notebooks/italian_teacher/models/minerva_marco_v3_merged\n","✅ Project root: /content/drive/MyDrive/Colab Notebooks/italian_teacher\n"]}],"source":["# Cell 3: Setup paths and verify model exists\n","import os\n","import sys\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/italian_teacher\"\n","MODEL_PATH = os.path.join(PROJECT_ROOT, \"models/minerva_marco_v3_merged\")\n","\n","# Add project to Python path for imports\n","sys.path.insert(0, PROJECT_ROOT)\n","\n","# Verify model exists\n","if not os.path.exists(MODEL_PATH):\n","    print(f\"❌ Model not found at: {MODEL_PATH}\")\n","    print(\"Please update MODEL_PATH to point to your merged LoRA model\")\n","else:\n","    print(f\"✅ Model found at: {MODEL_PATH}\")\n","    print(f\"✅ Project root: {PROJECT_ROOT}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eOti70qbnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759503927540,"user_tz":-60,"elapsed":338718,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"ee4b2a98-a6dc-4a50-b552-f558d7439b16"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO 10-03 15:00:07 [__init__.py:216] Automatically detected platform cuda.\n","⏳ Loading Marco v3 model with vLLM...\n","INFO 10-03 15:00:08 [utils.py:328] non-default args: {'trust_remote_code': True, 'dtype': 'half', 'max_model_len': 2048, 'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/content/drive/MyDrive/Colab Notebooks/italian_teacher/models/minerva_marco_v3_merged'}\n"]},{"output_type":"stream","name":"stderr","text":["The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"]},{"output_type":"stream","name":"stdout","text":["INFO 10-03 15:00:26 [__init__.py:742] Resolved architecture: MistralForCausalLM\n"]},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"stream","name":"stdout","text":["INFO 10-03 15:00:26 [__init__.py:1815] Using max model len 2048\n","INFO 10-03 15:00:29 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n","WARNING 10-03 15:00:32 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n","INFO 10-03 15:05:27 [llm.py:295] Supported_tasks: ['generate']\n","INFO 10-03 15:05:27 [__init__.py:36] No IOProcessor plugins requested by the model\n","✅ Marco v3 model loaded successfully!\n","🔥 GPU: NVIDIA L4\n","💾 GPU Memory: 0.00GB\n"]}],"source":["# Cell 4: Load model with vLLM (this takes ~30 seconds)\n","import torch\n","from vllm import LLM\n","\n","print(\"⏳ Loading Marco v3 model with vLLM...\")\n","\n","llm = LLM(\n","    model=MODEL_PATH,\n","    tensor_parallel_size=1,\n","    dtype=\"half\",\n","    max_model_len=2048,\n","    gpu_memory_utilization=0.85,\n","    trust_remote_code=True\n",")\n","\n","print(\"✅ Marco v3 model loaded successfully!\")\n","print(f\"🔥 GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n","print(f\"💾 GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6cODy2JMnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759503932118,"user_tz":-60,"elapsed":4571,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"b516c6f8-aff9-43c8-f64c-6e10b7e9a979"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ FastAPI application created (port 8001)\n","📋 Version: 1.0.5\n","🚀 Ready to start server!\n"]}],"source":["# Cell 5: Create FastAPI application\n","import nest_asyncio\n","from src.api.inference import create_inference_app\n","\n","# Allow nested event loops (required for Colab)\n","nest_asyncio.apply()\n","\n","# Port for Colab API (8001 to avoid conflict with local API on 8000)\n","COLAB_PORT = 8001\n","\n","# Create the FastAPI app\n","app = create_inference_app(llm, port=COLAB_PORT)\n","\n","print(f\"✅ FastAPI application created (port {COLAB_PORT})\")\n","print(f\"📋 Version: 1.0.5\")\n","print(\"🚀 Ready to start server!\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wwFbpLmAnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759503933970,"user_tz":-60,"elapsed":1837,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"faeab1b6-a892-4311-a057-4a08d558e697"},"outputs":[{"output_type":"stream","name":"stdout","text":["🌐 ngrok tunnel created!\n","\n","📍 Public URL: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\n","\n","🔗 API Endpoints:\n","   Health: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev/health\n","   Generate: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev/generate\n","\n","✅ Copy the public URL above for use in your local environment\n"]}],"source":["# Cell 6: Setup ngrok tunnel\n","from pyngrok import ngrok\n","\n","# Set your ngrok auth token (get free token at https://ngrok.com)\n","NGROK_AUTH_TOKEN = \"33VKJ1gR2EjYu8WvlRmSqOiUiJk_2qCL7X8Kp4vHTUAcu4xvh\"\n","\n","# Authenticate ngrok\n","ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","# Create tunnel\n","tunnel = ngrok.connect(COLAB_PORT)\n","public_url = str(tunnel.public_url)\n","\n","print(\"🌐 ngrok tunnel created!\")\n","print(f\"\\n📍 Public URL: {public_url}\")\n","print(f\"\\n🔗 API Endpoints:\")\n","print(f\"   Health: {public_url}/health\")\n","print(f\"   Generate: {public_url}/generate\")\n","print(f\"\\n✅ Copy the public URL above for use in your local environment\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"95GZmM89nz31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759503937026,"user_tz":-60,"elapsed":3054,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"ad2ba5a6-b2fe-4fd5-ff22-ae922bbcefda"},"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 Starting FastAPI server on port 8001...\n","⏳ Waiting for server to start...\n","\n","✅ SERVER IS RUNNING!\n","📡 Listening on http://0.0.0.0:8001\n","\n","🧪 Health check response:\n","{\n","  \"status\": \"healthy\",\n","  \"gpu_available\": true,\n","  \"gpu_memory_allocated_gb\": 0.0,\n","  \"model_loaded\": true,\n","  \"port\": 8001\n","}\n","\n","======================================================================\n","🌐 YOUR NGROK PUBLIC URL:\n","======================================================================\n","\n","https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\n","\n","======================================================================\n","\n","📋 COPY AND RUN ON YOUR MAC:\n","\n","export INFERENCE_API_URL=\"https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\"\n","./run_api.sh\n","\n","======================================================================\n","\n","⚡ Server is running! Keep this notebook open!\n","🛑 To stop: Runtime → Interrupt execution\n","======================================================================\n"]}],"source":["# Cell 7: Start FastAPI server\n","import uvicorn\n","from threading import Thread\n","import time\n","import requests\n","import json\n","\n","print(f\"🚀 Starting FastAPI server on port {COLAB_PORT}...\")\n","\n","# Create uvicorn config\n","config = uvicorn.Config(\n","    app=app,\n","    host=\"0.0.0.0\",\n","    port=COLAB_PORT,\n","    log_level=\"error\"\n",")\n","\n","# Create server\n","server = uvicorn.Server(config)\n","\n","# Start in background thread\n","def run_server():\n","    import asyncio\n","    asyncio.run(server.serve())\n","\n","server_thread = Thread(target=run_server, daemon=True)\n","server_thread.start()\n","\n","# Wait for server to be ready\n","print(\"⏳ Waiting for server to start...\")\n","time.sleep(3)\n","\n","# Test if it's working\n","try:\n","    response = requests.get(f\"http://localhost:{COLAB_PORT}/health\", timeout=2)\n","\n","    if response.status_code == 200:\n","        print(\"\\n✅ SERVER IS RUNNING!\")\n","        print(f\"📡 Listening on http://0.0.0.0:{COLAB_PORT}\\n\")\n","\n","        print(\"🧪 Health check response:\")\n","        print(json.dumps(response.json(), indent=2))\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"🌐 YOUR NGROK PUBLIC URL:\")\n","        print(\"=\"*70)\n","        print(f\"\\n{public_url}\\n\")\n","        print(\"=\"*70)\n","\n","        print(\"\\n📋 COPY AND RUN ON YOUR MAC:\\n\")\n","        print(f'export INFERENCE_API_URL=\"{public_url}\"')\n","        print(\"./run_api.sh\")\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"\\n⚡ Server is running! Keep this notebook open!\")\n","        print(\"🛑 To stop: Runtime → Interrupt execution\")\n","        print(\"=\"*70)\n","\n","except Exception as e:\n","    print(f\"\\n❌ Server failed to start: {e}\")\n","    print(\"\\n🔄 Try this:\")\n","    print(\"   1. Runtime → Restart runtime\")\n","    print(\"   2. Re-run all cells\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ublYMovYnz31"},"outputs":[],"source":["# Cell 8 (OPTIONAL): Test the API locally in Colab\n","import requests\n","import json\n","\n","print(\"🧪 Testing API endpoints...\\n\")\n","\n","# Test generation endpoint\n","test_request = {\n","    \"cefr_level\": \"A2\",\n","    \"grammar_focus\": \"present_tense\",\n","    \"topic\": \"daily routines\",\n","    \"quantity\": 3,\n","    \"exercise_types\": [\"fill_in_blank\", \"translation\", \"multiple_choice\"],\n","    \"temperature\": 0.7,\n","    \"max_tokens\": 2500\n","}\n","\n","response = requests.post(\n","    f\"http://localhost:{COLAB_PORT}/generate\",\n","    json=test_request,\n","    timeout=120\n",")\n","\n","if response.status_code == 200:\n","    result = response.json()\n","    print(\"✅ Generation successful!\")\n","    print(f\"\\nGenerated {len(result['exercises'])} exercises in {result['inference_time']:.2f}s\")\n","    print(f\"Tokens generated: {result['generated_tokens']}\")\n","    print(f\"Parsing strategy: {result['parsing_strategy']}\\n\")\n","\n","    for i, ex in enumerate(result['exercises'], 1):\n","        print(f\"Exercise {i}:\")\n","        print(json.dumps(ex, indent=2, ensure_ascii=False))\n","        print()\n","else:\n","    print(f\"❌ Generation failed: {response.status_code}\")\n","    print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"umFuPWMMnz32"},"source":["## 🎯 Next Steps\n","\n","1. **On your Mac**, export the ngrok URL:\n","   ```bash\n","   export INFERENCE_API_URL=\"https://your-ngrok-url.ngrok.io\"\n","   ```\n","\n","2. **Start your local API**:\n","   ```bash\n","   ./run_api.sh\n","   ```\n","\n","3. **Test the integration**:\n","   ```bash\n","   python test_exercise_quality.py\n","   ```\n","\n","4. **Create assignments** via your local API - they'll use Colab GPU for generation!\n","\n","## ⚠️ Important Notes\n","\n","- **Keep this notebook running** while using the API\n","- Free ngrok tunnels expire after ~2 hours\n","- Colab disconnects after ~90 min of inactivity (free tier)\n","- For production, consider Colab Pro ($10/month) or paid ngrok"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.0"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}