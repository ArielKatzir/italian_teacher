{"cells":[{"cell_type":"markdown","metadata":{"id":"8AL9XVwmnz3t"},"source":["# Italian Exercise Generator - Colab Inference API\n","\n","This notebook creates a FastAPI inference service for the Italian Exercise Generator model.\n","\n","**What it does:**\n","- Loads your fine-tuned `italian_exercise_generator_lora` model with vLLM (4.4x faster inference)\n","- Exposes a FastAPI endpoint for generating Italian exercises\n","- Creates a public tunnel via ngrok so your local API can access it\n","\n","**Usage:**\n","1. Run all cells in order\n","2. Copy the ngrok URL from the output\n","3. Export it locally: `export INFERENCE_API_URL=\"https://your-url.ngrok.io\"`\n","4. Start your local API: `./run_api.sh`\n","5. Your local API will now use Colab GPU for homework generation!"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WOAf9Lmsnz3x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760123583678,"user_tz":-60,"elapsed":12774,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"4b484755-9066-4eaa-bd04-bdd03a78f437"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting it-core-news-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('it_core_news_sm')\n","\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","‚úÖ Dependencies installed\n","‚úÖ Italian NLP model installed\n"]}],"source":["# Cell 1: Install dependencies\n","!pip install fastapi uvicorn pyngrok vllm nest-asyncio spacy -q\n","!python -m spacy download it_core_news_sm\n","print(\"‚úÖ Dependencies installed\")\n","print(\"‚úÖ Italian NLP model installed\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8dTrxCtJnz3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760123591772,"user_tz":-60,"elapsed":8091,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"16221df2-d269-4486-8e94-a3e6440fcec8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","‚úÖ Google Drive mounted\n"]}],"source":["# Cell 2: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"‚úÖ Google Drive mounted\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hOuMynWRnz3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760123593023,"user_tz":-60,"elapsed":1249,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"5594119b-2267-4e61-9349-3f9fa2b945ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ LoRA adapter found at: /content/drive/MyDrive/Colab Notebooks/italian_teacher/models/italian_exercise_generator_v4\n","‚úÖ Base model: swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\n","‚úÖ Project root: /content/drive/MyDrive/Colab Notebooks/italian_teacher\n"]}],"source":["# Cell 3: Setup paths and verify model exists\n","import os\n","import sys\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/italian_teacher\"\n","LORA_PATH = os.path.join(PROJECT_ROOT, \"models/italian_exercise_generator_v4\")\n","BASE_MODEL = \"swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\"\n","\n","# Add project to Python path for imports\n","sys.path.insert(0, PROJECT_ROOT)\n","\n","# Verify LoRA adapter exists\n","if not os.path.exists(LORA_PATH):\n","    print(f\"‚ùå LoRA adapter not found at: {LORA_PATH}\")\n","    print(\"Please update LORA_PATH to point to your italian_exercise_generator_lora model\")\n","else:\n","    print(f\"‚úÖ LoRA adapter found at: {LORA_PATH}\")\n","    print(f\"‚úÖ Base model: {BASE_MODEL}\")\n","    print(f\"‚úÖ Project root: {PROJECT_ROOT}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eOti70qbnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760123595021,"user_tz":-60,"elapsed":1996,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"adfa8349-520c-4b08-d773-897e844ea0a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Merged model already exists at: /content/drive/MyDrive/Colab Notebooks/italian_teacher/models/italian_exercise_generator_v4_merged\n","Skipping merge step...\n","\n","‚úÖ Ready to load merged model for vLLM: /content/drive/MyDrive/Colab Notebooks/italian_teacher/models/italian_exercise_generator_v4_merged\n"]}],"source":["# Cell 4: Merge LoRA adapter with base model (one-time, takes ~2-3 minutes)\n","import torch\n","import os\n","\n","MERGED_MODEL_PATH = os.path.join(PROJECT_ROOT, \"models/italian_exercise_generator_v4_merged\")\n","\n","# Check if already merged\n","if os.path.exists(MERGED_MODEL_PATH):\n","    print(f\"‚úÖ Merged model already exists at: {MERGED_MODEL_PATH}\")\n","    print(\"Skipping merge step...\")\n","    MODEL_PATH = MERGED_MODEL_PATH\n","else:\n","    print(\"‚è≥ Merging LoRA adapter with base model...\")\n","    print(f\"   Base model: {BASE_MODEL}\")\n","    print(f\"   LoRA adapter: {LORA_PATH}\")\n","    print(\"\")\n","\n","    from transformers import AutoModelForCausalLM, AutoTokenizer\n","    from peft import PeftModel\n","\n","    # Load base model\n","    print(\"1. Loading base model from HuggingFace (~8GB)...\")\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        BASE_MODEL,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\"\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","\n","    # Load LoRA adapter\n","    print(\"2. Loading LoRA adapter from Google Drive...\")\n","    model = PeftModel.from_pretrained(base_model, LORA_PATH)\n","\n","    # Merge and unload\n","    print(\"3. Merging LoRA weights into base model...\")\n","    model = model.merge_and_unload()\n","\n","    # Save merged model\n","    print(f\"4. Saving merged model to {MERGED_MODEL_PATH}...\")\n","    model.save_pretrained(MERGED_MODEL_PATH)\n","    tokenizer.save_pretrained(MERGED_MODEL_PATH)\n","\n","    MODEL_PATH = MERGED_MODEL_PATH\n","    print(f\"‚úÖ Model merged and saved successfully!\")\n","\n","    # Free memory\n","    del model\n","    del base_model\n","    torch.cuda.empty_cache()\n","\n","print(f\"\\n‚úÖ Ready to load merged model for vLLM: {MODEL_PATH}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6cODy2JMnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760123906630,"user_tz":-60,"elapsed":311607,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"8cc057c4-8875-4343-ec88-2694b8424204"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO 10-10 19:13:21 [__init__.py:216] Automatically detected platform cuda.\n","‚è≥ Loading merged model with vLLM for fast inference...\n","INFO 10-10 19:13:30 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'half', 'max_model_len': 2048, 'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/content/drive/MyDrive/Colab Notebooks/italian_teacher/models/italian_exercise_generator_v4_merged'}\n"]},{"output_type":"stream","name":"stderr","text":["The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"]},{"output_type":"stream","name":"stdout","text":["INFO 10-10 19:13:47 [model.py:547] Resolved architecture: LlamaForCausalLM\n"]},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"stream","name":"stdout","text":["INFO 10-10 19:13:47 [model.py:1510] Using max model len 2048\n","INFO 10-10 19:13:50 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n","WARNING 10-10 19:13:53 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n","INFO 10-10 19:18:26 [llm.py:306] Supported_tasks: ['generate']\n","‚úÖ Italian Exercise Generator model loaded with vLLM!\n","üî• GPU: NVIDIA L4\n","üíæ GPU Memory: 0.00GB\n"]}],"source":["# Cell 5: Load merged model with vLLM (~30 seconds)\n","from vllm import LLM\n","\n","print(\"‚è≥ Loading merged model with vLLM for fast inference...\")\n","\n","llm = LLM(\n","    model=MODEL_PATH,\n","    tensor_parallel_size=1,\n","    dtype=\"half\",\n","    max_model_len=2048,\n","    gpu_memory_utilization=0.85,\n","    trust_remote_code=True\n",")\n","\n","print(\"‚úÖ Italian Exercise Generator model loaded with vLLM!\")\n","print(f\"üî• GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n","print(f\"üíæ GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wwFbpLmAnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760123911134,"user_tz":-60,"elapsed":4500,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"55e0c7d5-9d33-40ef-fd2b-785c83230887"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ FastAPI application created (port 8001)\n","üìã Version: 2.0.0\n","üöÄ Ready to start server!\n"]}],"source":["# Cell 6: Create FastAPI application\n","import nest_asyncio\n","from src.api.inference import create_inference_app\n","\n","# Allow nested event loops (required for Colab)\n","nest_asyncio.apply()\n","\n","# Port for Colab API (8001 to avoid conflict with local API on 8000)\n","COLAB_PORT = 8001\n","\n","# Create the FastAPI app\n","app = create_inference_app(llm, port=COLAB_PORT)\n","\n","print(f\"‚úÖ FastAPI application created (port {COLAB_PORT})\")\n","print(f\"üìã Version: 2.0.0\")\n","print(\"üöÄ Ready to start server!\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"95GZmM89nz31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760123912873,"user_tz":-60,"elapsed":1738,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"5eca5e66-7521-4139-9ccc-cc25c36f6973"},"outputs":[{"output_type":"stream","name":"stdout","text":["üåê ngrok tunnel created!\n","\n","üìç Public URL: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\n","\n","üîó API Endpoints:\n","   Health: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev/health\n","   Generate: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev/generate\n","\n","‚úÖ Copy the public URL above for use in your local environment\n"]}],"source":["# Cell 7: Setup ngrok tunnel\n","from pyngrok import ngrok\n","\n","# Set your ngrok auth token (get free token at https://ngrok.com)\n","NGROK_AUTH_TOKEN = \"33VKJ1gR2EjYu8WvlRmSqOiUiJk_2qCL7X8Kp4vHTUAcu4xvh\"\n","\n","# Authenticate ngrok\n","ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","# Create tunnel\n","tunnel = ngrok.connect(COLAB_PORT)\n","public_url = str(tunnel.public_url)\n","\n","print(\"üåê ngrok tunnel created!\")\n","print(f\"\\nüìç Public URL: {public_url}\")\n","print(f\"\\nüîó API Endpoints:\")\n","print(f\"   Health: {public_url}/health\")\n","print(f\"   Generate: {public_url}/generate\")\n","print(f\"\\n‚úÖ Copy the public URL above for use in your local environment\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ublYMovYnz31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760123915948,"user_tz":-60,"elapsed":3073,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"10e7d638-3e8e-4518-f487-c8580ccb66de"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting FastAPI server on port 8001...\n","‚è≥ Waiting for server to start...\n","\n","‚úÖ SERVER IS RUNNING!\n","üì° Listening on http://0.0.0.0:8001\n","\n","üß™ Health check response:\n","{\n","  \"status\": \"healthy\",\n","  \"gpu_available\": true,\n","  \"gpu_memory_allocated_gb\": 0.0,\n","  \"model_loaded\": true,\n","  \"port\": 8001\n","}\n","\n","======================================================================\n","üåê YOUR NGROK PUBLIC URL:\n","======================================================================\n","\n","https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\n","\n","======================================================================\n","\n","üìã COPY AND RUN ON YOUR MAC:\n","\n","export INFERENCE_API_URL=\"https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\"\n","./run_api.sh\n","\n","======================================================================\n","\n","‚ö° Server is running! Keep this notebook open!\n","üõë To stop: Runtime ‚Üí Interrupt execution\n","======================================================================\n"]}],"source":["# Cell 8: Start FastAPI server\n","import uvicorn\n","from threading import Thread\n","import time\n","import requests\n","import json\n","\n","print(f\"üöÄ Starting FastAPI server on port {COLAB_PORT}...\")\n","\n","# Create uvicorn config\n","config = uvicorn.Config(\n","    app=app,\n","    host=\"0.0.0.0\",\n","    port=COLAB_PORT,\n","    log_level=\"error\"\n",")\n","\n","# Create server\n","server = uvicorn.Server(config)\n","\n","# Start in background thread\n","def run_server():\n","    import asyncio\n","    asyncio.run(server.serve())\n","\n","server_thread = Thread(target=run_server, daemon=True)\n","server_thread.start()\n","\n","# Wait for server to be ready\n","print(\"‚è≥ Waiting for server to start...\")\n","time.sleep(3)\n","\n","# Test if it's working\n","try:\n","    response = requests.get(f\"http://localhost:{COLAB_PORT}/health\", timeout=2)\n","\n","    if response.status_code == 200:\n","        print(\"\\n‚úÖ SERVER IS RUNNING!\")\n","        print(f\"üì° Listening on http://0.0.0.0:{COLAB_PORT}\\n\")\n","\n","        print(\"üß™ Health check response:\")\n","        print(json.dumps(response.json(), indent=2))\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"üåê YOUR NGROK PUBLIC URL:\")\n","        print(\"=\"*70)\n","        print(f\"\\n{public_url}\\n\")\n","        print(\"=\"*70)\n","\n","        print(\"\\nüìã COPY AND RUN ON YOUR MAC:\\n\")\n","        print(f'export INFERENCE_API_URL=\"{public_url}\"')\n","        print(\"./run_api.sh\")\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"\\n‚ö° Server is running! Keep this notebook open!\")\n","        print(\"üõë To stop: Runtime ‚Üí Interrupt execution\")\n","        print(\"=\"*70)\n","\n","except Exception as e:\n","    print(f\"\\n‚ùå Server failed to start: {e}\")\n","    print(\"\\nüîÑ Try this:\")\n","    print(\"   1. Runtime ‚Üí Restart runtime\")\n","    print(\"   2. Re-run all cells\")"]},{"cell_type":"code","source":[],"metadata":{"id":"7zMJhAtwrBMS","executionInfo":{"status":"ok","timestamp":1760123915953,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}}},"execution_count":8,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.0"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}