{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marco v3 Colab Inference API\n",
    "\n",
    "This notebook creates a FastAPI inference service for the Marco v3 Italian Teacher model.\n",
    "\n",
    "**What it does:**\n",
    "- Loads your fine-tuned Marco v3 LoRA model with vLLM (4.4x faster inference)\n",
    "- Exposes a FastAPI endpoint for generating Italian exercises\n",
    "- Creates a public tunnel via ngrok so your local API can access it\n",
    "\n",
    "**Usage:**\n",
    "1. Run all cells in order\n",
    "2. Copy the ngrok URL from the output\n",
    "3. Export it locally: `export INFERENCE_API_URL=\"https://your-url.ngrok.io\"`\n",
    "4. Start your local API: `./run_api.sh`\n",
    "5. Your local API will now use Colab GPU for homework generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install fastapi uvicorn pyngrok vllm nest-asyncio -q\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Setup paths and verify model exists\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/italian_teacher\"\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT, \"models/minerva_marco_v3_merged\")\n",
    "\n",
    "# Add project to Python path for imports\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Verify model exists\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"‚ùå Model not found at: {MODEL_PATH}\")\n",
    "    print(\"Please update MODEL_PATH to point to your merged LoRA model\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model found at: {MODEL_PATH}\")\n",
    "    print(f\"‚úÖ Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model with vLLM (this takes ~30 seconds)\n",
    "import torch\n",
    "from vllm import LLM\n",
    "\n",
    "print(\"‚è≥ Loading Marco v3 model with vLLM...\")\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"half\",\n",
    "    max_model_len=2048,\n",
    "    gpu_memory_utilization=0.85,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Marco v3 model loaded successfully!\")\n",
    "print(f\"üî• GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üíæ GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create FastAPI application\n",
    "import nest_asyncio\n",
    "from src.api.inference import create_inference_app\n",
    "\n",
    "# Allow nested event loops (required for Colab)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Port for Colab API (8001 to avoid conflict with local API on 8000)\n",
    "COLAB_PORT = 8001\n",
    "\n",
    "# Create the FastAPI app\n",
    "app = create_inference_app(llm, port=COLAB_PORT)\n",
    "\n",
    "print(f\"‚úÖ FastAPI application created (port {COLAB_PORT})\")\n",
    "print(f\"üìã Version: 1.0.5\")\n",
    "print(\"üöÄ Ready to start server!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Setup ngrok tunnel\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Set your ngrok auth token (get free token at https://ngrok.com)\n",
    "NGROK_AUTH_TOKEN = \"33VKJ1gR2EjYu8WvlRmSqOiUiJk_2qCL7X8Kp4vHTUAcu4xvh\"\n",
    "\n",
    "# Authenticate ngrok\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Create tunnel\n",
    "tunnel = ngrok.connect(COLAB_PORT)\n",
    "public_url = str(tunnel.public_url)\n",
    "\n",
    "print(\"üåê ngrok tunnel created!\")\n",
    "print(f\"\\nüìç Public URL: {public_url}\")\n",
    "print(f\"\\nüîó API Endpoints:\")\n",
    "print(f\"   Health: {public_url}/health\")\n",
    "print(f\"   Generate: {public_url}/generate\")\n",
    "print(f\"\\n‚úÖ Copy the public URL above for use in your local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Start FastAPI server\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "print(f\"üöÄ Starting FastAPI server on port {COLAB_PORT}...\")\n",
    "\n",
    "# Create uvicorn config\n",
    "config = uvicorn.Config(\n",
    "    app=app,\n",
    "    host=\"0.0.0.0\",\n",
    "    port=COLAB_PORT,\n",
    "    log_level=\"error\"\n",
    ")\n",
    "\n",
    "# Create server\n",
    "server = uvicorn.Server(config)\n",
    "\n",
    "# Start in background thread\n",
    "def run_server():\n",
    "    import asyncio\n",
    "    asyncio.run(server.serve())\n",
    "\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to be ready\n",
    "print(\"‚è≥ Waiting for server to start...\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Test if it's working\n",
    "try:\n",
    "    response = requests.get(f\"http://localhost:{COLAB_PORT}/health\", timeout=2)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"\\n‚úÖ SERVER IS RUNNING!\")\n",
    "        print(f\"üì° Listening on http://0.0.0.0:{COLAB_PORT}\\n\")\n",
    "\n",
    "        print(\"üß™ Health check response:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üåê YOUR NGROK PUBLIC URL:\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\n{public_url}\\n\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        print(\"\\nüìã COPY AND RUN ON YOUR MAC:\\n\")\n",
    "        print(f'export INFERENCE_API_URL=\"{public_url}\"')\n",
    "        print(\"./run_api.sh\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\\n‚ö° Server is running! Keep this notebook open!\")\n",
    "        print(\"üõë To stop: Runtime ‚Üí Interrupt execution\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Server failed to start: {e}\")\n",
    "    print(\"\\nüîÑ Try this:\")\n",
    "    print(\"   1. Runtime ‚Üí Restart runtime\")\n",
    "    print(\"   2. Re-run all cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 (OPTIONAL): Test the API locally in Colab\n",
    "import requests\n",
    "import json\n",
    "\n",
    "print(\"üß™ Testing API endpoints...\\n\")\n",
    "\n",
    "# Test generation endpoint\n",
    "test_request = {\n",
    "    \"cefr_level\": \"A2\",\n",
    "    \"grammar_focus\": \"present_tense\",\n",
    "    \"topic\": \"daily routines\",\n",
    "    \"quantity\": 3,\n",
    "    \"exercise_types\": [\"fill_in_blank\", \"translation\", \"multiple_choice\"],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 2500\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{COLAB_PORT}/generate\",\n",
    "    json=test_request,\n",
    "    timeout=120\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"‚úÖ Generation successful!\")\n",
    "    print(f\"\\nGenerated {len(result['exercises'])} exercises in {result['inference_time']:.2f}s\")\n",
    "    print(f\"Tokens generated: {result['generated_tokens']}\")\n",
    "    print(f\"Parsing strategy: {result['parsing_strategy']}\\n\")\n",
    "\n",
    "    for i, ex in enumerate(result['exercises'], 1):\n",
    "        print(f\"Exercise {i}:\")\n",
    "        print(json.dumps(ex, indent=2, ensure_ascii=False))\n",
    "        print()\n",
    "else:\n",
    "    print(f\"‚ùå Generation failed: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "1. **On your Mac**, export the ngrok URL:\n",
    "   ```bash\n",
    "   export INFERENCE_API_URL=\"https://your-ngrok-url.ngrok.io\"\n",
    "   ```\n",
    "\n",
    "2. **Start your local API**:\n",
    "   ```bash\n",
    "   ./run_api.sh\n",
    "   ```\n",
    "\n",
    "3. **Test the integration**:\n",
    "   ```bash\n",
    "   python test_exercise_quality.py\n",
    "   ```\n",
    "\n",
    "4. **Create assignments** via your local API - they'll use Colab GPU for generation!\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "- **Keep this notebook running** while using the API\n",
    "- Free ngrok tunnels expire after ~2 hours\n",
    "- Colab disconnects after ~90 min of inactivity (free tier)\n",
    "- For production, consider Colab Pro ($10/month) or paid ngrok"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
