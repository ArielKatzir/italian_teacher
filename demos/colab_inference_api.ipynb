{"cells":[{"cell_type":"markdown","metadata":{"id":"8AL9XVwmnz3t"},"source":["# Italian Exercise Generator - Colab Inference API\n","\n","This notebook creates a FastAPI inference service for the Italian Exercise Generator model.\n","\n","**What it does:**\n","- Loads your GRPO-trained `TeacherPet_italian_grpo` model with vLLM (4.4x faster inference)\n","- Exposes a FastAPI endpoint for generating Italian exercises\n","- Creates a public tunnel via ngrok so your local API can access it\n","\n","**Model: TeacherPet_italian_grpo**\n","- Training: GRPO (Group Relative Policy Optimization) - reinforcement learning\n","- Advantages: Better grammar accuracy, tense consistency, reduced hallucination\n","- No LoRA merging needed - this is a full fine-tuned model\n","\n","**Usage:**\n","1. Run all cells in order\n","2. Copy the ngrok URL from the output\n","3. Export it locally: `export INFERENCE_API_URL=\"https://your-url.ngrok.io\"`\n","4. Start your local API: `./run_api.sh`\n","5. Your local API will now use Colab GPU for homework generation!"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WOAf9Lmsnz3x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762171016361,"user_tz":0,"elapsed":71659,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"304db2a1-b806-4505-fa16-e0a786a66a38"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.2/438.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mCollecting it-core-news-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: it-core-news-sm\n","Successfully installed it-core-news-sm-3.8.0\n","\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('it_core_news_sm')\n","\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","âœ… Dependencies installed\n","âœ… Italian NLP model installed\n"]}],"source":["# Cell 1: Install dependencies\n","!pip install fastapi uvicorn pyngrok vllm nest-asyncio spacy -q\n","!python -m spacy download it_core_news_sm\n","print(\"âœ… Dependencies installed\")\n","print(\"âœ… Italian NLP model installed\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8dTrxCtJnz3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762171045090,"user_tz":0,"elapsed":28726,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"8d79f9cd-f907-47d5-ffcc-a74350691591"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","âœ… Google Drive mounted\n"]}],"source":["# Cell 2: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"âœ… Google Drive mounted\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hOuMynWRnz3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762171046439,"user_tz":0,"elapsed":1346,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"7ae24055-a4a1-4be0-978d-55f91266c612"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… GRPO model found at: /content/drive/MyDrive/Colab Notebooks/italian_teacher/models/TeacherPet_italian_grpo\n","âœ… Model: TeacherPet_italian_grpo\n","âœ… Training: GRPO (Group Relative Policy Optimization)\n","âœ… Project root: /content/drive/MyDrive/Colab Notebooks/italian_teacher\n"]}],"source":["# Cell 3: Setup paths and verify model exists\n","import os\n","import sys\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/italian_teacher\"\n","MODEL_PATH = os.path.join(PROJECT_ROOT, \"models/TeacherPet_italian_grpo\")\n","\n","# Add project to Python path for imports\n","sys.path.insert(0, PROJECT_ROOT)\n","\n","# Verify model exists\n","if not os.path.exists(MODEL_PATH):\n","    print(f\"âŒ Model not found at: {MODEL_PATH}\")\n","    print(\"Please update MODEL_PATH to point to your TeacherPet_italian_grpo model\")\n","    print(\"\\nExpected structure:\")\n","    print(\"  models/TeacherPet_italian_grpo/\")\n","    print(\"    â”œâ”€â”€ config.json\")\n","    print(\"    â”œâ”€â”€ tokenizer.json\")\n","    print(\"    â”œâ”€â”€ model-*.safetensors\")\n","    print(\"    â””â”€â”€ ...\")\n","else:\n","    print(f\"âœ… GRPO model found at: {MODEL_PATH}\")\n","    print(f\"âœ… Model: TeacherPet_italian_grpo\")\n","    print(f\"âœ… Training: GRPO (Group Relative Policy Optimization)\")\n","    print(f\"âœ… Project root: {PROJECT_ROOT}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eOti70qbnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762171049004,"user_tz":0,"elapsed":2563,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"419f6338-1c4f-4f56-ff5e-a5a8a6a0dcfa"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… TeacherPet_italian_grpo is a full GRPO-trained model\n","âœ… No LoRA merging required - model is ready to use with vLLM\n","\n","ğŸ“‹ Checking model files:\n","   âœ… config.json\n","   âœ… tokenizer.json\n","   âœ… tokenizer_config.json\n","   âœ… Found 4 model weight file(s)\n","\n","âœ… Ready to load model for vLLM: /content/drive/MyDrive/Colab Notebooks/italian_teacher/models/TeacherPet_italian_grpo\n"]}],"source":["# Cell 4: Verify model files (GRPO model is already a full model, no merging needed)\n","import torch\n","import os\n","\n","print(\"âœ… TeacherPet_italian_grpo is a full GRPO-trained model\")\n","print(\"âœ… No LoRA merging required - model is ready to use with vLLM\")\n","print(\"\")\n","\n","# Check for key model files\n","model_files = [\n","    \"config.json\",\n","    \"tokenizer.json\",\n","    \"tokenizer_config.json\"\n","]\n","\n","print(\"ğŸ“‹ Checking model files:\")\n","all_files_exist = True\n","for file in model_files:\n","    file_path = os.path.join(MODEL_PATH, file)\n","    exists = os.path.exists(file_path)\n","    status = \"âœ…\" if exists else \"âŒ\"\n","    print(f\"   {status} {file}\")\n","    if not exists:\n","        all_files_exist = False\n","\n","# Check for safetensors files\n","import glob\n","safetensors_files = glob.glob(os.path.join(MODEL_PATH, \"*.safetensors\"))\n","if safetensors_files:\n","    print(f\"   âœ… Found {len(safetensors_files)} model weight file(s)\")\n","else:\n","    print(f\"   âŒ No .safetensors files found\")\n","    all_files_exist = False\n","\n","if all_files_exist:\n","    print(f\"\\nâœ… Ready to load model for vLLM: {MODEL_PATH}\")\n","else:\n","    print(f\"\\nâŒ Some model files are missing. Please check your model directory.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6cODy2JMnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762171428662,"user_tz":0,"elapsed":379656,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"8da9391e-1ad2-456e-f585-43b56c5e9edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO 11-03 11:57:35 [__init__.py:216] Automatically detected platform cuda.\n","â³ Loading TeacherPet_italian_grpo with vLLM for fast inference...\n","ğŸ“Š Model: GRPO-trained (reinforcement learning optimized)\n","\n","INFO 11-03 11:57:47 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'half', 'max_model_len': 2048, 'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/content/drive/MyDrive/Colab Notebooks/italian_teacher/models/TeacherPet_italian_grpo'}\n"]},{"output_type":"stream","name":"stderr","text":["The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"]},{"output_type":"stream","name":"stdout","text":["INFO 11-03 11:58:07 [model.py:547] Resolved architecture: LlamaForCausalLM\n"]},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"stream","name":"stdout","text":["WARNING 11-03 11:58:07 [model.py:1733] Casting torch.bfloat16 to torch.float16.\n","INFO 11-03 11:58:07 [model.py:1510] Using max model len 2048\n","INFO 11-03 11:58:10 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n","WARNING 11-03 11:58:19 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n","INFO 11-03 12:03:48 [llm.py:306] Supported_tasks: ['generate']\n","\n","âœ… TeacherPet_italian_grpo model loaded with vLLM!\n","ğŸ”¥ GPU: NVIDIA L4\n","ğŸ’¾ GPU Memory: 0.00GB\n","ğŸ¯ Training: GRPO (Group Relative Policy Optimization)\n","âœ¨ Benefits: Better grammar, tense consistency, reduced hallucination\n"]}],"source":["# Cell 5: Load GRPO model with vLLM (~30 seconds)\n","from vllm import LLM\n","\n","print(\"â³ Loading TeacherPet_italian_grpo with vLLM for fast inference...\")\n","print(\"ğŸ“Š Model: GRPO-trained (reinforcement learning optimized)\")\n","print(\"\")\n","\n","llm = LLM(\n","    model=MODEL_PATH,\n","    tensor_parallel_size=1,\n","    dtype=\"half\",\n","    max_model_len=2048,\n","    gpu_memory_utilization=0.85,\n","    trust_remote_code=True\n",")\n","\n","print(\"\\nâœ… TeacherPet_italian_grpo model loaded with vLLM!\")\n","print(f\"ğŸ”¥ GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n","print(f\"ğŸ’¾ GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")\n","print(f\"ğŸ¯ Training: GRPO (Group Relative Policy Optimization)\")\n","print(f\"âœ¨ Benefits: Better grammar, tense consistency, reduced hallucination\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wwFbpLmAnz30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762171437795,"user_tz":0,"elapsed":9128,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"02f908b7-d5e1-4693-ce55-90347775839e"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… FastAPI application created (port 8001)\n","ğŸ“‹ Version: 3.0.0 (GRPO)\n","ğŸ¯ Model: TeacherPet_italian_grpo\n","ğŸš€ Ready to start server!\n"]}],"source":["# Cell 6: Create FastAPI application\n","import nest_asyncio\n","from src.api.inference import create_inference_app\n","\n","# Allow nested event loops (required for Colab)\n","nest_asyncio.apply()\n","\n","# Port for Colab API (8001 to avoid conflict with local API on 8000)\n","COLAB_PORT = 8001\n","\n","# Create the FastAPI app\n","app = create_inference_app(llm, port=COLAB_PORT)\n","\n","print(f\"âœ… FastAPI application created (port {COLAB_PORT})\")\n","print(f\"ğŸ“‹ Version: 3.0.0 (GRPO)\")\n","print(f\"ğŸ¯ Model: TeacherPet_italian_grpo\")\n","print(\"ğŸš€ Ready to start server!\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"95GZmM89nz31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762171442733,"user_tz":0,"elapsed":4934,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"fc5c7727-3645-4b83-f7b8-12e9dacb4805"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸŒ ngrok tunnel created!\n","\n","ğŸ“ Public URL: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\n","\n","ğŸ”— API Endpoints:\n","   Health: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev/health\n","   Generate: https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev/generate\n","\n","âœ… Copy the public URL above for use in your local environment\n"]}],"source":["# Cell 7: Setup ngrok tunnel\n","from pyngrok import ngrok\n","\n","# Set your ngrok auth token (get free token at https://ngrok.com)\n","NGROK_AUTH_TOKEN = \"33VKJ1gR2EjYu8WvlRmSqOiUiJk_2qCL7X8Kp4vHTUAcu4xvh\"\n","\n","# Authenticate ngrok\n","ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","# Create tunnel\n","tunnel = ngrok.connect(COLAB_PORT)\n","public_url = str(tunnel.public_url)\n","\n","print(\"ğŸŒ ngrok tunnel created!\")\n","print(f\"\\nğŸ“ Public URL: {public_url}\")\n","print(f\"\\nğŸ”— API Endpoints:\")\n","print(f\"   Health: {public_url}/health\")\n","print(f\"   Generate: {public_url}/generate\")\n","print(f\"\\nâœ… Copy the public URL above for use in your local environment\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ublYMovYnz31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762171445783,"user_tz":0,"elapsed":3048,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}},"outputId":"30e480d2-b216-48f6-86bb-06bc3ef8fc5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Starting FastAPI server on port 8001...\n","â³ Waiting for server to start...\n","\n","âœ… SERVER IS RUNNING!\n","ğŸ“¡ Listening on http://0.0.0.0:8001\n","\n","ğŸ§ª Health check response:\n","{\n","  \"status\": \"healthy\",\n","  \"gpu_available\": true,\n","  \"gpu_memory_allocated_gb\": 0.0,\n","  \"model_loaded\": true,\n","  \"port\": 8001\n","}\n","\n","======================================================================\n","ğŸŒ YOUR NGROK PUBLIC URL:\n","======================================================================\n","\n","https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\n","\n","======================================================================\n","\n","ğŸ“‹ COPY AND RUN ON YOUR MAC:\n","\n","export INFERENCE_API_URL=\"https://orthoscopic-nonengrossingly-lashon.ngrok-free.dev\"\n","./run_api.sh\n","\n","======================================================================\n","\n","âš¡ Server is running! Keep this notebook open!\n","ğŸ›‘ To stop: Runtime â†’ Interrupt execution\n","======================================================================\n"]}],"source":["# Cell 8: Start FastAPI server\n","import uvicorn\n","from threading import Thread\n","import time\n","import requests\n","import json\n","\n","print(f\"ğŸš€ Starting FastAPI server on port {COLAB_PORT}...\")\n","\n","# Create uvicorn config\n","config = uvicorn.Config(\n","    app=app,\n","    host=\"0.0.0.0\",\n","    port=COLAB_PORT,\n","    log_level=\"error\"\n",")\n","\n","# Create server\n","server = uvicorn.Server(config)\n","\n","# Start in background thread\n","def run_server():\n","    import asyncio\n","    asyncio.run(server.serve())\n","\n","server_thread = Thread(target=run_server, daemon=True)\n","server_thread.start()\n","\n","# Wait for server to be ready\n","print(\"â³ Waiting for server to start...\")\n","time.sleep(3)\n","\n","# Test if it's working\n","try:\n","    response = requests.get(f\"http://localhost:{COLAB_PORT}/health\", timeout=2)\n","\n","    if response.status_code == 200:\n","        print(\"\\nâœ… SERVER IS RUNNING!\")\n","        print(f\"ğŸ“¡ Listening on http://0.0.0.0:{COLAB_PORT}\\n\")\n","\n","        print(\"ğŸ§ª Health check response:\")\n","        print(json.dumps(response.json(), indent=2))\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"ğŸŒ YOUR NGROK PUBLIC URL:\")\n","        print(\"=\"*70)\n","        print(f\"\\n{public_url}\\n\")\n","        print(\"=\"*70)\n","\n","        print(\"\\nğŸ“‹ COPY AND RUN ON YOUR MAC:\\n\")\n","        print(f'export INFERENCE_API_URL=\"{public_url}\"')\n","        print(\"./run_api.sh\")\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"\\nâš¡ Server is running! Keep this notebook open!\")\n","        print(\"ğŸ›‘ To stop: Runtime â†’ Interrupt execution\")\n","        print(\"=\"*70)\n","\n","except Exception as e:\n","    print(f\"\\nâŒ Server failed to start: {e}\")\n","    print(\"\\nğŸ”„ Try this:\")\n","    print(\"   1. Runtime â†’ Restart runtime\")\n","    print(\"   2. Re-run all cells\")"]},{"cell_type":"code","source":[],"metadata":{"id":"7zMJhAtwrBMS","executionInfo":{"status":"ok","timestamp":1762171445787,"user_tz":0,"elapsed":2,"user":{"displayName":"Ariel Katzir","userId":"13010007500212358071"}}},"execution_count":8,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.0"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}