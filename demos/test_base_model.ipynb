{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Test - LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\n",
    "\n",
    "**Purpose**: Test if the base model (WITHOUT LoRA adapter) makes the same grammar mistakes.\n",
    "\n",
    "**Hypothesis**: Base model knows Italian correctly, but LoRA alpha=24 is too strong and causes catastrophic forgetting.\n",
    "\n",
    "**Test Cases**:\n",
    "1. Gender agreement: \"ragno\" (spider) - should use masculine \"il/un\"\n",
    "2. Gender agreement: \"lombrico\" (worm) - should use masculine \"il/un\"\n",
    "3. Topic adherence: Generate exercises about spiders without topic drift\n",
    "4. Semantic coherence: Avoid nonsensical scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install transformers accelerate torch -q\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load base model (NO LoRA)\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "BASE_MODEL = \"swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\"\n",
    "\n",
    "print(\"Loading base model (this may take 2-3 minutes)...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True  # Use 4-bit quantization to fit in Colab GPU\n",
    ")\n",
    "\n",
    "print(f\"✅ Base model loaded: {BASE_MODEL}\")\n",
    "print(f\"   Device: {model.device}\")\n",
    "print(f\"   Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define generation function\n",
    "def generate_exercises(cefr_level, grammar_focus, topic, quantity, exercise_types):\n",
    "    \"\"\"\n",
    "    Generate exercises using base model.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an expert Italian language teacher. Generate high-quality Italian language exercises \"\n",
    "        \"based on the assignment specification. Output ONLY a JSON array of exercises.\\n\\n\"\n",
    "        \"Each exercise must have:\\n\"\n",
    "        \"- type: exercise type\\n\"\n",
    "        \"- question: the exercise question in Italian\\n\"\n",
    "        \"- answer: the correct answer\\n\"\n",
    "        \"- explanation: explanation in Italian\\n\"\n",
    "        \"- options: array of 4 options (for multiple_choice only)\\n\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Generate {quantity} Italian language exercises:\\n\"\n",
    "        f\"CEFR Level: {cefr_level}\\n\"\n",
    "        f\"Grammar Focus: {grammar_focus}\\n\"\n",
    "        f\"Topic: {topic}\\n\"\n",
    "        f\"Exercise Types: {', '.join(exercise_types)}\\n\\n\"\n",
    "        f\"CRITICAL RULES:\\n\"\n",
    "        f\"1. TOPIC: Every exercise MUST be about \\\"{topic}\\\" - stay on topic throughout\\n\"\n",
    "        f\"2. REALISM: Use factual, natural scenarios appropriate for the topic\\n\"\n",
    "        f\"3. GRAMMAR: Every exercise MUST test \\\"{grammar_focus}\\\" at {cefr_level} level\\n\"\n",
    "        f\"4. MULTIPLE CHOICE: Provide 4 DIFFERENT grammatical forms as options\\n\"\n",
    "        f\"5. CONSISTENCY: Do not mix different topics or introduce unrelated subjects\\n\\n\"\n",
    "        f\"Output ONLY the JSON array, no additional text.\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Format with chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1500,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract JSON (everything after the user prompt)\n",
    "    if \"[{\" in response:\n",
    "        json_start = response.find('[{')\n",
    "        json_text = response[json_start:]\n",
    "        # Find end of JSON array\n",
    "        if \"}]\" in json_text:\n",
    "            json_end = json_text.rfind(\"}]\") + 2\n",
    "            json_text = json_text[:json_end]\n",
    "        return json_text\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "print(\"✅ Generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 1: Spider Exercises (Gender Test)\n",
    "\n",
    "**Expected**: \"il ragno\" / \"un ragno\" (masculine)\n",
    "\n",
    "**Failed in LoRA**: \"la ragno\" (feminine - WRONG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Test with spiders (gender test)\n",
    "import json\n",
    "\n",
    "print(\"=== TEST 1: Spider Exercises (Gender Test) ===\")\n",
    "print(\"Topic: spiders\")\n",
    "print(\"Grammar: past_tense\")\n",
    "print(\"Expected: 'il ragno' or 'un ragno' (masculine)\\n\")\n",
    "\n",
    "result = generate_exercises(\n",
    "    cefr_level=\"A2\",\n",
    "    grammar_focus=\"past_tense\",\n",
    "    topic=\"spiders\",\n",
    "    quantity=3,\n",
    "    exercise_types=[\"fill_in_blank\", \"translation\", \"multiple_choice\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAW OUTPUT:\")\n",
    "print(\"=\"*80)\n",
    "print(result)\n",
    "\n",
    "# Try to parse JSON\n",
    "try:\n",
    "    exercises = json.loads(result)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PARSED EXERCISES:\")\n",
    "    print(\"=\"*80)\n",
    "    for i, ex in enumerate(exercises, 1):\n",
    "        print(f\"\\nExercise {i}:\")\n",
    "        print(f\"  Type: {ex.get('type')}\")\n",
    "        print(f\"  Question: {ex.get('question')}\")\n",
    "        print(f\"  Answer: {ex.get('answer')}\")\n",
    "        if ex.get('options'):\n",
    "            print(f\"  Options: {ex.get('options')}\")\n",
    "        \n",
    "        # Check for gender errors\n",
    "        question = ex.get('question', '')\n",
    "        answer = ex.get('answer', '')\n",
    "        text = question + \" \" + answer\n",
    "        \n",
    "        if 'la ragno' in text.lower() or 'una ragno' in text.lower():\n",
    "            print(\"  ❌ GENDER ERROR FOUND: 'la ragno' or 'una ragno' (should be masculine)\")\n",
    "        elif 'il ragno' in text.lower() or 'un ragno' in text.lower():\n",
    "            print(\"  ✅ CORRECT: Uses masculine article with 'ragno'\")\n",
    "            \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\n❌ JSON parsing failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 2: Worm Exercises (Gender Test)\n",
    "\n",
    "**Expected**: \"il lombrico\" / \"un lombrico\" (masculine)\n",
    "\n",
    "**Failed in LoRA**: \"le lombrichi\" (feminine - WRONG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test with worms (gender test)\n",
    "print(\"=== TEST 2: Worm Exercises (Gender Test) ===\")\n",
    "print(\"Topic: worms\")\n",
    "print(\"Grammar: past_tense\")\n",
    "print(\"Expected: 'il lombrico' or 'un lombrico' (masculine)\\n\")\n",
    "\n",
    "result = generate_exercises(\n",
    "    cefr_level=\"A2\",\n",
    "    grammar_focus=\"past_tense\",\n",
    "    topic=\"worms\",\n",
    "    quantity=3,\n",
    "    exercise_types=[\"fill_in_blank\", \"translation\", \"multiple_choice\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAW OUTPUT:\")\n",
    "print(\"=\"*80)\n",
    "print(result)\n",
    "\n",
    "# Try to parse JSON\n",
    "try:\n",
    "    exercises = json.loads(result)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PARSED EXERCISES:\")\n",
    "    print(\"=\"*80)\n",
    "    for i, ex in enumerate(exercises, 1):\n",
    "        print(f\"\\nExercise {i}:\")\n",
    "        print(f\"  Type: {ex.get('type')}\")\n",
    "        print(f\"  Question: {ex.get('question')}\")\n",
    "        print(f\"  Answer: {ex.get('answer')}\")\n",
    "        if ex.get('options'):\n",
    "            print(f\"  Options: {ex.get('options')}\")\n",
    "        \n",
    "        # Check for gender errors\n",
    "        question = ex.get('question', '')\n",
    "        answer = ex.get('answer', '')\n",
    "        text = question + \" \" + answer\n",
    "        \n",
    "        if 'la lombric' in text.lower() or 'una lombric' in text.lower() or 'le lombric' in text.lower():\n",
    "            print(\"  ❌ GENDER ERROR FOUND: Uses feminine article with 'lombrico' (should be masculine)\")\n",
    "        elif 'il lombric' in text.lower() or 'un lombric' in text.lower() or 'i lombric' in text.lower():\n",
    "            print(\"  ✅ CORRECT: Uses masculine article with 'lombrico'\")\n",
    "        \n",
    "        # Check for topic drift\n",
    "        if any(word in text.lower() for word in ['lumaca', 'snail', 'granchio', 'crab']):\n",
    "            print(\"  ⚠️  TOPIC DRIFT: Mentions snails or crabs instead of worms\")\n",
    "            \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\n❌ JSON parsing failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 3: Simple Italian Sentences (General Grammar Test)\n",
    "\n",
    "Test if base model can generate grammatically correct Italian sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Simple grammar test\n",
    "print(\"=== TEST 3: Simple Italian Grammar ===\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"Complete the sentence with correct article: ___ ragno è nero. (The spider is black)\",\n",
    "    \"Complete the sentence with correct article: ___ lombrico vive nella terra. (The worm lives in the soil)\",\n",
    "    \"Translate to Italian: The spider climbed the wall.\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an Italian language expert. Answer concisely and correctly.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the answer (after the prompt)\n",
    "    answer = response.split(\"<|assistant|>\")[-1].strip() if \"<|assistant|>\" in response else response\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # Check for errors\n",
    "    if 'la ragno' in answer.lower() or 'una ragno' in answer.lower():\n",
    "        print(\"❌ GENDER ERROR: 'la ragno' or 'una ragno'\")\n",
    "    elif 'la lombric' in answer.lower() or 'una lombric' in answer.lower():\n",
    "        print(\"❌ GENDER ERROR: 'la lombrico' or 'una lombrico'\")\n",
    "    else:\n",
    "        print(\"✅ No obvious gender errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Analysis\n",
    "\n",
    "Run all cells above, then analyze the results:\n",
    "\n",
    "### If Base Model is CORRECT:\n",
    "- ✅ Base model knows \"il ragno\", \"il lombrico\" correctly\n",
    "- ✅ Base model adheres to topics\n",
    "- ✅ Base model generates realistic scenarios\n",
    "- **Conclusion**: LoRA alpha=24 is too strong and causes catastrophic forgetting\n",
    "- **Solution**: Lower LoRA alpha to 4-6 to preserve base knowledge\n",
    "\n",
    "### If Base Model is WRONG:\n",
    "- ❌ Base model also makes gender errors\n",
    "- **Conclusion**: Need different base model or different approach\n",
    "- **Alternative**: Use spaCy post-processing to fix all gender errors\n",
    "\n",
    "### Next Steps:\n",
    "1. If base model is correct: Implement weaker LoRA (alpha=6, rank=8, fewer modules)\n",
    "2. If base model is wrong: Consider different base model or rely on spaCy validation\n",
    "3. Implement EWC regularization regardless to prevent forgetting during training"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
